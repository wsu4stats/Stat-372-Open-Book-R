<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Clustering Analysis | STAT 372 Open Textbook (R)</title>
  <meta name="description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Clustering Analysis | STAT 372 Open Textbook (R)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Clustering Analysis | STAT 372 Open Textbook (R)" />
  
  <meta name="twitter:description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  

<meta name="author" content="Dr.Â Wanhua Su" />


<meta name="date" content="2025-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="discriminant-analysis-and-classification.html"/>
<link rel="next" href="canonical-correlation-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#some-examples"><i class="fa fa-check"></i><b>2.2</b> Some Examples</a>
<ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-1-storm-survival-of-sparrows"><i class="fa fa-check"></i>Example 1: Storm Survival of Sparrows</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-2-spam-or-e-mail"><i class="fa fa-check"></i>Example 2: Spam or E-mail?</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-3-classification-of-iris"><i class="fa fa-check"></i>Example 3: Classification of Iris</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#multivariate-methods-covered-in-stat-372"><i class="fa fa-check"></i><b>2.3</b> Multivariate Methods Covered in STAT 372</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#review-univariate-analysis"><i class="fa fa-check"></i><b>2.4</b> Review: Univariate Analysis</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#random-variable-and-its-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Random Variable and Its Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>2.4.2</b> Properties of Expectation and Variance</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.4.3</b> Continuous Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#revisit-learning-learning-outcomes"><i class="fa fa-check"></i>Revisit Learning Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="matrix-algebra.html"><a href="matrix-algebra.html"><i class="fa fa-check"></i><b>3</b> Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="matrix-algebra.html"><a href="matrix-algebra.html#learning-outcomes-1"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="3.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#vectors"><i class="fa fa-check"></i><b>3.1</b> Vectors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#some-basic-operations-on-vectors"><i class="fa fa-check"></i><b>3.1.1</b> Some Basic Operations on Vectors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrices"><i class="fa fa-check"></i><b>3.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#basic-operations-on-matrix"><i class="fa fa-check"></i><b>3.2.1</b> Basic Operations on Matrix</a></li>
<li class="chapter" data-level="3.2.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.2</b> Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="3.2.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#spectral-eigen-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Spectral (Eigen) Decomposition</a></li>
<li class="chapter" data-level="3.2.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#singular-value-decomposition"><i class="fa fa-check"></i><b>3.2.4</b> Singular-Value Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#mean-vectors-and-covariance-matrices"><i class="fa fa-check"></i><b>3.3</b> Mean Vectors and Covariance Matrices</a></li>
<li class="chapter" data-level="3.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#sample-mean-vector-and-covariance-matrix"><i class="fa fa-check"></i><b>3.4</b> Sample Mean Vector and Covariance Matrix</a></li>
<li class="chapter" data-level="3.5" data-path="matrix-algebra.html"><a href="matrix-algebra.html#review-exercises"><i class="fa fa-check"></i><b>3.5</b> Review Exercises</a></li>
<li class="chapter" data-level="" data-path="matrix-algebra.html"><a href="matrix-algebra.html#revisit-the-learning-outcomes"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html"><i class="fa fa-check"></i><b>4</b> Displaying Multivariate Data and Measures of Distance</a>
<ul>
<li class="chapter" data-level="" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#learning-outcomes-2"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="4.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#display-multivariate-data"><i class="fa fa-check"></i><b>4.1</b> Display Multivariate Data</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#scatterplot"><i class="fa fa-check"></i><b>4.1.1</b> Scatterplot</a></li>
<li class="chapter" data-level="4.1.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#graphs-of-growth-curves"><i class="fa fa-check"></i><b>4.1.2</b> Graphs of Growth Curves</a></li>
<li class="chapter" data-level="4.1.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#star-plots"><i class="fa fa-check"></i><b>4.1.3</b> Star Plots</a></li>
<li class="chapter" data-level="4.1.4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#chernoff-faces-plot"><i class="fa fa-check"></i><b>4.1.4</b> Chernoff Faces Plot</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-in-multivariate-analysis"><i class="fa fa-check"></i><b>4.2</b> Distance in Multivariate Analysis</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distances-for-quantitative-variables"><i class="fa fa-check"></i><b>4.2.1</b> Distances for Quantitative Variables</a></li>
<li class="chapter" data-level="4.2.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-for-categorical-variables"><i class="fa fa-check"></i><b>4.2.2</b> Distance for Categorical Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-for-mixed-variable-types"><i class="fa fa-check"></i><b>4.2.3</b> Distance for Mixed Variable Types</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3</b> Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#properties-of-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Properties of Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="4.3.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#contour-of-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3.3</b> Contour of Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#the-sampling-distribution-of-mathbfbar-x-and-boldsymbols"><i class="fa fa-check"></i><b>4.4</b> The Sampling Distribution of <span class="math inline">\(\mathbf{\bar X}\)</span> and <span class="math inline">\(\boldsymbol{S}\)</span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distributions-related-to-normal-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Distributions Related to Normal Distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#applications-to-distributions-related-to-sample-means"><i class="fa fa-check"></i><b>4.4.2</b> Applications to Distributions Related to Sample Means</a></li>
<li class="chapter" data-level="4.4.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#generalize-to-multivariate-cases"><i class="fa fa-check"></i><b>4.4.3</b> Generalize to Multivariate Cases</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#review-exercises-1"><i class="fa fa-check"></i><b>4.5</b> Review Exercises</a></li>
<li class="chapter" data-level="" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#revisit-the-learning-outcomes-1"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests on Mean Vectors</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#learning-outcomes-3"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="5.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-one-mean-vector"><i class="fa fa-check"></i><b>5.1</b> Hypothesis Test for one Mean Vector</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case"><i class="fa fa-check"></i><b>5.1.1</b> Univariate Case</a></li>
<li class="chapter" data-level="5.1.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case"><i class="fa fa-check"></i><b>5.1.2</b> Multivariate Case</a></li>
<li class="chapter" data-level="5.1.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#evaluating-multivariate-normality"><i class="fa fa-check"></i><b>5.1.3</b> Evaluating Multivariate Normality</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-two-mean-vectors"><i class="fa fa-check"></i><b>5.2</b> Hypothesis Test for Two Mean Vectors</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-two-independent-samples"><i class="fa fa-check"></i><b>5.2.1</b> Univariate Case Based on Two Independent Samples</a></li>
<li class="chapter" data-level="5.2.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-two-independent-samples"><i class="fa fa-check"></i><b>5.2.2</b> Multivariate Case Based on Two Independent Samples</a></li>
<li class="chapter" data-level="5.2.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#two-sample-non-pooled-hotellings-t2-test"><i class="fa fa-check"></i><b>5.2.3</b> Two-sample Non-pooled Hotellingâs <span class="math inline">\(T^2\)</span> Test</a></li>
<li class="chapter" data-level="5.2.4" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#two-sample-hotellings-t2-confidence-interval"><i class="fa fa-check"></i><b>5.2.4</b> Two-sample Hotellingâs <span class="math inline">\(T^2\)</span> Confidence Interval</a></li>
<li class="chapter" data-level="5.2.5" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-a-paired-sample"><i class="fa fa-check"></i><b>5.2.5</b> Univariate Case Based on a Paired Sample</a></li>
<li class="chapter" data-level="5.2.6" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-a-paired-sample"><i class="fa fa-check"></i><b>5.2.6</b> Multivariate Case Based on a Paired Sample</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-several-mean-vectors"><i class="fa fa-check"></i><b>5.3</b> Hypothesis Test for Several Mean Vectors</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-one-way-anova-f-test"><i class="fa fa-check"></i><b>5.3.1</b> Univariate Case: One-Way ANOVA F Test</a></li>
<li class="chapter" data-level="5.3.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-one-way-manova"><i class="fa fa-check"></i><b>5.3.2</b> Multivariate Case: One-Way MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#revisit-the-learning-outcomes-2"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>6</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#learning-outcomes-4"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="6.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#finding-the-principal-components"><i class="fa fa-check"></i><b>6.1</b> Finding the Principal Components</a></li>
<li class="chapter" data-level="6.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#scaling-in-principal-component-analysis"><i class="fa fa-check"></i><b>6.2</b> Scaling in Principal Component Analysis</a></li>
<li class="chapter" data-level="6.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#limitations-of-principal-component-analysis"><i class="fa fa-check"></i><b>6.3</b> Limitations of Principal Component Analysis</a></li>
<li class="chapter" data-level="6.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#further-reading"><i class="fa fa-check"></i><b>6.4</b> Further Reading</a></li>
<li class="chapter" data-level="" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#revisit-the-learning-outcomes-3"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>7</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="factor-analysis.html"><a href="factor-analysis.html#learning-outcomes-5"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="7.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-of-factor-analysis"><i class="fa fa-check"></i><b>7.1</b> Model of Factor Analysis</a></li>
<li class="chapter" data-level="7.2" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-loadings-l_ij-and-specific-variance-psi_i"><i class="fa fa-check"></i><b>7.2</b> Estimating Factor Loadings <span class="math inline">\(l_{ij}\)</span> and Specific Variance <span class="math inline">\(\psi_i\)</span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#the-principle-component-method"><i class="fa fa-check"></i><b>7.2.1</b> The Principle Component Method</a></li>
<li class="chapter" data-level="7.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#the-maximum-likelihood-method"><i class="fa fa-check"></i><b>7.2.2</b> The Maximum Likelihood Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-rotation"><i class="fa fa-check"></i><b>7.3</b> Factor Rotation</a></li>
<li class="chapter" data-level="7.4" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-scores"><i class="fa fa-check"></i><b>7.4</b> Factor Scores</a></li>
<li class="chapter" data-level="" data-path="factor-analysis.html"><a href="factor-analysis.html#revisit-the-learning-outcomes-4"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html"><i class="fa fa-check"></i><b>8</b> Discriminant Analysis and Classification</a>
<ul>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#learning-outcomes-6"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="8.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#introduction-1"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#performance-measure"><i class="fa fa-check"></i><b>8.2</b> Performance Measure</a></li>
<li class="chapter" data-level="8.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#overfitting-and-cross-validation"><i class="fa fa-check"></i><b>8.3</b> Overfitting and Cross Validation</a></li>
<li class="chapter" data-level="8.4" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classification-models"><i class="fa fa-check"></i><b>8.4</b> Classification Models</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.4.1</b> <span class="math inline">\(K\)</span> Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#logistic-regression-for-binary-response"><i class="fa fa-check"></i><b>8.5</b> Logistic Regression for Binary Response</a>
<ul>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#interpretation-of-beta_i"><i class="fa fa-check"></i>Interpretation of <span class="math inline">\(\beta_i\)</span></a></li>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#estimation-of-beta_i"><i class="fa fa-check"></i>Estimation of <span class="math inline">\(\beta_i\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#logistic-regression-for-multi-class-nominal-data"><i class="fa fa-check"></i><b>8.6</b> Logistic Regression for Multi-class Nominal Data</a></li>
<li class="chapter" data-level="8.7" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#cumulative-logit-model-for-multi-class-ordinal-data"><i class="fa fa-check"></i><b>8.7</b> Cumulative Logit Model for Multi-class Ordinal Data</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#cumulative-logit-models-with-proportional-odds"><i class="fa fa-check"></i><b>8.7.1</b> Cumulative Logit Models with Proportional Odds</a></li>
<li class="chapter" data-level="8.7.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-probability-of-each-category"><i class="fa fa-check"></i><b>8.7.2</b> Model Probability of Each Category</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-selection-for-logistic-regression"><i class="fa fa-check"></i><b>8.8</b> Model Selection for Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#aic-and-bic"><i class="fa fa-check"></i><b>8.8.1</b> AIC and BIC</a></li>
<li class="chapter" data-level="8.8.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#forward-selection"><i class="fa fa-check"></i><b>8.8.2</b> Forward Selection</a></li>
<li class="chapter" data-level="8.8.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#backward-elimination"><i class="fa fa-check"></i><b>8.8.3</b> Backward Elimination</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-checking"><i class="fa fa-check"></i><b>8.9</b> Model Checking</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#residual-analysis"><i class="fa fa-check"></i><b>8.9.1</b> Residual Analysis</a></li>
<li class="chapter" data-level="8.9.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#preditive-power-accuracy-and-roc-curve"><i class="fa fa-check"></i><b>8.9.2</b> Preditive Power: Accuracy and ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classification-tree-recursive-partitioning"><i class="fa fa-check"></i><b>8.10</b> Classification Tree (Recursive Partitioning)</a></li>
<li class="chapter" data-level="8.11" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#regression-tree"><i class="fa fa-check"></i><b>8.11</b> Regression Tree</a></li>
<li class="chapter" data-level="8.12" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#random-forest"><i class="fa fa-check"></i><b>8.12</b> Random Forest</a></li>
<li class="chapter" data-level="8.13" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#support-vector-machines"><i class="fa fa-check"></i><b>8.13</b> Support Vector Machines</a></li>
<li class="chapter" data-level="8.14" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#neural-networks"><i class="fa fa-check"></i><b>8.14</b> Neural Networks</a></li>
<li class="chapter" data-level="8.15" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classical-methods"><i class="fa fa-check"></i><b>8.15</b> Classical Methods</a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#mahalanobis-distance-method"><i class="fa fa-check"></i><b>8.15.1</b> Mahalanobis Distance Method</a></li>
<li class="chapter" data-level="8.15.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#bayes-posterior"><i class="fa fa-check"></i><b>8.15.2</b> Bayes Posterior</a></li>
<li class="chapter" data-level="8.15.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#fishers-discriminant-analysis"><i class="fa fa-check"></i><b>8.15.3</b> Fisherâs Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#summary"><i class="fa fa-check"></i><b>8.16</b> Summary</a></li>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#revisit-learning-outcomes"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="clustering-analysis.html"><a href="clustering-analysis.html"><i class="fa fa-check"></i><b>9</b> Clustering Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#learning-outcomes-7"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="9.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#introduction-2"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#clustering-methods"><i class="fa fa-check"></i><b>9.2</b> Clustering Methods</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#hierarchical-method"><i class="fa fa-check"></i><b>9.2.1</b> Hierarchical Method</a></li>
<li class="chapter" data-level="9.2.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#k-means"><i class="fa fa-check"></i><b>9.2.2</b> K-Means</a></li>
<li class="chapter" data-level="9.2.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#model-based-clustering"><i class="fa fa-check"></i><b>9.2.3</b> Model-Based Clustering</a></li>
<li class="chapter" data-level="9.2.4" data-path="clustering-analysis.html"><a href="clustering-analysis.html#pros-and-cons-of-clustering-methods"><i class="fa fa-check"></i><b>9.2.4</b> Pros and Cons of Clustering Methods</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#determine-k-number-of-clusters"><i class="fa fa-check"></i><b>9.3</b> Determine <span class="math inline">\(K\)</span>: Number of Clusters</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#the-elbow-plot-method"><i class="fa fa-check"></i><b>9.3.1</b> The Elbow Plot Method</a></li>
<li class="chapter" data-level="9.3.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#the-silhouette-score"><i class="fa fa-check"></i><b>9.3.2</b> The Silhouette Score</a></li>
<li class="chapter" data-level="9.3.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#gap-statistics"><i class="fa fa-check"></i><b>9.3.3</b> Gap Statistics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#side-note-on-the-em-algorithm"><i class="fa fa-check"></i>Side-Note on the EM Algorithm</a></li>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#revisit-learning-outcomes-1"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html"><i class="fa fa-check"></i><b>10</b> Canonical Correlation Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#learning-outcomes-8"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="10.1" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#objective"><i class="fa fa-check"></i><b>10.1</b> Objective</a></li>
<li class="chapter" data-level="10.2" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#obtain-the-canonical-variates-pairs"><i class="fa fa-check"></i><b>10.2</b> Obtain the Canonical Variates Pairs</a></li>
<li class="chapter" data-level="10.3" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#interpretation"><i class="fa fa-check"></i><b>10.3</b> Interpretation</a></li>
<li class="chapter" data-level="10.4" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#testing-mathbfsigma_120"><i class="fa fa-check"></i><b>10.4</b> Testing <span class="math inline">\(\mathbf{\Sigma}_{12}=0\)</span></a></li>
<li class="chapter" data-level="" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#revisit-learning-outcomes-2"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html"><i class="fa fa-check"></i><b>11</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#learning-outcomes-9"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="11.1" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#objective-1"><i class="fa fa-check"></i><b>11.1</b> Objective</a></li>
<li class="chapter" data-level="11.2" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#methods"><i class="fa fa-check"></i><b>11.2</b> Methods</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#classical-scaling"><i class="fa fa-check"></i><b>11.2.1</b> Classical Scaling</a></li>
<li class="chapter" data-level="11.2.2" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#metric-scaling"><i class="fa fa-check"></i><b>11.2.2</b> Metric Scaling</a></li>
<li class="chapter" data-level="11.2.3" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#non-metric-scaling"><i class="fa fa-check"></i><b>11.2.3</b> Non-metric Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#example"><i class="fa fa-check"></i><b>11.3</b> Example</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#revisit-learning-outcomes-3"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 372 Open Textbook (R)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering-analysis" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">9</span> Clustering Analysis<a href="clustering-analysis.html#clustering-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="learning-outcomes-7" class="section level2 unnumbered hasAnchor">
<h2>Learning Outcomes<a href="clustering-analysis.html#learning-outcomes-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After finishing this chapter, students should be able to</p>
<ul>
<li>Explain the differences between classification and cluster problems.</li>
<li>Describe briefly the main idea and procedure of hierarchical clustering, <span class="math inline">\(K\)</span>-means, and model-based clustering methods.</li>
<li>Conduct a clustering analysis using hierarchical clustering method, <span class="math inline">\(K\)</span>-means, and model-based clustering in R.</li>
<li>Interpret the R computer outputs of hierarchical clustering, <span class="math inline">\(K\)</span>-means, and model-based clustering methods.</li>
</ul>
</div>
<div id="introduction-2" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Introduction<a href="clustering-analysis.html#introduction-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have already introduced multivariate data analysis and classification. This note introduces another major application in machine learning: clustering.</p>
<p>The main difference between a clustering problem and a classification problem is that the class labels are unknown for clustering. The objectives of a clustering problem are</p>
<ul>
<li>Determine the proper number of clusters <span class="math inline">\(K\)</span>.</li>
<li>Group the observations into <span class="math inline">\(K\)</span> clusters.</li>
<li>Allocate new observations to one of those <span class="math inline">\(K\)</span> clusters.</li>
</ul>
</div>
<div id="clustering-methods" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Clustering Methods<a href="clustering-analysis.html#clustering-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Hierarchical clustering method, <span class="math inline">\(K\)</span>-means, and model-based clustering will be covered.</p>
<div id="hierarchical-method" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Hierarchical Method<a href="clustering-analysis.html#hierarchical-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like the forward selection and backward elimination model selection methods in multiple regression, either merging (start with one observation as one group) or division (start with all observation in one cluster) can be used in hierarchical methods. Suppose there are <span class="math inline">\(n\)</span> observations, we can calculate the <span class="math inline">\(n\times n\)</span> pairwise distance matrix using Euclidean, Manhattan, Mahalanobis, Hamming, and Gowerâs distance. Steps to conduct a hierarchical clustering are as follows:</p>
<ol style="list-style-type: decimal">
<li>One observation one group</li>
<li>Merge the two groups with the smallest distance or largest similarity</li>
<li>Repeat step 2 until all observations are in one group.</li>
</ol>
<p>There are three way to measure the distance <strong>between</strong> two groups <span class="math inline">\(G_i\)</span> and <span class="math inline">\(G_j\)</span>, <span class="math inline">\(d(G_i, G_j)\)</span>.</p>
<ul>
<li>single linkage: <span class="math inline">\(d(G_i, G_j)=\min_{p\in G_i, q\in G_j} d(p,q)\)</span>.The distance between the closest pair of objects belonging to two different groups.</li>
<li>complete linkage: <span class="math inline">\(d(G_i, G_j)=\max_{p\in G_i, q\in G_j} d(p,q)\)</span>.The distance between the furthest pair of objects belonging to two different groups.</li>
<li>average linkage:<span class="math inline">\(d(G_i, G_j)=\text{average}_{p\in G_i, q\in G_j} d(p,q)\)</span>.The average distance of all between-group pairs.</li>
</ul>
<div id="example-hierarchical-clustering" class="section level4 unnumbered hasAnchor">
<h4>Example: Hierarchical Clustering<a href="clustering-analysis.html#example-hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(x_1=5, x_2=3, x_3=9, x_4=6, x_5=1\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Find the distance matrix.</li>
</ol>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="clustering-analysis.html#cb1-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">1</span>)</span>
<span id="cb1-2"><a href="clustering-analysis.html#cb1-2" tabindex="-1"></a>(<span class="fu">dist</span>(x))</span></code></pre></div>
<pre><code>##   1 2 3 4
## 2 2      
## 3 4 6    
## 4 1 3 3  
## 5 4 2 8 5</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>Conduct a hierarchical clustering with single linkage.</li>
</ol>
We first merge <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_4\)</span> which pair has the shortest distance 1. Update the distance matrix using the single linkage.
<p><span class="math display">\[  
\begin{aligned}
d_{(1,4), 2}&amp;=\min\{d_{12}, d_{42}\}=\min\{2, 3\}=2\\
d_{(1,4), 3}&amp;=\min\{d_{13}, d_{43}\}=\min\{4, 3\}=3\\
d_{(1,4), 5}&amp;=\min\{d_{15}, d_{45}\}=\min\{4, 5\}=4\\
\end{aligned}
\]</span></p>
<p>We can either merge <span class="math inline">\(\{x_1, x_4\}\)</span> with <span class="math inline">\(x_2\)</span> or <span class="math inline">\(x_2\)</span> with <span class="math inline">\(x_5\)</span>, both of which have distance 2. I prefer to keep the group size smaller; therefore, I merge <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_5\)</span>. Update the distance matrix.</p>
<p><span class="math display">\[
\begin{aligned}
d_{(1,4), (2,5)}&amp;=\min\{d_{(1,4),2}, d_{(1,4),5}\}=\min\{2, 4\}=2\\
d_{(2,5), 3}&amp;=\min\{d_{23}, d_{53}\}=\min\{6, 8\}=6\\
\end{aligned}
\]</span></p>
<p>We merge <span class="math inline">\(\{x_1, x_4\}\)</span> and <span class="math inline">\(\{x_2, x_5\}\)</span>. Lastly, we merger <span class="math inline">\(\{x_1, x_4, x_2, x_5\}\)</span> and <span class="math inline">\(x_3\)</span> with distance
<span class="math display">\[
d_{(1,4,2,5),3}=\min\{d_{(1,4),3}, d_{(2,5),3}\}=\min\{3, 6\}=3
\]</span>
As a result, the cluster dendrogram is as follows:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="clustering-analysis.html#cb3-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">1</span>)</span>
<span id="cb3-2"><a href="clustering-analysis.html#cb3-2" tabindex="-1"></a>shcm <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x),<span class="at">method=</span><span class="st">&quot;single&quot;</span>)</span>
<span id="cb3-3"><a href="clustering-analysis.html#cb3-3" tabindex="-1"></a><span class="fu">plot</span>(shcm, <span class="at">ylab=</span><span class="st">&quot;Distance&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;Variables&quot;</span>,<span class="at">hang=</span><span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="Plots/dendro1-1.png" width="65%" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Conduct a hierarchical clustering with complete linkage.</li>
</ol>
<p>Again, we first merge <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_4\)</span> and update the distance matrix using the complete linkage.</p>
<p><span class="math display">\[
\begin{aligned}
d_{(1,4), 2}&amp;=\max\{d_{12}, d_{42}\}=\max\{2, 3\}=3\\
d_{(1,4), 3}&amp;=\max\{d_{13}, d_{43}\}=\max\{4, 3\}=4\\
d_{(1,4), 5}&amp;=\max\{d_{15}, d_{45}\}=\max\{4, 5\}=5\\
\end{aligned}
\]</span></p>
<p>Next, we merge <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_5\)</span> and update the distance matrix.</p>
<p><span class="math display">\[
\begin{aligned}
d_{(1,4), (2,5)}&amp;=\max\{d_{(1,4),2}, d_{(1,4),5}\}=\max\{3, 5\}=5\\
d_{(2,5), 3}&amp;=\max\{d_{23}, d_{53}\}=\max\{6, 8\}=8\\
\end{aligned}
\]</span>
We merge <span class="math inline">\(\{x_1, x_4\}\)</span> and <span class="math inline">\(x_3\)</span>. Lastly, we merger <span class="math inline">\(\{x_1, x_4, x_3\}\)</span> and <span class="math inline">\(\{x_2, x-5\}\)</span> with distance
<span class="math display">\[
d_{(1,4,3),(2,5)}=\max\{d_{(1,4),(2,5)}, d_{3,(2,5)}\}=\max\{5, 8\}=8
\]</span>
As a result, the cluster dendrogram is as follows:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="clustering-analysis.html#cb4-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">1</span>)</span>
<span id="cb4-2"><a href="clustering-analysis.html#cb4-2" tabindex="-1"></a>chcm <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x),<span class="at">method=</span><span class="st">&quot;complete&quot;</span>)</span>
<span id="cb4-3"><a href="clustering-analysis.html#cb4-3" tabindex="-1"></a><span class="fu">plot</span>(chcm, <span class="at">ylab=</span><span class="st">&quot;Distance&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;Variables&quot;</span>,<span class="at">hang=</span><span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="Plots/dendro2-1.png" width="65%" /></p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Conduct a hierarchical clustering with average linkage.</li>
</ol>
<p>Again, we first merge <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_4\)</span> and update the distance matrix using the average linkage.
[</p>
<span class="math display">\[\begin{array}{c|ccc}
&amp;(1, 4)&amp;2&amp;3\\
\hline
2&amp;2.5&amp;&amp;\\
3&amp;3.5&amp;6&amp;\\
5&amp;4.5&amp;2&amp;8\\
\end{array}\]</span>
<p>]</p>
<p><span class="math display">\[
\begin{aligned}
d_{(1,4), 2}&amp;=\frac{d_{12}+d_{42}}{2}=\frac{2+3}{2}=2.5\\
d_{(1,4), 3}&amp;=\frac{d_{13}+d_{43}}{2}=\frac{4+3}{2}=3.5\\
d_{(1,4), 5}&amp;=\frac{d_{15}+d_{45}}{2}=\frac{4+5}{2}=4.5\\
\end{aligned}
\]</span></p>
<p>Next, we merge <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_5\)</span> and update the distance matrix.</p>
[
<span class="math display">\[\begin{array}{c|cc}
&amp;(1, 4)&amp;(2, 5)\\
\hline
(2, 5)&amp;3.5&amp;\\
3&amp;3.5&amp;7\\
\end{array}\]</span>
<p>]</p>
<p><span class="math display">\[
\begin{aligned}
d_{(1,4), (2,5)}&amp;=\frac{d_{(1,4),2}+d_{(1,4),5}}{2}=\frac{2.5+4.5}{2}=3.5\\
d_{(2,5), 3}&amp;=\frac{d_{23}+d_{53}}{2}=\frac{6+8}{2}=7\\
\end{aligned}
\]</span></p>
<p>We merge <span class="math inline">\(\{x_1, x_4\}\)</span> and <span class="math inline">\(\{x_2, x_5\}\)</span>. Lastly, we merger <span class="math inline">\(\{x_1, x_4, x_2, x_5\}\)</span> and <span class="math inline">\(x_3\)</span> with distance
<span class="math display">\[
d_{(1,4,2,5),3}=\frac{d_{(1,4),3}+d_{(2,5),3}}{2}=\frac{3.5+7}{2}=5.25
\]</span>
As a result, the cluster dendrogram is as follows:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="clustering-analysis.html#cb5-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">1</span>)</span>
<span id="cb5-2"><a href="clustering-analysis.html#cb5-2" tabindex="-1"></a>ahcm <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x),<span class="at">method=</span><span class="st">&quot;average&quot;</span>)</span>
<span id="cb5-3"><a href="clustering-analysis.html#cb5-3" tabindex="-1"></a><span class="fu">plot</span>(ahcm, <span class="at">ylab=</span><span class="st">&quot;Distance&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;Variables&quot;</span>,<span class="at">hang=</span><span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="Plots/dendro3-1.png" width="65%" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="clustering-analysis.html#cb6-1" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(ahcm,<span class="dv">2</span>)</span>
<span id="cb6-2"><a href="clustering-analysis.html#cb6-2" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(ahcm,<span class="dv">3</span>)</span></code></pre></div>
<p>This dendrogram tells us that cutting at distance=5 results in two clusters <span class="math inline">\(\{x_3\}, \{x_1,x_4,x_2,x_5\}\)</span>; cutting at distance=3 results in three clusters <span class="math inline">\(\{x_3\}, \{x_1,x_4\}, \{x_2,x_5\}\)</span>.</p>
</div>
</div>
<div id="k-means" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> K-Means<a href="clustering-analysis.html#k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compared to the hierarchical clustering, <span class="math inline">\(K\)</span>-mean is much more efficient in computation. The algorithm of <span class="math inline">\(K\)</span>-mean is as follows:</p>
<ol style="list-style-type: decimal">
<li>Arbitrarily pick the centers of the <span class="math inline">\(K\)</span> clusters <span class="math inline">\(\mathbf{m}_1^{(\mbox{old})}, \mathbf{m}_2^{(\mbox{old})},\cdots, \mathbf{m}_K^{(\mbox{old})}\)</span>.</li>
<li>Assign each object to one and only one of the clusters, choose the one with the shortest distance from the object to the group center. That is <span class="math inline">\(Y=\text{argmin}_j d(\mathbf{x},\mathbf{m}_j)\)</span>.</li>
<li>Update the group (cluster) centers
<span class="math display">\[
\mathbf{m}_i^{(\mbox{new})}=\frac{1}{n_i}\sum_{\mathbf{x}\in G_i} \mathbf{x}, i=1, 2, \cdots, K, \quad \mbox{$n_i$ is the number of objects in group $i$}
\]</span></li>
<li>Repeat steps (2) and (3) until convergence, i.e., the centers wonât change anymore.</li>
</ol>
<p>Here is an illustration adopted from Dr.Â Jeffrey L. Andrewsâs notes. Twenty-five observations were generated from bivariate normal distributions with mean vectors <span class="math inline">\(\mathbf{\mu}_1=(0, 0)^T\)</span> and <span class="math inline">\(\mathbf{\mu}_2=(5, -5)^T\)</span>, and covariance matrix
<span class="math display">\[
\mathbf{\Sigma}_1=\mathbf{\Sigma}_2=\left[
\begin{array}{cc}
1&amp;0\\
0&amp;1
\end{array}
\right].
\]</span></p>
<p>The K-means method converges in three iterations, i.e., the labels of observation donât change after three rounds.</p>
<p><img src="Plots/kmeans.png" width="100%" /></p>
<p><strong>Note</strong>: The result of <span class="math inline">\(K\)</span>-means will be affected by the initial value, it is important to run the algorithm multiple times from different random initial configurations.</p>
<p>Letâs revisit the simple example.</p>
<div id="example-k-means-clustering" class="section level4 unnumbered hasAnchor">
<h4>Example: K-Means Clustering<a href="clustering-analysis.html#example-k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(x_1=5, x_2=3, x_3=9, x_4=6, x_5=1\)</span>. Assume <span class="math inline">\(K=2\)</span> clusters.</p>
<ul>
<li>[(a)] If we set the initial centers <span class="math inline">\(\mathbf{m}_1^{(0)}=0, \mathbf{m}_2^{(0)}=11\)</span>.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Allocation. Group 1: <span class="math inline">\(\{x_5, x_2, x_1\}\)</span>; group 2: <span class="math inline">\(\{x_4, x_3\}\)</span>.</li>
<li>Recalculate the centers.
<span class="math display">\[
\mathbf{m}_1^{(1)}=\frac{1+3+5}{3}=3, \mathbf{m}_2^{(1)}=\frac{6+9}{2}=7.5
\]</span></li>
<li>Re-allocate. Group 1: <span class="math inline">\(\{x_5, x_2, x_1\}\)</span>; group 2: <span class="math inline">\(\{x_4, x_3\}\)</span>. The centers are the same as the previous step. Algorithm converges. The final grouping is group 1: <span class="math inline">\(\{x_5, x_2, x_1\}\)</span> and group 2: <span class="math inline">\(\{x_4, x_3\}\)</span> with centers 3 and 7.5 respectively.</li>
</ol>
<ul>
<li>[(b)] If we set the initial centers <span class="math inline">\(\mathbf{m}_1^{(0)}=4, \mathbf{m}_2^{(0)}=5\)</span>.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Allocation. Group 1: <span class="math inline">\(\{x_5, x_2\}\)</span>; group 2: <span class="math inline">\(\{x_1, x_4, x_3\}\)</span>.</li>
<li>Recalculate the centers.
<span class="math display">\[
\mathbf{m}_1^{(1)}=\frac{1+3}{2}=2, \mathbf{m}_2^{(1)}=\frac{5+6+9}{3}=6.667
\]</span></li>
<li>Re-allocate. Group 1: <span class="math inline">\(\{x_5, x_2\}\)</span>; group 2: <span class="math inline">\(\{x_1, x_4, x_3\}\)</span>. The centers are the same as the previous step. Algorithm converges. The final grouping is group 1: <span class="math inline">\(\{x_5, x_2\}\)</span> and group 2: <span class="math inline">\(\{x_1, x_4, x_3\}\)</span> with centers 2 and 6.667 respectively.</li>
</ol>
</div>
</div>
<div id="model-based-clustering" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Model-Based Clustering<a href="clustering-analysis.html#model-based-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model-based clustering method models the joint distribution of the data using a mixture model of <span class="math inline">\(K\)</span> components
<span class="math display">\[
f(\mathbf{x})=\sum_{i=1}^K p_if_i(\mathbf{x}), \quad \mbox{ with component proportions  $p_i\ge 0$ and $\sum p_i=1$.}
\]</span>
A popular choice of the components is <span class="math inline">\(f_i(\mathbf{x})\sim \mbox{MVN}(\mathbf{\mu}_i, \mathbf{\Sigma}_i)\)</span>, which results in a multivariate normal mixture model. Suppose there are <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\)</span>, the likelihood function of the multivariate normal mixture is
<span class="math display">\[
L(p_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i)=\prod_{j=1}^n f(\mathbf{x}_j)=\prod_{j=1}^n \left(\sum_{i=1}^K p_i f_i(\mathbf{x}_j) \right)=\prod_{j=1}^n \left(\sum_{i=1}^K p_i \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}_i|^{1/2}}\exp\{-\frac{1}{2}(\mathbf{x}_j-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}_j-\mathbf{\mu}_i)\} \right).
\]</span>
There are a lot of unknown parameters in the above model:</p>
<ul>
<li>The mixture proportions <span class="math inline">\(p_1, p_2, \cdots, p_K\)</span> with the constraint <span class="math inline">\(\sum p_i=1 \Longrightarrow K-1\)</span> parameters.</li>
<li>The mean vectors <span class="math inline">\(\mathbf{\mu}_i, i=1, \cdots, K\)</span>, each mean vector is a <span class="math inline">\(p\times 1\)</span> vector <span class="math inline">\(\Longrightarrow p\times K\)</span> parameters.
-The covariance matrices <span class="math inline">\(\mathbf{\Sigma}_i, i=1, \cdots, K\)</span>. A covariance matrix is symmetric with <span class="math inline">\(\text{var}(X_i), i=1, \cdots, p\)</span> as the diagonal elements and covariance <span class="math inline">\(\text{cov}(X_i, X_j), i&lt;j , i,j=1, \cdots, p\)</span>. Therefore, each covariance matrix consists of <span class="math inline">\(\binom{p}{2}+p=\frac{p(p+1)}{2}\)</span> parameters. There are <span class="math inline">\(K\)</span> covariance matrices <span class="math inline">\(\Longrightarrow \frac{p(p+1)}{2}\times K\)</span> parameters.</li>
</ul>
<p>As a result, the full model has <span class="math inline">\((k-1)+pK+\frac{p(p+1)}{2}\times K\)</span> parameters. Take the Iris data for example, with <span class="math inline">\(K=3, p=4\)</span>, the number of parameters of the mixture model ends up to be 44. When <span class="math inline">\(p\)</span> is large, the following reduced models can be considered:</p>
[
<span class="math display">\[\begin{array}{c|c}
\hline
\text{Covariance structure}&amp;\text{Total # of parameters}\\
\hline
\mathbf{\Sigma}_i=\eta \mathbf{I}&amp;(k-1)+pK+1\\
\mathbf{\Sigma}_i=\eta_i \mathbf{I}&amp;(k-1)+pK+K\\
\mathbf{\Sigma}_i=\eta_i \mbox{diag}(\lambda_1, \lambda_2, \cdots, \lambda_p)&amp;(k-1)+pK+K+p\\
\hline
\end{array}\]</span>
<p>]</p>
<p>Maximum likelihood method can be used to estimate the values of the parameters. The main idea of maximum likelihood estimate (MLE) is to find the values of the parameters such that the likelihood function <span class="math inline">\(L_{\mbox{max}}\)</span> is maximized. The Akaike Information criterion (AIC) and the Bayesian Information criterion (BIC) are two popular model selection criterion. We select a model that gives the largest AIC or BIC which are given by
<span class="math display">\[
\mbox{AIC}=2\ln L_{\mbox{max}}-2(\mbox{\# of free parameters}); \quad \mbox{BIC}=2\ln L_{\mbox{max}}-\ln (n) (\mbox{\# of free parameters})
\]</span>
where <span class="math inline">\(n\)</span> is the number of observations. BIC is more stringent on the number of parameters than AIC, since <span class="math inline">\(\ln (n)&gt;2\)</span> for all <span class="math inline">\(n\ge 8\)</span></p>
<div id="em-algorithm" class="section level4 unnumbered hasAnchor">
<h4>EM Algorithm<a href="clustering-analysis.html#em-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In order to solve the MLE of the mixture model, membership information (each object belongs to which group) is required. The membership information; however, is missing. Therefore, the <em>Expectation-Maximization</em> (EM) algorithm is used to find the MLE. The EM algorithm is a very popular method handling missing values.</p>
<p>In order to capture the membership information, we introduce a vector of <span class="math inline">\(K\)</span> binary (unobserved) variables <span class="math inline">\(\mathbf{Z}_i^T=[Z_{i1}, Z_{i2}, \cdots, Z_{iK}]\)</span> for each object <span class="math inline">\(\mathbf{x}_i\)</span>, where <span class="math inline">\(z_{ij}=1, z_{il}=0, l\ne j\)</span> if object <span class="math inline">\(i\)</span> belongs to group <span class="math inline">\(j\)</span>. Then the membership weight of <span class="math inline">\(\mathbf{x}_i\)</span> in group <span class="math inline">\(j\)</span> is
<span class="math display">\[
w_{ij}=P(Z_{ij}=1|\mathbf{x}_i)=E(Z_{ij}|\mathbf{x}_i)=P(Z_{ij}=1|\mathbf{x}_i)=\frac{f_j(\mathbf{x}_i)p_j}{\sum_{m=1}^K f_m(\mathbf{x}_i)p_m}, \quad i=1, 2, \cdots, n, \quad j=1, 2,\cdots, K
\]</span>
For each <span class="math inline">\(\mathbf{x}_i\)</span>, <span class="math inline">\(\sum_{j=1}^K w_{ij}=1\)</span>. As a result, we have a <span class="math inline">\(n\times K\)</span> membership matrix with each row sum equal to 1.</p>
<p>The steps of the EM algorithm are as follows:</p>
<ol style="list-style-type: decimal">
<li>Set the initial values for the parameters <span class="math inline">\(p_1, \cdots, p_K; \mathbf{\mu}_1, \cdots, \mathbf{\mu}_K; \mathbf{\Sigma}_1, \cdots, \mathbf{\Sigma}_K\)</span>.</li>
<li>E-step. Update the membership weight <span class="math inline">\(w_{ij}=E(Z_{ij})\)</span>:
<span class="math display">\[
w_{ij}=\frac{f_j(\mathbf{x}_i)p_j}{\sum_{m=1}^K f_m(\mathbf{x}_i)p_m}, \quad i=1, 2, \cdots, n, \quad j=1, 2,\cdots, K
\]</span></li>
<li>M-step. Update the parameters
<span class="math display">\[
p_j^{\mathbf{new}}=\frac{n_j}{n}, \quad \mbox{with } \quad n_j=\sum_{i=1}^n w_{ij} \quad (\mbox{sum of $j$ column of the membership weight matrix})
\]</span>
<span class="math display">\[
\mathbf{\mu}_j^{\mbox{new}}=\frac{1}{n_j}\sum_{i=1}^n w_{ij}\mathbf{x}_i, \quad \mathbf{\Sigma}_j^{\mbox{new}}=\frac{1}{n_j}\sum_{i=1}^n w_{ij}(\mathbf{x}_i-\mathbf{\mu}_j^{\mbox{new}})(\mathbf{x}_i-\mathbf{\mu}_j^{\mbox{new}})^T
\]</span></li>
<li>Repeat steps (2) &amp; (3) until convergence (no change).</li>
</ol>
<p>Note: the <span class="math inline">\(K\)</span>-means algorithm can be regarded as a special of the multivariate normal mixture model by letting <span class="math inline">\(\mathbf{\Sigma}_i=\mathbf{I}\)</span> and the membership weight be 1 for the group with the largest posterior probability and 0 for all the other groups, i.e.,
<span class="math display">\[
w_{ij}=\left\{
\begin{array}{ll}
1&amp;\mbox{if group $j$ has the largest posterior probability},\\
0&amp;\mbox{otherwise}.
\end{array}
\right.
\]</span></p>
</div>
<div id="example-model-based-clustering" class="section level4 hasAnchor" number="9.2.3.1">
<h4><span class="header-section-number">9.2.3.1</span> Example: Model-based Clustering<a href="clustering-analysis.html#example-model-based-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="clustering-analysis.html#cb7-1" tabindex="-1"></a><span class="fu">library</span>(mclust)</span>
<span id="cb7-2"><a href="clustering-analysis.html#cb7-2" tabindex="-1"></a>yind <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb7-3"><a href="clustering-analysis.html#cb7-3" tabindex="-1"></a>(tt <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(iris[,<span class="sc">-</span>yind], <span class="at">modelNames=</span><span class="st">&quot;VVV&quot;</span>))</span></code></pre></div>
<pre><code>## &#39;Mclust&#39; model object: (VVV,2) 
## 
## Available components: 
##  [1] &quot;call&quot;           &quot;data&quot;           &quot;modelName&quot;      &quot;n&quot;             
##  [5] &quot;d&quot;              &quot;G&quot;              &quot;BIC&quot;            &quot;loglik&quot;        
##  [9] &quot;df&quot;             &quot;bic&quot;            &quot;icl&quot;            &quot;hypvol&quot;        
## [13] &quot;parameters&quot;     &quot;z&quot;              &quot;classification&quot; &quot;uncertainty&quot;</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="clustering-analysis.html#cb9-1" tabindex="-1"></a><span class="fu">plot</span>(tt,<span class="at">what=</span><span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p><img src="Plots/modelbasediris-1.png" width="65%" /></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="clustering-analysis.html#cb10-1" tabindex="-1"></a>tt<span class="sc">$</span>classification</span></code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [149] 2 2</code></pre>
<p>If <span class="math inline">\(\verb`VVV`\)</span> (varying volume, shape, and orientation) model is used, the optimal number of clusters is 2 based on the computer output. As we know that versicolor and virginica are more difficult to separate. The model-based clustering method classifies setosa as one cluster and versicolor and virginica as another cluster. If we use only the <span class="math inline">\(\verb`Petal.Length`\)</span> and <span class="math inline">\(\verb`Petal.Width`\)</span> as predictors, the optimal number of clusters is three which is correct.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="clustering-analysis.html#cb12-1" tabindex="-1"></a>(mb <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(iris[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">modelNames=</span><span class="st">&quot;VVV&quot;</span>))</span></code></pre></div>
<pre><code>## &#39;Mclust&#39; model object: (VVV,3) 
## 
## Available components: 
##  [1] &quot;call&quot;           &quot;data&quot;           &quot;modelName&quot;      &quot;n&quot;             
##  [5] &quot;d&quot;              &quot;G&quot;              &quot;BIC&quot;            &quot;loglik&quot;        
##  [9] &quot;df&quot;             &quot;bic&quot;            &quot;icl&quot;            &quot;hypvol&quot;        
## [13] &quot;parameters&quot;     &quot;z&quot;              &quot;classification&quot; &quot;uncertainty&quot;</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="clustering-analysis.html#cb14-1" tabindex="-1"></a><span class="fu">plot</span>(mb,<span class="at">what=</span><span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p><img src="Plots/vvviris-1.png" width="65%" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="clustering-analysis.html#cb15-1" tabindex="-1"></a><span class="fu">table</span>(iris[,yind],mb<span class="sc">$</span>classification)</span></code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa     50  0  0
##   versicolor  0 49  1
##   virginica   0  3 47</code></pre>
<p>The <span class="math inline">\(\texttt{modelNames}\)</span> parameter in the <span class="math inline">\(\texttt{Mclust}\)</span> function allows us to specify the type of model to fit. It is a character vector that can take different values representing combinations of component and covariance structures. Type <span class="math inline">\(\texttt{?mclustModelNames}\)</span> in the R console to retrieve the information on the available model names. The following models are available in package <span class="math inline">\(\texttt{mclust}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\textbf{Univariate Mixture}\)</span>
-âEâ: equal variance (one-dimensional)
-âVâ: varying/unequal variance (one-dimensional)</li>
</ol>
<p>2.<span class="math inline">\(\textbf{Multivariate Mixture}\)</span></p>
<ul>
<li>âEIIâ: Spherical, equal volume (variance).</li>
<li>âVIIâ: Spherical, varying/unequal volume (variance).</li>
<li>âEEIâ: Diagonal, equal volume and shape.</li>
<li>âVEIâ: Diagonal, varying volume, equal shape</li>
<li>âEVIâ: Diagonal, equal volume, varying shape.</li>
<li>âVVIâ: Diagonal, varying volume and shape.</li>
<li>âEEEâ: Ellipsoidal, equal volume, shape, and orientation.</li>
<li>âVEEâ: Ellipsoidal, equal shape and orientation.</li>
<li>âEVEâ: Ellipsoidal, equal volume and orientation.</li>
<li>âVVEâ: Ellipsoidal, equal orientation.</li>
<li>âEEVâ: Ellipsoidal, equal volume and equal shape.</li>
<li>âVEVâ: Ellipsoidal, equal shape.</li>
<li>âEVVâ: Ellipsoidal, equal volume.</li>
<li>âVVVâ: Ellipsoidal, varying volume, shape, and orientation.</li>
</ul>
<p>3.<span class="math inline">\(\textbf{Single Component}\)</span></p>
<ul>
<li>âXâ: univariate normal</li>
<li>âXIIâ: spherical multivariate normal</li>
<li>âXIIâ: diagonal multivariate normal</li>
<li>âXXXâ: ellipsoidal multivariate normal</li>
</ul>
</div>
</div>
<div id="pros-and-cons-of-clustering-methods" class="section level3 hasAnchor" number="9.2.4">
<h3><span class="header-section-number">9.2.4</span> Pros and Cons of Clustering Methods<a href="clustering-analysis.html#pros-and-cons-of-clustering-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following table summarizes some of the pros and cons of hierarchical clustering, <span class="math inline">\(K\)</span>-means and model-based clustering methods.
<span class="math display">\[
\begin{array}{c|c|c}
\hline
\text{Methods}&amp;\text{Pros}&amp;\text{Cons}\\
\hline
\text{Hierarchical}&amp;\text{no need to specify $K$}; &amp;\text{generally unique solution}\\
&amp;\text{dendrogram easy to interpret}; &amp;\text{expensive computation}\\
\hline
\text{$K$-means}&amp;\text{low computation cost}; &amp;\text{solution depends on initial value}\\
&amp;\text{intuitive}; &amp; \text{large variation}\\
\hline
\text{Model-Based}&amp;\text{MLE has good properties}; &amp;\text{results not valid if data do not} \\
&amp;\text{simultaneously determine $K$ and the centers of the clusters}&amp;\text{follow multivariate normal distribution}\\
\hline
\end{array}
\]</span></p>
</div>
</div>
<div id="determine-k-number-of-clusters" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Determine <span class="math inline">\(K\)</span>: Number of Clusters<a href="clustering-analysis.html#determine-k-number-of-clusters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The optimal number of clusters can be determined by several methods.</p>
<div id="the-elbow-plot-method" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> The Elbow Plot Method<a href="clustering-analysis.html#the-elbow-plot-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use the <span class="math inline">\(\verb`elbow plot`\)</span> (scree plot) to help identify the most proper value of <span class="math inline">\(K\)</span>. Recall the decomposition of the total variation of the observations (SST) into between-group variation (SSB) and within-group variation (SSW), i.e., <span class="math inline">\(SST=SSB+SSW\)</span>. If all objects are in one group, then <span class="math inline">\(SSW=SST\)</span>. The within-group variation drops when number of clusters <span class="math inline">\(K\)</span> increases. The <span class="math inline">\(\verb`elbow plot`\)</span> plots SSW versus # of groups, we choose the value of <span class="math inline">\(K\)</span> at where the plot starts flat. Recall the resulting dendrogram of clustering the observations <span class="math inline">\(x_1=5, x_2=3, x_3=9, x_4=6, x_5=1\)</span> using hierarchical clustering with average linkage is</p>
<p><img src="Plots/elbowplot-1.png" width="65%" /></p>
<p>To draw the scree plot, we calculate the within-cluster variation SSW for different values of <span class="math inline">\(K\)</span>:</p>
<ul>
<li>When <span class="math inline">\(K=2\)</span>, the two clusters are <span class="math inline">\(\{x_1, x_4, x_2, x_5\}\)</span> and <span class="math inline">\(\{x_3\}\)</span>. The centers for the two groups are <span class="math inline">\(m_1=\frac{5+6+3+1}{4}=3.75\)</span> and <span class="math inline">\(m_2=9\)</span>. And the
<span class="math display">\[
SSW=[(5-3.75)^2+(6-3.75)^2+(3-3.75)^2+(1-3.75)^2]+0=14.75
\]</span></li>
<li>When <span class="math inline">\(K=3\)</span>, the three clusters are <span class="math inline">\(\{x_1, x_4\}\)</span>,<span class="math inline">\(\{x_2, x_5\}\)</span> and <span class="math inline">\(\{x_3\}\)</span>. The centers for the three groups are <span class="math inline">\(m_1=\frac{5+6}{2}=5.5, m_2=\frac{3+1}{2}=2\)</span> and <span class="math inline">\(m_3=9\)</span>. And the
<span class="math display">\[
SSW=[(5-5.5)^2+(6-5.5)^2]+[(3-2)^2+(1-2)^2]+0=2.5
\]</span></li>
<li>When <span class="math inline">\(K=4\)</span>, the four clusters are <span class="math inline">\(\{x_1, x_4\}, \{x_2\}, \{x_5\}\)</span> and <span class="math inline">\(\{x_3\}\)</span>. The centers for the four groups are <span class="math inline">\(m_1=\frac{5+6}{2}=5.5, m_2=3, m_3=1\)</span> and <span class="math inline">\(m_4=9\)</span>. And the
<span class="math display">\[
SSW=[(5-5.5)^2+(6-5.5)^2]+0+0+0=0.5
\]</span></li>
</ul>
<p>The scree plot is given by</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="clustering-analysis.html#cb17-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>, <span class="fu">c</span>(<span class="fl">14.75</span>,<span class="fl">2.5</span>,<span class="fl">0.5</span>),<span class="at">xlab=</span><span class="st">&quot;K: # of Groups&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;SSW: Within Sum of Squares&quot;</span>, <span class="at">cex=</span><span class="fl">1.5</span>,<span class="at">type=</span><span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<p><img src="Plots/scree-1.png" width="65%" /></p>
<p>The scree plot shows that curve becomes flat at <span class="math inline">\(K=3\)</span>; therefore, <span class="math inline">\(K=3\)</span> is a proper number of clusters. We can calculate the within-group sum of squares using K-means:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="clustering-analysis.html#cb18-1" tabindex="-1"></a>kvec <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span></span>
<span id="cb18-2"><a href="clustering-analysis.html#cb18-2" tabindex="-1"></a>klen <span class="ot">&lt;-</span> <span class="fu">length</span>(kvec)</span>
<span id="cb18-3"><a href="clustering-analysis.html#cb18-3" tabindex="-1"></a>wss <span class="ot">&lt;-</span> <span class="fu">numeric</span>(klen)</span>
<span id="cb18-4"><a href="clustering-analysis.html#cb18-4" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>klen) {</span>
<span id="cb18-5"><a href="clustering-analysis.html#cb18-5" tabindex="-1"></a>  kmeans_result <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(x, <span class="at">centers =</span> kvec[i])</span>
<span id="cb18-6"><a href="clustering-analysis.html#cb18-6" tabindex="-1"></a>  wss[i] <span class="ot">&lt;-</span> kmeans_result<span class="sc">$</span>tot.withinss</span>
<span id="cb18-7"><a href="clustering-analysis.html#cb18-7" tabindex="-1"></a>}</span>
<span id="cb18-8"><a href="clustering-analysis.html#cb18-8" tabindex="-1"></a><span class="fu">plot</span>(kvec, wss, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Elbow Method&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Number of Clusters&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Within-cluster Sum of Squares&quot;</span>)</span></code></pre></div>
<p><img src="Plots/kscree2-1.png" width="65%" /></p>
</div>
<div id="the-silhouette-score" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> The Silhouette Score<a href="clustering-analysis.html#the-silhouette-score" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. The formula for the silhouette score for a single data point ii is given by:
<span class="math display">\[
S(i)=\frac{b(i)-a(i)}{\max\{a(i), b(i)\}}
\]</span>
where <span class="math inline">\(a(i)\)</span> is the average distance from the th data point to the other data points in the same cluster (cohesion), <span class="math inline">\(b(i)\)</span> is the smallest average distance from the th data point to data points in a different cluster, minimized over clusters (separation). The overall silhouette score for the clustering is the average of the silhouette scores for all data points. If the silhouette score is close to 1, it indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If itâs close to -1, it suggests that the object is poorly matched to its own cluster and well matched to neighboring clusters.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="clustering-analysis.html#cb19-1" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb19-2"><a href="clustering-analysis.html#cb19-2" tabindex="-1"></a>kvec <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span></span>
<span id="cb19-3"><a href="clustering-analysis.html#cb19-3" tabindex="-1"></a>klen <span class="ot">&lt;-</span> <span class="fu">length</span>(kvec)</span>
<span id="cb19-4"><a href="clustering-analysis.html#cb19-4" tabindex="-1"></a>ss <span class="ot">&lt;-</span> <span class="fu">numeric</span>(klen)</span>
<span id="cb19-5"><a href="clustering-analysis.html#cb19-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>klen) {</span>
<span id="cb19-6"><a href="clustering-analysis.html#cb19-6" tabindex="-1"></a>  kmeans_result <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(x, <span class="at">centers =</span> kvec[i])</span>
<span id="cb19-7"><a href="clustering-analysis.html#cb19-7" tabindex="-1"></a>  temp <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(kmeans_result<span class="sc">$</span>cluster, <span class="fu">dist</span>(x))</span>
<span id="cb19-8"><a href="clustering-analysis.html#cb19-8" tabindex="-1"></a>  obj <span class="ot">&lt;-</span> <span class="fu">summary</span>(temp)</span>
<span id="cb19-9"><a href="clustering-analysis.html#cb19-9" tabindex="-1"></a>  ss[i] <span class="ot">&lt;-</span> obj<span class="sc">$</span>avg.width</span>
<span id="cb19-10"><a href="clustering-analysis.html#cb19-10" tabindex="-1"></a>}</span>
<span id="cb19-11"><a href="clustering-analysis.html#cb19-11" tabindex="-1"></a><span class="fu">plot</span>(kvec, ss, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Silhouette Method&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Number of Clusters&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Average Silhouette Width&quot;</span>)</span></code></pre></div>
<p><img src="Plots/silho-1.png" width="65%" />
Since <span class="math inline">\(K=2\)</span> gives the largest silhouette score, the optimal number of clusters is two.</p>
</div>
<div id="gap-statistics" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Gap Statistics<a href="clustering-analysis.html#gap-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Gap statistics compare the clustering performance of the actual data with that of a reference dataset with no inherent clustering structure. It can be calculate as
<span class="math display">\[
\text{Gap}(k)=\frac{1}{B}\sum_{b=1}^B \log (W_b)-\log(W_k)
\]</span>
where <span class="math inline">\(W_k\)</span> is the total intra-cluster variation for the clustering solution with <span class="math inline">\(k\)</span> clusters, <span class="math inline">\(B\)</span> is the number of bootstrap samples drawn from the reference dataset, <span class="math inline">\(W_b\)</span> is the total intra-cluster variation for the <span class="math inline">\(\texttt{b}\)</span>th bootstrap sample.</p>
<p>The optimal number of clusters is often chosen as the value of <span class="math inline">\(k\)</span> that maximizes the Gap Statistics. The idea is that if the clustering solution for the actual data is better than the clustering solutions for random data, then the Gap Statistics will be positive, indicating a meaningful structure in the data for the chosen number of clusters.</p>
<p>Two built-in functions in R can be used to determine the optimal number of clusters:</p>
<ul>
<li><span class="math inline">\(\verb`fviz.nbclust`\)</span> function from package <span class="math inline">\(\verb`factoextra`\)</span>: using Elbow, Silhouhette and Gap statistic methods.</li>
<li><span class="math inline">\(\verb`NbClust`\)</span> function: voting of 30 indices for choosing the best number of clusters.</li>
</ul>
<p>Take the Iris data for example. We standardize the features before conducting clustering analysis.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="clustering-analysis.html#cb20-1" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb20-2"><a href="clustering-analysis.html#cb20-2" tabindex="-1"></a><span class="fu">library</span>(NbClust)</span>
<span id="cb20-3"><a href="clustering-analysis.html#cb20-3" tabindex="-1"></a><span class="co"># Elbow method</span></span>
<span id="cb20-4"><a href="clustering-analysis.html#cb20-4" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(<span class="fu">scale</span>(iris[,<span class="sc">-</span><span class="dv">5</span>]), kmeans, <span class="at">method =</span> <span class="st">&quot;wss&quot;</span>) <span class="sc">+</span></span>
<span id="cb20-5"><a href="clustering-analysis.html#cb20-5" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">4</span>, <span class="at">linetype =</span> <span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb20-6"><a href="clustering-analysis.html#cb20-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Elbow method&quot;</span>)</span></code></pre></div>
<p><img src="Plots/standardizeftrs-1.png" width="65%" /></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="clustering-analysis.html#cb21-1" tabindex="-1"></a><span class="co"># Silhouette method</span></span>
<span id="cb21-2"><a href="clustering-analysis.html#cb21-2" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(<span class="fu">scale</span>(iris[,<span class="sc">-</span><span class="dv">5</span>]), kmeans, <span class="at">method =</span> <span class="st">&quot;silhouette&quot;</span>)<span class="sc">+</span></span>
<span id="cb21-3"><a href="clustering-analysis.html#cb21-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Silhouette method&quot;</span>)</span></code></pre></div>
<p><img src="Plots/standardizeftrs-2.png" width="65%" /></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="clustering-analysis.html#cb22-1" tabindex="-1"></a><span class="co"># Gap statistic method</span></span>
<span id="cb22-2"><a href="clustering-analysis.html#cb22-2" tabindex="-1"></a><span class="co"># nboot = 50 to keep the function speedy. </span></span>
<span id="cb22-3"><a href="clustering-analysis.html#cb22-3" tabindex="-1"></a><span class="co"># recommended value: nboot= 500 for your analysis.</span></span>
<span id="cb22-4"><a href="clustering-analysis.html#cb22-4" tabindex="-1"></a><span class="co"># Use verbose = FALSE to hide computing progression.</span></span>
<span id="cb22-5"><a href="clustering-analysis.html#cb22-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb22-6"><a href="clustering-analysis.html#cb22-6" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(<span class="fu">scale</span>(iris[,<span class="sc">-</span><span class="dv">5</span>]), kmeans, <span class="at">nstart =</span> <span class="dv">25</span>,  <span class="at">method =</span> <span class="st">&quot;gap_stat&quot;</span>, <span class="at">nboot =</span> <span class="dv">50</span>)<span class="sc">+</span></span>
<span id="cb22-7"><a href="clustering-analysis.html#cb22-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Gap statistic method&quot;</span>)</span></code></pre></div>
<p><img src="Plots/standardizeftrs-3.png" width="65%" /></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="clustering-analysis.html#cb23-1" tabindex="-1"></a><span class="co">#voting of multiple indices</span></span>
<span id="cb23-2"><a href="clustering-analysis.html#cb23-2" tabindex="-1"></a>nb <span class="ot">&lt;-</span> <span class="fu">NbClust</span>(<span class="fu">scale</span>(iris[,<span class="sc">-</span><span class="dv">5</span>]), <span class="at">distance =</span> <span class="st">&quot;euclidean&quot;</span>,<span class="at">min.nc=</span><span class="dv">2</span>, <span class="at">max.nc=</span><span class="dv">10</span>, </span>
<span id="cb23-3"><a href="clustering-analysis.html#cb23-3" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;kmeans&quot;</span>,<span class="at">index =</span> <span class="st">&quot;all&quot;</span>)</span></code></pre></div>
<p><img src="Plots/multipleindices-1.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="Plots/multipleindices-2.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 11 proposed 2 as the best number of clusters 
## * 10 proposed 3 as the best number of clusters 
## * 1 proposed 4 as the best number of clusters 
## * 1 proposed 6 as the best number of clusters 
## * 1 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="clustering-analysis.html#cb26-1" tabindex="-1"></a>nb2 <span class="ot">&lt;-</span> <span class="fu">NbClust</span>(<span class="fu">scale</span>(iris[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>)]), <span class="at">distance =</span> <span class="st">&quot;euclidean&quot;</span>,<span class="at">min.nc=</span><span class="dv">2</span>, <span class="at">max.nc=</span><span class="dv">10</span>, </span>
<span id="cb26-2"><a href="clustering-analysis.html#cb26-2" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;kmeans&quot;</span>,<span class="at">index =</span> <span class="st">&quot;all&quot;</span>)</span></code></pre></div>
<p><img src="Plots/multipleindices-3.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="Plots/multipleindices-4.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 10 proposed 2 as the best number of clusters 
## * 10 proposed 3 as the best number of clusters 
## * 2 proposed 8 as the best number of clusters 
## * 2 proposed 9 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
<p>Again, the optimal number of clusters is two when using all four predictors while the optimal number of clusters is either two or three when using only and .</p>
</div>
</div>
<div id="side-note-on-the-em-algorithm" class="section level2 unnumbered hasAnchor">
<h2>Side-Note on the EM Algorithm<a href="clustering-analysis.html#side-note-on-the-em-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The main idea underlying the EM algorithm is as follows. Suppose we observe data partially. Let <span class="math inline">\(Y\)</span> presents the data we observed and <span class="math inline">\(Z\)</span> the missing data. Together, we have the complete data <span class="math inline">\(X=(Y, Z)\)</span>. Let <span class="math inline">\(g(y, z\mid \theta)\)</span> be the density function based on the complete data and <span class="math inline">\(f(y\mid \theta)\)</span> be the density function based on the observed data. Since <span class="math inline">\(z\)</span> is unobserved/latent, we cannot evaluate <span class="math inline">\(g(y, z\mid \theta)\)</span> using the regular maximum likelihood method. However, the density function based on the observed data can be calculated as
<span class="math display">\[
f(y\mid \theta)=\int g(y, z\mid \theta)dz,
\]</span>
and then it is straightforward to apply the MLE method to <span class="math inline">\(f(y\mid \theta)\)</span>.</p>
<p>Given this setup, the basic steps of the EM algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>E-step: Compute the expected value of the latent variables given the observed data (<span class="math inline">\(y\)</span>) and the current parameter estimates (<span class="math inline">\(\theta_t\)</span>) be the current estimate of <span class="math inline">\(\theta\)</span>. That is,
<span class="math display">\[
Q(\theta\mid\theta_t)=\mathbb{E}_Z\left[\log g(y,z\mid\theta)\mid y, \theta_t\right].
\]</span>
The expectation is taken with respect to the missing data density, i.e.,
<span class="math display">\[
h(z\mid y,\theta)=\frac{g(y,z\mid\theta)}{f(y\mid\theta)}.
\]</span></li>
<li>M-step: Maximize the expected log-likelihood <span class="math inline">\(Q(\theta\mid\theta_t)\)</span> with respect to <span class="math inline">\(\theta\)</span> to get the next value of <span class="math inline">\(\theta_{t+1}\)</span>.</li>
<li>Repeat the E-step and M-step until the parameters converge or until a predefined number of iterations is reached.</li>
</ol>
<p>One appealing property of the EM algorithm is that the likelihood function based on the observed data <span class="math inline">\(\log f(y|\theta)\)</span> always increases with each iteration, i.e.,
<span class="math display">\[
\log f(y|\theta_{t+1})-\log f(y|\theta_t)\ge 0.
\]</span></p>
<p><span class="math inline">\(\textbf{Proof}:\)</span>
<span class="math display">\[\begin{align*}
\log f(y\mid\theta_{t+1})-\log f(y\mid\theta_t)&amp; =
\log\int g(y,z\mid\theta_{t+1})dz - \log\int g(y,z\mid\theta_t)dz=\log\frac{\int g(y,z\mid\theta_{t+1})dz}{\int g(y,z\mid\theta_t)dz}\\
&amp; =\log\frac{\int g(y,z\mid\theta_{t})\frac{g(y,z\mid\theta_{t+1})}{g(y,z\mid\theta_t)}dz}{\int g(y,z\mid\theta_t)dz}=\log \int\frac{g(y,z\mid\theta_{t})}{\int g(y,z\mid\theta_{t})}\frac{g(y,z\mid\theta_{t+1})}{g(y,z\mid\theta_t)}dz\\
&amp;=\log \int \frac{g(y,z\mid\theta_{t+1})}{g(y,z\mid\theta_t)}h(z\mid y, \theta_t)dz=\log\mathbb{E}_{Z}\left [\frac{g(y,z\mid\theta_{t+1})}{g(y,z\mid\theta_t)}\right]\\
&amp;\ge \mathbb{E}\left[\log \frac{g(y,z\mid\theta_{t+1})}{g(y,z\mid\theta_t)} \right]=\mathbb{E}_{Z}\left[\log g(y,z\mid\theta_{t+1}\right]-\mathbb{E}_{Z}\left[\log g(y,z\mid\theta_{t}\right]\\
&amp;=Q(\theta_{t+1}\mid \theta_t)-Q(\theta_t\mid \theta_t)\ge 0.
\end{align*}\]</span></p>
<p>Here are several methods and techniques to diagnose convergence of the EM algorithm:</p>
<ul>
<li>Monitor the log-likelihood values across iterations. In the EM algorithm, the log-likelihood should increase for each iteration. If the log-likelihood stops increasing or becomes flat, it might indicate convergence. Set a tolerance threshold for the change in log-likelihood or parameter estimates. If the change falls below this threshold, consider the algorithm converged.</li>
<li>Use information criteria such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) to assess model fit. Lower values indicate better fit, and a stabilized or minimal change may indicate convergence.
<span class="math display">\[
\text{AIC}=-2\log(L)+2k; \quad \text{BIC}=-2\log(L)+k\log(n)
\]</span>
where <span class="math inline">\(L\)</span> is the maximum likelihood of the model, <span class="math inline">\(k\)</span> is the number of estimated parameters in the model, and <span class="math inline">\(n\)</span> is the sample size.</li>
<li>If applicable, run the EM algorithm with multiple initial values or chains. Compare the results across different runs to ensure consistency and convergence.</li>
</ul>
<p><span class="math inline">\(\textbf{Example}\)</span>: EM Algorithm for Gaussian Mixture Models</p>
<p>Suppose <span class="math inline">\(y_1, \cdots, y_n\)</span> are sampled independently from a mixture of two normal distributions with density:
<span class="math display">\[
f(y\mid\theta)=\lambda f_1(y\mid\mu_1,\sigma_1^2) + (1-\lambda)f_2(y\mid\mu_2,\sigma_2^2).
\]</span></p>
<ol style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(\lambda=0.4, \mu_1=0, \sigma_1=1, \mu_2=3, \sigma_2=1\)</span>, generate <span class="math inline">\(n=1000\)</span> observations from the Gaussian mixture model. Create a data frame with <span class="math inline">\(x\)</span> as the values and <span class="math inline">\(y\)</span> as the class label <span class="math inline">\(\{1, 2\}\)</span>.</li>
<li>Suppose we donât know the class label, we can use the EM algorithm to estimate the parameters, i.e., <span class="math inline">\(\lambda, \mu_i, \sigma_1=i, i=1, 2\)</span>.</li>
</ol>
<ul>
<li><p>[(2a)] If the class label is unknown, we need to introduce a latent variable <span class="math inline">\(Z\)</span> to indicate the class label: <span class="math inline">\(z=1\)</span> for class 1 and <span class="math inline">\(z=0\)</span> for class 2. The latent variable <span class="math inline">\(Z\)</span> can be modeled by a Bernoulli distribution, <span class="math inline">\(Z\sim \text{Bernoulli}(\lambda)\)</span>. Given the data <span class="math inline">\(x_1, \cdots, x_n\)</span> and class label <span class="math inline">\(z_1, \cdots, z_n\)</span>, write down the likelihood function of the completed data.
<span class="math display">\[
g(x,z\mid\theta)=
[f_1(x\mid\mu_1,\sigma_1^2)]^{z}[f_2(x\mid\mu_2,\sigma^2_2)]^{1-z}\lambda^z(1-\lambda)^{1-z}, \quad \text{with }z=0, 1.
\]</span>
It can be shown that the likelihood function based on the observed data is
<span class="math display">\[
f(x\mid\theta)=\sum _{z=0}^1[f_1(x\mid\mu_1,\sigma_1^2)]^{z}[f_2(x\mid\mu_2,\sigma^2_2)]^{1-z}\lambda^z(1-\lambda)^{1-z}.
\]</span>
And then the log-likelihood function based on the completed data is
<span class="math display">\[
\log g(x, z\mid\theta) =
\sum_{i=1}^n
[z_i\log f_1(x_i\mid\mu_1,\sigma^2_1) +
(1-z_i)\log f_2(x_i\mid\mu_2,\sigma^2_2) +
z_i\log\lambda +
(1-z_i)\log(1-\lambda)].
\]</span></p></li>
<li><p>[(2b)] E-step of the EM algorithm: calculate
\begin{align*}
Q()&amp;=<em>Z[g(x, z)]\
&amp;=\
&amp;=</em>{i=1}^n \
&amp;=_{i=1}^n
_if_1(x_i_1,_1^2)</p></li>
<li><p>(1-_i) f_2(x_i_2,_2^2)</p></li>
<li><p>_i</p></li>
<li><p>(1-<em>i)(1-)\
&amp; = </em>{i=1}^n
_i</p></li>
<li><p>(1-_i)+ _i\
&amp;+ (1-_i)(1-).
\end{align*}
Note that the conditional distribution of <span class="math inline">\(Z\)</span>
<span class="math display">\[
h(z\mid x,\theta) \propto [\lambda f_1(x\mid\mu_1,\sigma_1^2)]^z[(1-\lambda)f_2(x\mid\mu_2,\sigma_2^2)]^{1-z}\sim \text{Bernoulli}\left(\frac{\lambda f_1(x\mid\mu_1,\sigma_1^2)}{\lambda f_1(x\mid\mu_1,\sigma_1^2)+ (1-\lambda)f_2(x\mid\mu_2,\sigma_2^2)}
\right).
\]</span>
Therefore,
<span class="math display">\[
\pi_i=\mathbb{E}[z_i\mid x_i, \theta]=\frac{\lambda f_1(x\mid\mu_1,\sigma_1^2)}{\lambda f_1(x_i\mid\mu_1,\sigma_1^2)+ (1-\lambda)f_2(x_i\mid\mu_2,\sigma_2^2)}
\]</span></p></li>
<li><p>[(2c)] M-step of the EM algorithm: find the values of <span class="math inline">\(\theta\)</span> such that
<span class="math display">\[
Q(\theta)=\sum_{i=1}^n\pi_i\left[
-\frac{1}{2}\log 2\pi\sigma_1^2-\frac{1}{2\sigma_1^2}(x_i-\mu_1)^2
\right]+(1-\pi_i)\left[-\frac{1}{2}\log 2\pi\sigma_2^2-\frac{1}{2\sigma_2^2}(x_i-\mu_2)^2\right]+\pi_i\log\lambda+(1-\pi_i)\log(1-\lambda)
\]</span>
is maximized. Take the derivatives with respect to <span class="math inline">\(\lambda, \mu_1, \mu_2, \sigma_1^2, \sigma_2^2\)</span>, we obtain</p></li>
</ul>
<p><span class="math display">\[\begin{align*}
\frac{\partial Q}{\partial \lambda}&amp;=0\Longrightarrow \sum\left[\pi_i \frac{1}{\lambda}+(1-\pi_i)\frac{1}{1-\lambda}(-1)\right]\Longrightarrow \hat \lambda=\frac{\sum \pi_i}{n}\\
\frac{\partial Q}{\partial \mu_1}&amp;=0\Longrightarrow \sum \left[\pi_i (-\frac{1}{2\sigma_1^2})2(x_i-\mu_1)(-1)\right]=0\Longrightarrow \hat \mu_1 =\frac{\sum \pi_i x_i}{\sum \pi_i}\\
\frac{\partial Q}{\partial \mu_2}&amp;=0\Longrightarrow \sum \left[(1-\pi_i) (-\frac{1}{2\sigma_2^2})2(x_i-\mu_2)(-1)\right]=0\Longrightarrow \hat \mu_2 =\frac{\sum (1-\pi_i) x_i}{\sum (1-\pi_i)}\\
\frac{\partial Q}{\partial \sigma_1^2}&amp;=0\Longrightarrow \sum \left[\pi_i (0-\frac{1}{\sigma_1^2})-\frac{1}{2(\sigma_1^2)^2}(-1)(x_i-\mu_1)^2\right]=0\Longrightarrow \hat \sigma^2_1=\frac{\sum\pi_i(x_i-\mu_1)^2}{\sum\pi_i}\\
\frac{\partial Q}{\partial \sigma_2^2}&amp;=0\Longrightarrow \sum \left[(1-\pi_i) (0-\frac{1}{\sigma_2^2})-\frac{1}{2(\sigma_2^2)^2}(-1)(x_i-\mu_2)^2\right]=0\Longrightarrow \hat \sigma^2_2=\frac{\sum (1-\pi_i)(x_i-\mu_2)^2}{\sum(1-\pi_i)}
\end{align*}\]</span></p>
<p>We first generate observations from a mixture of two univariate normal distributions.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="clustering-analysis.html#cb29-1" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb29-2"><a href="clustering-analysis.html#cb29-2" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb29-3"><a href="clustering-analysis.html#cb29-3" tabindex="-1"></a>mu1 <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb29-4"><a href="clustering-analysis.html#cb29-4" tabindex="-1"></a>mu2 <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb29-5"><a href="clustering-analysis.html#cb29-5" tabindex="-1"></a>sigma1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb29-6"><a href="clustering-analysis.html#cb29-6" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb29-7"><a href="clustering-analysis.html#cb29-7" tabindex="-1"></a>n1 <span class="ot">&lt;-</span> n<span class="sc">*</span>lambda</span>
<span id="cb29-8"><a href="clustering-analysis.html#cb29-8" tabindex="-1"></a>n2 <span class="ot">&lt;-</span> n<span class="sc">-</span>n1</span>
<span id="cb29-9"><a href="clustering-analysis.html#cb29-9" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4061</span>)</span>
<span id="cb29-10"><a href="clustering-analysis.html#cb29-10" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n1, mu1, sigma1)</span>
<span id="cb29-11"><a href="clustering-analysis.html#cb29-11" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n2, mu2, sigma2)</span>
<span id="cb29-12"><a href="clustering-analysis.html#cb29-12" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">c</span>(x1,x2), <span class="at">y=</span><span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="fu">c</span>(n1, n2)))</span></code></pre></div>
<p>We can also use the built-in R function.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="clustering-analysis.html#cb30-1" tabindex="-1"></a><span class="fu">library</span>(mclust)</span>
<span id="cb30-2"><a href="clustering-analysis.html#cb30-2" tabindex="-1"></a>mclust_norm <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(df<span class="sc">$</span>x, <span class="at">G =</span> <span class="dv">2</span>) <span class="co">#two components</span></span>
<span id="cb30-3"><a href="clustering-analysis.html#cb30-3" tabindex="-1"></a><span class="fu">summary</span>(mclust_norm)</span></code></pre></div>
<pre><code>## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust E (univariate, equal variance) model with 2 components: 
## 
##  log-likelihood    n df       BIC       ICL
##       -1918.038 1000  4 -3863.706 -4014.206
## 
## Clustering table:
##   1   2 
## 400 600</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="clustering-analysis.html#cb32-1" tabindex="-1"></a>mclust_norm<span class="sc">$</span>parameters</span></code></pre></div>
<pre><code>## $pro
## [1] 0.4002299 0.5997701
## 
## $mean
##           1           2 
## -0.02290644  2.95847477 
## 
## $variance
## $variance$modelName
## [1] &quot;E&quot;
## 
## $variance$d
## [1] 1
## 
## $variance$G
## [1] 2
## 
## $variance$sigmasq
## [1] 0.9714779</code></pre>
<p>Here is another built-in R function to fit mixture Gaussian models.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="clustering-analysis.html#cb34-1" tabindex="-1"></a><span class="fu">library</span>(mixtools)</span>
<span id="cb34-2"><a href="clustering-analysis.html#cb34-2" tabindex="-1"></a>gmm <span class="ot">&lt;-</span> <span class="fu">normalmixEM</span>(df<span class="sc">$</span>x, <span class="at">k =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## number of iterations= 103</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="clustering-analysis.html#cb36-1" tabindex="-1"></a><span class="fu">summary</span>(gmm)</span></code></pre></div>
<pre><code>## summary of normalmixEM object:
##            comp 1   comp 2
## lambda 0.40899673 0.591003
## mu     0.00958599 2.980212
## sigma  1.01195669 0.967534
## loglik at estimate:  -1917.941</code></pre>
</div>
<div id="revisit-learning-outcomes-1" class="section level2 unnumbered hasAnchor">
<h2>Revisit Learning Outcomes<a href="clustering-analysis.html#revisit-learning-outcomes-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After finishing this chapter, students should be able to</p>
<ul>
<li>Explain the differences between classification and cluster problems.</li>
<li>Describe briefly the main idea and procedure of hierarchical clustering, <span class="math inline">\(K\)</span>-means, and model-based clustering methods.</li>
<li>Conduct a clustering analysis using hierarchical clustering method, <span class="math inline">\(K\)</span>-means, and model-based clustering in R.</li>
<li>Interpret the R computer outputs of hierarchical clustering, <span class="math inline">\(K\)</span>-means, and model-based clustering methods.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="discriminant-analysis-and-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="canonical-correlation-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
