<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Discriminant Analysis and Classification | STAT 372 Open Textbook (R)</title>
  <meta name="description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Discriminant Analysis and Classification | STAT 372 Open Textbook (R)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Discriminant Analysis and Classification | STAT 372 Open Textbook (R)" />
  
  <meta name="twitter:description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  

<meta name="author" content="Dr.Â Wanhua Su" />


<meta name="date" content="2025-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="factor-analysis.html"/>
<link rel="next" href="clustering-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#some-examples"><i class="fa fa-check"></i><b>2.2</b> Some Examples</a>
<ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-1-storm-survival-of-sparrows"><i class="fa fa-check"></i>Example 1: Storm Survival of Sparrows</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-2-spam-or-e-mail"><i class="fa fa-check"></i>Example 2: Spam or E-mail?</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-3-classification-of-iris"><i class="fa fa-check"></i>Example 3: Classification of Iris</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#multivariate-methods-covered-in-stat-372"><i class="fa fa-check"></i><b>2.3</b> Multivariate Methods Covered in STAT 372</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#review-univariate-analysis"><i class="fa fa-check"></i><b>2.4</b> Review: Univariate Analysis</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#random-variable-and-its-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Random Variable and Its Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>2.4.2</b> Properties of Expectation and Variance</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.4.3</b> Continuous Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#revisit-learning-learning-outcomes"><i class="fa fa-check"></i>Revisit Learning Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="matrix-algebra.html"><a href="matrix-algebra.html"><i class="fa fa-check"></i><b>3</b> Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="matrix-algebra.html"><a href="matrix-algebra.html#learning-outcomes-1"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="3.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#vectors"><i class="fa fa-check"></i><b>3.1</b> Vectors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#some-basic-operations-on-vectors"><i class="fa fa-check"></i><b>3.1.1</b> Some Basic Operations on Vectors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrices"><i class="fa fa-check"></i><b>3.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#basic-operations-on-matrix"><i class="fa fa-check"></i><b>3.2.1</b> Basic Operations on Matrix</a></li>
<li class="chapter" data-level="3.2.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.2</b> Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="3.2.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#spectral-eigen-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Spectral (Eigen) Decomposition</a></li>
<li class="chapter" data-level="3.2.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#singular-value-decomposition"><i class="fa fa-check"></i><b>3.2.4</b> Singular-Value Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#mean-vectors-and-covariance-matrices"><i class="fa fa-check"></i><b>3.3</b> Mean Vectors and Covariance Matrices</a></li>
<li class="chapter" data-level="3.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#sample-mean-vector-and-covariance-matrix"><i class="fa fa-check"></i><b>3.4</b> Sample Mean Vector and Covariance Matrix</a></li>
<li class="chapter" data-level="3.5" data-path="matrix-algebra.html"><a href="matrix-algebra.html#review-exercises"><i class="fa fa-check"></i><b>3.5</b> Review Exercises</a></li>
<li class="chapter" data-level="" data-path="matrix-algebra.html"><a href="matrix-algebra.html#revisit-the-learning-outcomes"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html"><i class="fa fa-check"></i><b>4</b> Displaying Multivariate Data and Measures of Distance</a>
<ul>
<li class="chapter" data-level="" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#learning-outcomes-2"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="4.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#display-multivariate-data"><i class="fa fa-check"></i><b>4.1</b> Display Multivariate Data</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#scatterplot"><i class="fa fa-check"></i><b>4.1.1</b> Scatterplot</a></li>
<li class="chapter" data-level="4.1.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#graphs-of-growth-curves"><i class="fa fa-check"></i><b>4.1.2</b> Graphs of Growth Curves</a></li>
<li class="chapter" data-level="4.1.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#star-plots"><i class="fa fa-check"></i><b>4.1.3</b> Star Plots</a></li>
<li class="chapter" data-level="4.1.4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#chernoff-faces-plot"><i class="fa fa-check"></i><b>4.1.4</b> Chernoff Faces Plot</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-in-multivariate-analysis"><i class="fa fa-check"></i><b>4.2</b> Distance in Multivariate Analysis</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distances-for-quantitative-variables"><i class="fa fa-check"></i><b>4.2.1</b> Distances for Quantitative Variables</a></li>
<li class="chapter" data-level="4.2.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-for-categorical-variables"><i class="fa fa-check"></i><b>4.2.2</b> Distance for Categorical Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-for-mixed-variable-types"><i class="fa fa-check"></i><b>4.2.3</b> Distance for Mixed Variable Types</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3</b> Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#properties-of-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Properties of Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="4.3.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#contour-of-multivariate-normal-distribution"><i class="fa fa-check"></i><b>4.3.3</b> Contour of Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#the-sampling-distribution-of-mathbfbar-x-and-boldsymbols"><i class="fa fa-check"></i><b>4.4</b> The Sampling Distribution of <span class="math inline">\(\mathbf{\bar X}\)</span> and <span class="math inline">\(\boldsymbol{S}\)</span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distributions-related-to-normal-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Distributions Related to Normal Distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#applications-to-distributions-related-to-sample-means"><i class="fa fa-check"></i><b>4.4.2</b> Applications to Distributions Related to Sample Means</a></li>
<li class="chapter" data-level="4.4.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#generalize-to-multivariate-cases"><i class="fa fa-check"></i><b>4.4.3</b> Generalize to Multivariate Cases</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#review-exercises-1"><i class="fa fa-check"></i><b>4.5</b> Review Exercises</a></li>
<li class="chapter" data-level="" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#revisit-the-learning-outcomes-1"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests on Mean Vectors</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#learning-outcomes-3"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="5.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-one-mean-vector"><i class="fa fa-check"></i><b>5.1</b> Hypothesis Test for one Mean Vector</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case"><i class="fa fa-check"></i><b>5.1.1</b> Univariate Case</a></li>
<li class="chapter" data-level="5.1.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case"><i class="fa fa-check"></i><b>5.1.2</b> Multivariate Case</a></li>
<li class="chapter" data-level="5.1.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#evaluating-multivariate-normality"><i class="fa fa-check"></i><b>5.1.3</b> Evaluating Multivariate Normality</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-two-mean-vectors"><i class="fa fa-check"></i><b>5.2</b> Hypothesis Test for Two Mean Vectors</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-two-independent-samples"><i class="fa fa-check"></i><b>5.2.1</b> Univariate Case Based on Two Independent Samples</a></li>
<li class="chapter" data-level="5.2.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-two-independent-samples"><i class="fa fa-check"></i><b>5.2.2</b> Multivariate Case Based on Two Independent Samples</a></li>
<li class="chapter" data-level="5.2.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#two-sample-non-pooled-hotellings-t2-test"><i class="fa fa-check"></i><b>5.2.3</b> Two-sample Non-pooled Hotellingâs <span class="math inline">\(T^2\)</span> Test</a></li>
<li class="chapter" data-level="5.2.4" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#two-sample-hotellings-t2-confidence-interval"><i class="fa fa-check"></i><b>5.2.4</b> Two-sample Hotellingâs <span class="math inline">\(T^2\)</span> Confidence Interval</a></li>
<li class="chapter" data-level="5.2.5" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-a-paired-sample"><i class="fa fa-check"></i><b>5.2.5</b> Univariate Case Based on a Paired Sample</a></li>
<li class="chapter" data-level="5.2.6" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-a-paired-sample"><i class="fa fa-check"></i><b>5.2.6</b> Multivariate Case Based on a Paired Sample</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-several-mean-vectors"><i class="fa fa-check"></i><b>5.3</b> Hypothesis Test for Several Mean Vectors</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-one-way-anova-f-test"><i class="fa fa-check"></i><b>5.3.1</b> Univariate Case: One-Way ANOVA F Test</a></li>
<li class="chapter" data-level="5.3.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-one-way-manova"><i class="fa fa-check"></i><b>5.3.2</b> Multivariate Case: One-Way MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#revisit-the-learning-outcomes-2"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>6</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#learning-outcomes-4"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="6.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#finding-the-principal-components"><i class="fa fa-check"></i><b>6.1</b> Finding the Principal Components</a></li>
<li class="chapter" data-level="6.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#scaling-in-principal-component-analysis"><i class="fa fa-check"></i><b>6.2</b> Scaling in Principal Component Analysis</a></li>
<li class="chapter" data-level="6.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#limitations-of-principal-component-analysis"><i class="fa fa-check"></i><b>6.3</b> Limitations of Principal Component Analysis</a></li>
<li class="chapter" data-level="6.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#further-reading"><i class="fa fa-check"></i><b>6.4</b> Further Reading</a></li>
<li class="chapter" data-level="" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#revisit-the-learning-outcomes-3"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>7</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="factor-analysis.html"><a href="factor-analysis.html#learning-outcomes-5"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="7.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-of-factor-analysis"><i class="fa fa-check"></i><b>7.1</b> Model of Factor Analysis</a></li>
<li class="chapter" data-level="7.2" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-loadings-l_ij-and-specific-variance-psi_i"><i class="fa fa-check"></i><b>7.2</b> Estimating Factor Loadings <span class="math inline">\(l_{ij}\)</span> and Specific Variance <span class="math inline">\(\psi_i\)</span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#the-principle-component-method"><i class="fa fa-check"></i><b>7.2.1</b> The Principle Component Method</a></li>
<li class="chapter" data-level="7.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#the-maximum-likelihood-method"><i class="fa fa-check"></i><b>7.2.2</b> The Maximum Likelihood Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-rotation"><i class="fa fa-check"></i><b>7.3</b> Factor Rotation</a></li>
<li class="chapter" data-level="7.4" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-scores"><i class="fa fa-check"></i><b>7.4</b> Factor Scores</a></li>
<li class="chapter" data-level="" data-path="factor-analysis.html"><a href="factor-analysis.html#revisit-the-learning-outcomes-4"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html"><i class="fa fa-check"></i><b>8</b> Discriminant Analysis and Classification</a>
<ul>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#learning-outcomes-6"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="8.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#introduction-1"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#performance-measure"><i class="fa fa-check"></i><b>8.2</b> Performance Measure</a></li>
<li class="chapter" data-level="8.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#overfitting-and-cross-validation"><i class="fa fa-check"></i><b>8.3</b> Overfitting and Cross Validation</a></li>
<li class="chapter" data-level="8.4" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classification-models"><i class="fa fa-check"></i><b>8.4</b> Classification Models</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.4.1</b> <span class="math inline">\(K\)</span> Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#logistic-regression-for-binary-response"><i class="fa fa-check"></i><b>8.5</b> Logistic Regression for Binary Response</a>
<ul>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#interpretation-of-beta_i"><i class="fa fa-check"></i>Interpretation of <span class="math inline">\(\beta_i\)</span></a></li>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#estimation-of-beta_i"><i class="fa fa-check"></i>Estimation of <span class="math inline">\(\beta_i\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#logistic-regression-for-multi-class-nominal-data"><i class="fa fa-check"></i><b>8.6</b> Logistic Regression for Multi-class Nominal Data</a></li>
<li class="chapter" data-level="8.7" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#cumulative-logit-model-for-multi-class-ordinal-data"><i class="fa fa-check"></i><b>8.7</b> Cumulative Logit Model for Multi-class Ordinal Data</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#cumulative-logit-models-with-proportional-odds"><i class="fa fa-check"></i><b>8.7.1</b> Cumulative Logit Models with Proportional Odds</a></li>
<li class="chapter" data-level="8.7.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-probability-of-each-category"><i class="fa fa-check"></i><b>8.7.2</b> Model Probability of Each Category</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-selection-for-logistic-regression"><i class="fa fa-check"></i><b>8.8</b> Model Selection for Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#aic-and-bic"><i class="fa fa-check"></i><b>8.8.1</b> AIC and BIC</a></li>
<li class="chapter" data-level="8.8.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#forward-selection"><i class="fa fa-check"></i><b>8.8.2</b> Forward Selection</a></li>
<li class="chapter" data-level="8.8.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#backward-elimination"><i class="fa fa-check"></i><b>8.8.3</b> Backward Elimination</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-checking"><i class="fa fa-check"></i><b>8.9</b> Model Checking</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#residual-analysis"><i class="fa fa-check"></i><b>8.9.1</b> Residual Analysis</a></li>
<li class="chapter" data-level="8.9.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#preditive-power-accuracy-and-roc-curve"><i class="fa fa-check"></i><b>8.9.2</b> Preditive Power: Accuracy and ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classification-tree-recursive-partitioning"><i class="fa fa-check"></i><b>8.10</b> Classification Tree (Recursive Partitioning)</a></li>
<li class="chapter" data-level="8.11" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#regression-tree"><i class="fa fa-check"></i><b>8.11</b> Regression Tree</a></li>
<li class="chapter" data-level="8.12" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#random-forest"><i class="fa fa-check"></i><b>8.12</b> Random Forest</a></li>
<li class="chapter" data-level="8.13" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#support-vector-machines"><i class="fa fa-check"></i><b>8.13</b> Support Vector Machines</a></li>
<li class="chapter" data-level="8.14" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#neural-networks"><i class="fa fa-check"></i><b>8.14</b> Neural Networks</a></li>
<li class="chapter" data-level="8.15" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classical-methods"><i class="fa fa-check"></i><b>8.15</b> Classical Methods</a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#mahalanobis-distance-method"><i class="fa fa-check"></i><b>8.15.1</b> Mahalanobis Distance Method</a></li>
<li class="chapter" data-level="8.15.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#bayes-posterior"><i class="fa fa-check"></i><b>8.15.2</b> Bayes Posterior</a></li>
<li class="chapter" data-level="8.15.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#fishers-discriminant-analysis"><i class="fa fa-check"></i><b>8.15.3</b> Fisherâs Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#summary"><i class="fa fa-check"></i><b>8.16</b> Summary</a></li>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#revisit-learning-outcomes"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="clustering-analysis.html"><a href="clustering-analysis.html"><i class="fa fa-check"></i><b>9</b> Clustering Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#learning-outcomes-7"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="9.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#introduction-2"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#clustering-methods"><i class="fa fa-check"></i><b>9.2</b> Clustering Methods</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#hierarchical-method"><i class="fa fa-check"></i><b>9.2.1</b> Hierarchical Method</a></li>
<li class="chapter" data-level="9.2.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#k-means"><i class="fa fa-check"></i><b>9.2.2</b> K-Means</a></li>
<li class="chapter" data-level="9.2.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#model-based-clustering"><i class="fa fa-check"></i><b>9.2.3</b> Model-Based Clustering</a></li>
<li class="chapter" data-level="9.2.4" data-path="clustering-analysis.html"><a href="clustering-analysis.html#pros-and-cons-of-clustering-methods"><i class="fa fa-check"></i><b>9.2.4</b> Pros and Cons of Clustering Methods</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#determine-k-number-of-clusters"><i class="fa fa-check"></i><b>9.3</b> Determine <span class="math inline">\(K\)</span>: Number of Clusters</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#the-elbow-plot-method"><i class="fa fa-check"></i><b>9.3.1</b> The Elbow Plot Method</a></li>
<li class="chapter" data-level="9.3.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#the-silhouette-score"><i class="fa fa-check"></i><b>9.3.2</b> The Silhouette Score</a></li>
<li class="chapter" data-level="9.3.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#gap-statistics"><i class="fa fa-check"></i><b>9.3.3</b> Gap Statistics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#side-note-on-the-em-algorithm"><i class="fa fa-check"></i>Side-Note on the EM Algorithm</a></li>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#revisit-learning-outcomes-1"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html"><i class="fa fa-check"></i><b>10</b> Canonical Correlation Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#learning-outcomes-8"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="10.1" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#objective"><i class="fa fa-check"></i><b>10.1</b> Objective</a></li>
<li class="chapter" data-level="10.2" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#obtain-the-canonical-variates-pairs"><i class="fa fa-check"></i><b>10.2</b> Obtain the Canonical Variates Pairs</a></li>
<li class="chapter" data-level="10.3" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#interpretation"><i class="fa fa-check"></i><b>10.3</b> Interpretation</a></li>
<li class="chapter" data-level="10.4" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#testing-mathbfsigma_120"><i class="fa fa-check"></i><b>10.4</b> Testing <span class="math inline">\(\mathbf{\Sigma}_{12}=0\)</span></a></li>
<li class="chapter" data-level="" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#revisit-learning-outcomes-2"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html"><i class="fa fa-check"></i><b>11</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#learning-outcomes-9"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="11.1" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#objective-1"><i class="fa fa-check"></i><b>11.1</b> Objective</a></li>
<li class="chapter" data-level="11.2" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#methods"><i class="fa fa-check"></i><b>11.2</b> Methods</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#classical-scaling"><i class="fa fa-check"></i><b>11.2.1</b> Classical Scaling</a></li>
<li class="chapter" data-level="11.2.2" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#metric-scaling"><i class="fa fa-check"></i><b>11.2.2</b> Metric Scaling</a></li>
<li class="chapter" data-level="11.2.3" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#non-metric-scaling"><i class="fa fa-check"></i><b>11.2.3</b> Non-metric Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#example"><i class="fa fa-check"></i><b>11.3</b> Example</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#revisit-learning-outcomes-3"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 372 Open Textbook (R)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discriminant-analysis-and-classification" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> Discriminant Analysis and Classification<a href="discriminant-analysis-and-classification.html#discriminant-analysis-and-classification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="learning-outcomes-6" class="section level2 unnumbered hasAnchor">
<h2>Learning Outcomes<a href="discriminant-analysis-and-classification.html#learning-outcomes-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After finishing this chapter, students should be able to</p>
<ul>
<li>Use proper performance metrics to compare classification methods.</li>
<li>Use cross-validation method to fit classification models.</li>
<li>Explain the main idea of each classification method covered: K-nearest neighbor, logistic regression, classification tree, random forests, neural network, support vector machines, and Fisherâs LDA and QDA.</li>
<li>Use R to fit the models listed above and interpret the computer outputs.</li>
</ul>
</div>
<div id="introduction-1" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Introduction<a href="discriminant-analysis-and-classification.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Discriminant analysis and classification is one of most important applications in multivariate analysis. In a discriminant analysis and classification, each observation consists of a vector of explanatory variables <span class="math inline">\(\mathbf{x}=\{x_1, x_2, \cdots, x_p\}\)</span> and one categorical response variable <span class="math inline">\(y\in \{1, 2, \cdots, k\}\)</span> indicating which class this observation belongs to. The main objectives of a classification problem is to build a discriminant function to separate the observations into <span class="math inline">\(k\)</span> categories and classify new observations to one of the <span class="math inline">\(k\)</span> classes.</p>
<p>Here are two examples:</p>
<ul>
<li>Classify the Iris flowers. There are three species, each flower has four measurements: sepal length and sepal width, petal length and petal width.</li>
<li>Classify emails and spams. The data set has 4601 messages and each message has 57 explanatory variables: frequencies of 48 words such as remove, you edu; frequencies of 6 characters such as $, ! and other 3 variable telling the average length of capital letters and total number of capital letters in the email. The question of interest is whether we could build a model to predict a messageâs probability of being a spam, i.e., a spam filter.</li>
</ul>
<p>The main idea of building the discriminant function is to model <span class="math inline">\(P(Y=j|\mathbf{x}), j=1, 2, \cdots, k\)</span> and assign the observation to the class yielding the largest probability.</p>
</div>
<div id="performance-measure" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Performance Measure<a href="discriminant-analysis-and-classification.html#performance-measure" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are many classification methods, when it comes to compare the performance of different methods, we use the misclassification rate which is defined as the proportion of objects that are classified to a wrong group. The misclassification table for a binary case can be presented as a 2<span class="math inline">\(\times\)</span> 2 table:</p>
<span class="math display">\[\begin{array}{|c|c|c|}
\hline
\text{True}&amp;\text{Classification result} &amp; \text{Classification result}\\

\text{label}&amp;1&amp;0\\
\hline
1&amp;TP \text{(true positive)} &amp; FN \text{(false negative)}\\
\hline
0&amp;FP \text{(false positive)}&amp;TN \text{(true negative)}\\
\hline
\end{array}\]</span>
<p>And the misclassification rate is defined as
<span class="math display">\[
\mbox{Misclassification Rate}=\frac{\mbox{FN}+\mbox{FP}}{\mbox{TP}+\mbox{FN}+\mbox{FP}+\mbox{TN}}.
%=1-\frac{\mbox{sum of diagonal elements}}{n},
\]</span>
The smaller the misclassification rate the better the classifier. The definition of the misclassification rate can be generalized to more than two classes.
<span class="math display">\[
\mbox{Misclassification Rate}=1-\mbox{accuracy}=1-\frac{\mbox{sum of diagonal elements}}{n},
\]</span>
where <span class="math inline">\(n=\mbox{FN}+\mbox{FP}+\mbox{TP}+\mbox{TN}\)</span> is the sample size. Misclassification rate, however, is not appropriate performance measure when the classes are extremely unbalanced. Take the following confusion table for example,</p>
<span class="math display">\[\begin{array}{|c|c|c|c|}
\hline
\text{True}&amp;\text{Classification Result}&amp;\text{Classification Result}\\

\text{label}&amp;1&amp;0&amp;\text{Total}\\
\hline
1&amp;30&amp;20&amp;50\\
\hline
0&amp;40&amp;1960&amp;2000\\
\hline
\text{Total}&amp;70&amp;1980&amp;2050\\
\hline
\end{array}\]</span>
<p>the misclassification rate for this classifier is
<span class="math display">\[
\mbox{MR}=\frac{\mbox{FN}+\mbox{FP}}{\mbox{TP}+\mbox{FN}+\mbox{FP}+\mbox{TN}}=\frac{40+20}{2050}=\frac{60}{2050}
\]</span>
which is even larger than the misclassification rate <span class="math inline">\(\frac{50}{2050}\)</span> if we simply classify all observations as class-0. The classifier, however, is useful in that it correctly classifies 30 out of 50 class-1 observations. Note that number of class-0 observations (<span class="math inline">\(n_0\)</span>) is much larger than the number of class-1 observations (<span class="math inline">\(n_1\)</span>) in this example; therefore, misclassification rate is not proper for this example.</p>
<p>Another popular performance metric for classification problem is the receiver operating characteristic (ROC) curve or more precisely, the area under the ROC curve (AUC). The ROC curve plots the true positive versus false positive. The following figure illustrates several typical ROC curves.</p>
<ul>
<li>Left panel: the model has no discriminant power, it cannot separate the two classes. The area under the ROC curve is close to 0.5. The model has the same effect as random ranking.</li>
<li>Middle panel: the model has a good discriminant power, it separates the two classes quite well. The area under the ROC curve is 0.8726.</li>
<li>Right panel: the model has a perfect discriminant power, it separates the two classes perfectly. The area under the ROC curve is close to 1.</li>
</ul>
<p><img src="Plots/roccurve-1.png" width="33%" /><img src="Plots/roccurve-2.png" width="33%" /><img src="Plots/roccurve-3.png" width="33%" />
The area under the ROC curve equals the probability of correctly ranking a random class-1 and class-0 pair. The larger the area the better. If a classifier assigns score to the subjects randomly or by guessing, the AUC is 0.5, any classifier with an AUC below 0.5 is useless. The perfect classifier has AUC 1.</p>
</div>
<div id="overfitting-and-cross-validation" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Overfitting and Cross Validation<a href="discriminant-analysis-and-classification.html#overfitting-and-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are some tuning parameters in a discriminant function needed to estimate using the data. We need to watch out for <em>overfitting</em> when fitting the models. It is easier to explain the idea of overfitting using regression models for a continuous variable. For example in Figure <a href="#fig:overfit"><strong>??</strong></a>, I want to model the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> based on the black dots. Which model is the best and how well does the model in predicting new observations? The first model is just a simple linear regression (a straight line), the second one is a quadratic regression with the <span class="math inline">\(x^2\)</span> term; the last one goes through all data points. The last model fits the black dots the best since all the data points are on the lines, however, it might not good for predicting new observations. As for a prediction, the quadratic regression does a better job, therefore, we say the first model underfits the data and the last one overfits the data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:overfit-1"></span>
<embed src="Plots/underfit.pdf" title="Overfitting/Underfitting Problem" width="33%" height="250px" type="application/pdf" />
<p class="caption">
Figure 8.1: Overfitting/Underfitting Problem
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:overfit-2"></span>
<embed src="Plots/quadratic.pdf" title="Overfitting/Underfitting Problem" width="33%" height="250px" type="application/pdf" />
<p class="caption">
Figure 8.2: Overfitting/Underfitting Problem
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:overfit-3"></span>
<embed src="Plots/overfit.pdf" title="Overfitting/Underfitting Problem" width="33%" height="250px" type="application/pdf" />
<p class="caption">
Figure 8.3: Overfitting/Underfitting Problem
</p>
</div>
<p>In order to avoid overfitting, we use the idea of <em>cross-validation</em> when we choose the optimal values of the parameters in the model. The key idea is to train or build the model using part of the data and evaluate using the hidden data. Choose the value of the parameter which gives the smallest cross-validation error rate. We will see one example later. If each time you remove only one observation, this is called leave-one-out cross-validation. If the data set is huge, leave-one-out is too time-consuming, you can use <span class="math inline">\(k\)</span>-fold cross-validation, and divide the data into <span class="math inline">\(k\)</span> folds with the same number of observations, the same distribution of the classes. Fit the model using the <span class="math inline">\(k-1\)</span> folds of the data and use the remaining fold for evaluation. Pick the one that gives the smallest error rate.</p>
</div>
<div id="classification-models" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Classification Models<a href="discriminant-analysis-and-classification.html#classification-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The classification models covered in this note are <span class="math inline">\(K\)</span> nearest neighbors (KNN), logistic regression, recursive partitioning (classification tree), random forests, neural network (NNet), and support vector machine (SVM).</p>
<div id="k-nearest-neighbors" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> <span class="math inline">\(K\)</span> Nearest Neighbors<a href="discriminant-analysis-and-classification.html#k-nearest-neighbors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assign the unlabeled point to the majority class among its <span class="math inline">\(k\)</span>-nearest neighbors. Euclidean distance is one measure to find the nearest neighbors.</p>
<p>Figure <a href="discriminant-analysis-and-classification.html#fig:knn">8.4</a> shows the basic idea of the KNN method. Suppose there are two classes: red triangle and blue square, want to assign the black circle to one of the two classes. When <span class="math inline">\(k=5\)</span>, among the five nearest neighbors, four of them are red triangles; therefore, <span class="math inline">\(P_{\mbox{red}}=\frac{4}{5}\)</span> and <span class="math inline">\(P_{\mbox{black}}=\frac{1}{5}\)</span>. As a result, we should classify the black circle as a red triangle.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:knn"></span>
<img src="Plots/knn.jpg" alt="Basic Idea of KNN" width="512px" height="415px" />
<p class="caption">
Figure 8.4: Basic Idea of KNN
</p>
</div>
<ul>
<li>Two classes: red triangle and blue square.</li>
<li>Need to assign the black circle to one of the two classes.</li>
<li>When <span class="math inline">\(k=5\)</span>, <span class="math inline">\(P_{\mathbf{red}}=\frac{4}{5}\)</span>.</li>
<li>When <span class="math inline">\(k=8\)</span>, <span class="math inline">\(P_{\mathbf{red}}=\frac{7}{8}\)</span>.</li>
<li>A cluster of blue square on upper-left corner.</li>
</ul>
<p>Cross validation can be used to choose the optimal value of <span class="math inline">\(k\)</span>: the number of nearest of neighbors. The algorithm (with leave-one-out cross validation) is as follows:</p>
<ol style="list-style-type: decimal">
<li>Split the data into training and testing sets using stratified sampling.</li>
<li>Loop over a grid of values of <span class="math inline">\(k\)</span>, the number of neighbors. For example, <span class="math inline">\(k=1, 3, 5, 7, 9, \cdots, n\)</span> (only consider odd values to avoid ties){</li>
<li>Loop over <span class="math inline">\(i=1, 2, \cdots, n\)</span> (all observations){</li>
<li>Predict <span class="math inline">\(y_i\)</span> with <span class="math inline">\(x_i\)</span> omitted using KNN}</li>
<li>Given the value of <span class="math inline">\(k\)</span>, find the <span class="math inline">\(k\)</span> nearest neighbors for each observation in the training set, and assign it to the majority class of its neighbors.</li>
<li>Calculate the misclassification rate for the current value of <span class="math inline">\(k\)</span> }</li>
<li>Plot error rate (or accuracy) versus <span class="math inline">\(k\)</span>, choose the value of <span class="math inline">\(k\)</span> that gives the smallest error rate.</li>
</ol>
<p>Take the Iris data for example, there are three species, 50 flowers for each species. Divide the data into training (80% i.e., 120 flowers, 40 from each species) and test sets (20%, 10 from each species) using stratified sampling. Fit the model on the training set, the plot shows that when <span class="math inline">\(k=6, 9, 11\)</span> or <span class="math inline">\(k=16\)</span> the KNN model gives the smallest error (or largest accuracy). R outputs <span class="math inline">\(k=16\)</span> as the optimal value of <span class="math inline">\(k\)</span>. In practice, we can try all the four optimal <span class="math inline">\(k\)</span> values and pick the one gives the largest accuracy. Apply the KNN method on the test data with <span class="math inline">\(k=16\)</span>, we obtain the confusion table and the resulting accuracy is <span class="math inline">\(\frac{10+9+10}{30}=0.97\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="discriminant-analysis-and-classification.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(caret) <span class="co"># Load the caret package</span></span>
<span id="cb1-2"><a href="discriminant-analysis-and-classification.html#cb1-2" tabindex="-1"></a><span class="fu">data</span>(iris) <span class="co"># Load the iris data set</span></span>
<span id="cb1-3"><a href="discriminant-analysis-and-classification.html#cb1-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> iris<span class="sc">$</span>Species <span class="co"># Set the response variable</span></span>
<span id="cb1-4"><a href="discriminant-analysis-and-classification.html#cb1-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> iris[,<span class="sc">-</span><span class="dv">5</span>] <span class="co"># Set the predictor variables</span></span>
<span id="cb1-5"><a href="discriminant-analysis-and-classification.html#cb1-5" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb1-6"><a href="discriminant-analysis-and-classification.html#cb1-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6194</span>)</span>
<span id="cb1-7"><a href="discriminant-analysis-and-classification.html#cb1-7" tabindex="-1"></a>splitIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(y, <span class="at">p =</span> .<span class="dv">8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb1-8"><a href="discriminant-analysis-and-classification.html#cb1-8" tabindex="-1"></a>training <span class="ot">&lt;-</span> x[splitIndex,] <span class="co"># training set</span></span>
<span id="cb1-9"><a href="discriminant-analysis-and-classification.html#cb1-9" tabindex="-1"></a>testing  <span class="ot">&lt;-</span> x[<span class="sc">-</span>splitIndex,] <span class="co"># test set</span></span>
<span id="cb1-10"><a href="discriminant-analysis-and-classification.html#cb1-10" tabindex="-1"></a>training_labels <span class="ot">&lt;-</span> y[splitIndex] <span class="co"># response variable in the training set</span></span>
<span id="cb1-11"><a href="discriminant-analysis-and-classification.html#cb1-11" tabindex="-1"></a>testing_labels <span class="ot">&lt;-</span> y[<span class="sc">-</span>splitIndex] <span class="co"># response variable in the test set</span></span>
<span id="cb1-12"><a href="discriminant-analysis-and-classification.html#cb1-12" tabindex="-1"></a><span class="fu">table</span>(training_labels) <span class="co">#stratified sampling for training and testing sets</span></span></code></pre></div>
<pre><code>## training_labels
##     setosa versicolor  virginica 
##         40         40         40</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="discriminant-analysis-and-classification.html#cb3-1" tabindex="-1"></a><span class="co"># Train the model using a grid search for best k</span></span>
<span id="cb3-2"><a href="discriminant-analysis-and-classification.html#cb3-2" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;cv&quot;</span>, <span class="at">number=</span><span class="dv">10</span>) <span class="co"># 10-fold cross-validation</span></span>
<span id="cb3-3"><a href="discriminant-analysis-and-classification.html#cb3-3" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">k =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>) <span class="co">#grid of k</span></span>
<span id="cb3-4"><a href="discriminant-analysis-and-classification.html#cb3-4" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">train</span>(training, training_labels, <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="at">tuneGrid =</span> grid, <span class="at">trControl =</span> ctrl)</span>
<span id="cb3-5"><a href="discriminant-analysis-and-classification.html#cb3-5" tabindex="-1"></a><span class="fu">plot</span>(model)</span></code></pre></div>
<p><img src="Plots/traintestiris-1.png" width="60%" /></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="discriminant-analysis-and-classification.html#cb4-1" tabindex="-1"></a><span class="fu">print</span>(model<span class="sc">$</span>bestTune<span class="sc">$</span>k) <span class="co"># Print the best k value</span></span></code></pre></div>
<pre><code>## [1] 16</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="discriminant-analysis-and-classification.html#cb6-1" tabindex="-1"></a><span class="co">#accuracy on the testing set</span></span>
<span id="cb6-2"><a href="discriminant-analysis-and-classification.html#cb6-2" tabindex="-1"></a>knnfit <span class="ot">&lt;-</span> <span class="fu">knn3Train</span>(training,testing, training_labels, <span class="at">k=</span>model<span class="sc">$</span>bestTune<span class="sc">$</span>k)</span>
<span id="cb6-3"><a href="discriminant-analysis-and-classification.html#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="discriminant-analysis-and-classification.html#cb6-4" tabindex="-1"></a>(cm <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">Predict=</span>knnfit, <span class="at">True=</span>testing_labels)) <span class="co"># confusion matrix</span></span></code></pre></div>
<pre><code>##             True
## Predict      setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          9         0
##   virginica       0          1        10</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="discriminant-analysis-and-classification.html#cb8-1" tabindex="-1"></a>accuracy <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(cm))<span class="sc">/</span><span class="fu">sum</span>(cm)</span>
<span id="cb8-2"><a href="discriminant-analysis-and-classification.html#cb8-2" tabindex="-1"></a><span class="fu">print</span>(accuracy) <span class="co"># Print the accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.9666667</code></pre>
</div>
</div>
<div id="logistic-regression-for-binary-response" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Logistic Regression for Binary Response<a href="discriminant-analysis-and-classification.html#logistic-regression-for-binary-response" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In multiple regression, We model the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x_1, x_2, \cdots, x_k\)</span>, <span class="math inline">\(\mu_{Y|x_1, x_2, \cdots, x_k}\)</span>, and its predictors <span class="math inline">\(x_1, x_2, \cdots, x_k\)</span> using a hyperplane, that is
<span class="math display">\[
\mu_{Y|x_1, x_2, \cdots, x_k}=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k
\]</span>
The meaning of <span class="math inline">\(\beta_0, \beta_1, \cdots, \beta_k\)</span>:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: the <span class="math inline">\(\textbf{mean response}\)</span> when <span class="math inline">\(x_1=0, x_2=0, \cdots, x_k=0\)</span>.</li>
<li><span class="math inline">\(\beta_i\)</span>: the change in the <span class="math inline">\(\textbf{mean response}\)</span> when <span class="math inline">\(x_i\)</span> <span class="math inline">\(\textbf{increases by 1 unit}\)</span> while keeping other predictor variables the same.</li>
</ul>
<p>Similar to the multiple regression, we would like to model <span class="math inline">\(P(Y=j|\mathbf{x})\)</span> using the explanatory variables <span class="math inline">\(\mathbf{x}=\{x_1, \cdots, x_p\}\)</span>. However, the domain of <span class="math inline">\(P(Y=j|\mathbf{x})\)</span> is <span class="math inline">\([0, 1]\)</span> which is bounded. The logistic regression models the log odd ratio and the explanatory variables using a regression model. We start with the binary case to introduce the idea. The logistic regression model for a binary response variable is given by
<span class="math display">\[
\ln \frac{P(Y=\text{setosa})}{P(Y=0)}=\ln \frac{P(Y=1)}{1-P(Y=1)}=\beta_0+\beta_1x_1+\cdots+\beta_px_p \Longrightarrow P(Y=1)=\frac{e^{\beta_0+\beta_1x_1+\cdots+\beta_px_p}}{1+e^{\beta_0+\beta_1x_1+\cdots+\beta_px_p}},
\]</span>
the observation is classified as <span class="math inline">\(y=1\)</span> if <span class="math inline">\(P(Y=1)\ge 0.5\)</span>.</p>
<div id="interpretation-of-beta_i" class="section level3 unnumbered hasAnchor">
<h3>Interpretation of <span class="math inline">\(\beta_i\)</span><a href="discriminant-analysis-and-classification.html#interpretation-of-beta_i" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The term <span class="math inline">\(\frac{P(Y=1)}{P(Y=0)}\)</span> is called the odds which is the expected number of successes per failure. Logistic regression models the log odds using linear regression terms <span class="math inline">\(\beta_0+\beta_1x_1+\cdots+\beta_px_p\)</span>. Consider a continuous variable <span class="math inline">\(x_j\)</span>, if its value increases by 1 unit from <span class="math inline">\(a\)</span> to <span class="math inline">\(a+1\)</span> while keeping all the other variables the same, we have
<span class="math display">\[
\ln \frac{P(Y=1|x_j=a+1)}{P(Y=0|x_j=a+1)} -\ln \frac{P(Y=1|x_j=a)}{P(Y=0|x_j=a)}=\beta_j\Longrightarrow \mbox{odds ratio}=e^{\beta_j}.
\]</span></p>
<ul>
<li><p>The sign of <span class="math inline">\(\beta_j\)</span> indicates whether the odds ratio increases or decreases. If <span class="math inline">\(\beta_j&gt;0\)</span>, the odds ratio increases as <span class="math inline">\(x_j\)</span> increases; if <span class="math inline">\(\beta_j&lt;0\)</span>, the odds ratio decreases as <span class="math inline">\(x_j\)</span> increases.</p></li>
<li><p>The magnitude of the coefficient <span class="math inline">\(|\beta_j|\)</span> represents the size of the effect of the predictor variable on the odds ratio. Larger magnitudes indicate a stronger influence.</p></li>
<li><p>Interpretation of <span class="math inline">\(\beta_j\)</span>:</p></li>
<li><p>If <span class="math inline">\(\beta_j&gt;0\)</span>, one unit increase in <span class="math inline">\(x_j\)</span> increases the odds ratio by <span class="math inline">\((e^{\beta_j}-1)\times 100\%\)</span> as all other variables remain the same.</p></li>
<li><p>If <span class="math inline">\(\beta_j&lt;0\)</span>, one unit increase in <span class="math inline">\(x_j\)</span> reduces the odds ratio by <span class="math inline">\((1-e^{\beta_j})\times 100\%\)</span> as all other variables remain the same.</p></li>
</ul>
</div>
<div id="estimation-of-beta_i" class="section level3 unnumbered hasAnchor">
<h3>Estimation of <span class="math inline">\(\beta_i\)</span><a href="discriminant-analysis-and-classification.html#estimation-of-beta_i" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The estimation of the regression coefficients <span class="math inline">\(\beta_0, \beta_1, \cdots, \beta_p\)</span> is based on the likelihood methods and maximum likelihood estimates are obtained using the Newton-Raphson method. Let <span class="math inline">\(p_i=P(Y_i=1)\)</span>, then <span class="math inline">\(Y_i\sim \mbox{Bernoulli}(p_i)\)</span> and the likelihood function for <span class="math inline">\(\beta\)</span>s is
<span class="math display">\[
L(\beta_0, \beta_1, \cdots, \beta_p; y_1, y_2, \cdots, y_n)=\prod_{i=1}^n p_i^{y_i}(1-y_i)^{1-y_i}
\]</span>
and the log-likelihood is
<span class="math display">\[
l(\beta_0, \beta_1, \cdots, \beta_p)=\sum_{i=1}^n y_i\ln(p_i)+\sum_{i=1}^n (1-y_i)\ln(1-p_i) \mbox{ with } p_i=\frac{e^{\beta_0+\beta_1x_1+\cdots+\beta_px_p}}{1+e^{\beta_0+\beta_1x_1+\cdots+\beta_px_p}}.
\]</span>
To obtain the maximum likelihood estimate of <span class="math inline">\(\beta_j\)</span> we need to take the partial derivative of <span class="math inline">\(l(\beta_0, \beta_1, \cdots, \beta_p)\)</span> with respect to <span class="math inline">\(\beta_j\)</span>, and find the solutions for the score equations:
<span class="math display">\[
\frac{\partial l}{\partial \beta_0}=0, \quad \frac{\partial l}{\partial \beta_1}=0, \quad \cdots, \quad \frac{\partial l}{\partial \beta_p}=0.
\]</span>
In general, there is no closed form solutions and Newton-Raphson method will be used to obtain the estimates of <span class="math inline">\(\beta\)</span>s.</p>
<p>Apply logistic regression on the Iris data with the last two species and the computer outputs are as follows.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="discriminant-analysis-and-classification.html#cb10-1" tabindex="-1"></a>iris2 <span class="ot">&lt;-</span> iris[iris<span class="sc">$</span>Species <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;virginica&quot;</span>, <span class="st">&quot;versicolor&quot;</span>), ] <span class="co">#use the last two species</span></span>
<span id="cb10-2"><a href="discriminant-analysis-and-classification.html#cb10-2" tabindex="-1"></a>iris2<span class="sc">$</span>Species <span class="ot">&lt;-</span> <span class="fu">droplevels</span>(iris2<span class="sc">$</span>Species) <span class="co">#drop the level with no observations</span></span>
<span id="cb10-3"><a href="discriminant-analysis-and-classification.html#cb10-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6194</span>) <span class="co"># Split the data into training and testing sets</span></span>
<span id="cb10-4"><a href="discriminant-analysis-and-classification.html#cb10-4" tabindex="-1"></a>splitIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(iris2<span class="sc">$</span>Species, <span class="at">p =</span> .<span class="dv">8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb10-5"><a href="discriminant-analysis-and-classification.html#cb10-5" tabindex="-1"></a>training <span class="ot">&lt;-</span> iris2[splitIndex,] <span class="co"># training set</span></span>
<span id="cb10-6"><a href="discriminant-analysis-and-classification.html#cb10-6" tabindex="-1"></a>testing  <span class="ot">&lt;-</span> iris2[<span class="sc">-</span>splitIndex,] <span class="co"># test set</span></span>
<span id="cb10-7"><a href="discriminant-analysis-and-classification.html#cb10-7" tabindex="-1"></a><span class="fu">table</span>(training<span class="sc">$</span>Species) <span class="co">#stratified sampling</span></span></code></pre></div>
<pre><code>## 
## versicolor  virginica 
##         40         40</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="discriminant-analysis-and-classification.html#cb12-1" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Species<span class="sc">~</span>.,data <span class="ot">&lt;-</span> training,<span class="at">family=</span><span class="st">&quot;binomial&quot;</span>) <span class="co">#generalized linear </span></span></code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<p>There are two warning messages</p>
<ol style="list-style-type: decimal">
<li>glm.fit: algorithm did not converge</li>
<li>glm.fit: fitted probabilities numerically 0 or 1 occurred</li>
</ol>
<p>The âdid not convergeâ issue can be resolved by increasing the number of maximum iterations.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="discriminant-analysis-and-classification.html#cb15-1" tabindex="-1"></a>mm0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Species<span class="sc">~</span>.,data <span class="ot">&lt;-</span> training,<span class="at">family=</span><span class="st">&quot;binomial&quot;</span>,<span class="at">maxit=</span><span class="dv">1000</span>) </span></code></pre></div>
<p>The second warning is generated when the generalized linear model (GLM) that you are fitting to your data has predicted probabilities that are equal to 0 or 1, which can cause problems in the optimization process. This issue can arise when you have a highly imbalanced response variable, or when there are predictor variables with high levels of multicollinearity, or when there are variable separating the two classes perfectly.</p>
<p>From the scatter plot matrix, the variables <span class="math inline">\(\texttt{Petal Length}\)</span> and <span class="math inline">\(\texttt{Petal Width}\)</span> can separate the two species very well. <span class="math inline">\(\texttt{Sepal Length}\)</span> and <span class="math inline">\(\texttt{Petal Length}\)</span> are highly correlated, so are <span class="math inline">\(\texttt{Petal Length}\)</span> and <span class="math inline">\(\texttt{Petal Width}\)</span>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="discriminant-analysis-and-classification.html#cb16-1" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb16-2"><a href="discriminant-analysis-and-classification.html#cb16-2" tabindex="-1"></a><span class="fu">ggpairs</span>(training[,<span class="sc">-</span><span class="dv">5</span>],<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">color =</span> training[,<span class="dv">5</span>]))</span></code></pre></div>
<p><img src="Plots/ggally-1.png" width="672" /></p>
<p>Remove the variable <span class="math inline">\(\texttt{Petal Length}\)</span> and refit the logistic model, we have.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="discriminant-analysis-and-classification.html#cb17-1" tabindex="-1"></a>logitfit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Species<span class="sc">~</span>.,<span class="at">data=</span>training[,<span class="sc">-</span><span class="dv">3</span>],<span class="at">family=</span><span class="st">&quot;binomial&quot;</span>) </span>
<span id="cb17-2"><a href="discriminant-analysis-and-classification.html#cb17-2" tabindex="-1"></a><span class="fu">summary</span>(logitfit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Species ~ ., family = &quot;binomial&quot;, data = training[, 
##     -3])
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.50433  -0.00325   0.00000   0.00257   1.78049  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -51.572     27.804  -1.855   0.0636 .
## Sepal.Length    2.901      2.200   1.318   0.1874  
## Sepal.Width    -9.146      6.411  -1.427   0.1537  
## Petal.Width    35.481     19.049   1.863   0.0625 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 110.904  on 79  degrees of freedom
## Residual deviance:  10.387  on 76  degrees of freedom
## AIC: 18.387
## 
## Number of Fisher Scoring iterations: 10</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="discriminant-analysis-and-classification.html#cb19-1" tabindex="-1"></a>logitpred <span class="ot">&lt;-</span> <span class="fu">predict</span>(logitfit,<span class="at">newdata=</span>testing[,<span class="sc">-</span><span class="dv">3</span>], <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb19-2"><a href="discriminant-analysis-and-classification.html#cb19-2" tabindex="-1"></a>(ltab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">Predit=</span><span class="fu">ifelse</span>(logitpred<span class="sc">&lt;</span><span class="fl">0.5</span>,<span class="st">&quot;versicolor&quot;</span>,<span class="st">&quot;virginica&quot;</span>),<span class="at">True=</span>testing<span class="sc">$</span>Species))</span></code></pre></div>
<pre><code>##             True
## Predit       versicolor virginica
##   versicolor         10         3
##   virginica           0         7</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="discriminant-analysis-and-classification.html#cb21-1" tabindex="-1"></a>(laccuracy <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(ltab))<span class="sc">/</span><span class="fu">sum</span>(ltab))</span></code></pre></div>
<pre><code>## [1] 0.85</code></pre>
<p><span class="math inline">\(\textbf{Example}\)</span>: Binary Logistic Regression for Admission Data</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="discriminant-analysis-and-classification.html#cb23-1" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/admission.csv&quot;</span>) <span class="co">#</span></span>
<span id="cb23-2"><a href="discriminant-analysis-and-classification.html#cb23-2" tabindex="-1"></a><span class="fu">head</span>(data0) <span class="co">#show the first six rows of the data set</span></span></code></pre></div>
<pre><code>##   admit gre  gpa prestige
## 1     0 380 3.61        3
## 2     1 660 3.67        3
## 3     1 800 4.00        1
## 4     1 640 3.19        4
## 5     0 520 2.93        4
## 6     1 760 3.00        2</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="discriminant-analysis-and-classification.html#cb25-1" tabindex="-1"></a><span class="fu">str</span>(data0) <span class="co">#admit is numerical, we need to change it to categorical</span></span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  4 variables:
##  $ admit   : int  0 1 1 1 0 1 1 0 1 0 ...
##  $ gre     : int  380 660 800 640 520 760 560 400 540 700 ...
##  $ gpa     : num  3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...
##  $ prestige: int  3 3 1 4 4 2 1 2 3 2 ...</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="discriminant-analysis-and-classification.html#cb27-1" tabindex="-1"></a>data <span class="ot">&lt;-</span> data0</span>
<span id="cb27-2"><a href="discriminant-analysis-and-classification.html#cb27-2" tabindex="-1"></a>data<span class="sc">$</span>admit <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>admit) <span class="co">#convert admit to a factor</span></span>
<span id="cb27-3"><a href="discriminant-analysis-and-classification.html#cb27-3" tabindex="-1"></a><span class="co">#fit a logistic regression, treat &quot;prestige&quot; as numerical</span></span>
<span id="cb27-4"><a href="discriminant-analysis-and-classification.html#cb27-4" tabindex="-1"></a>mlogit <span class="ot">&lt;-</span> <span class="fu">glm</span>(admit<span class="sc">~</span>.,<span class="at">data=</span>data,<span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb27-5"><a href="discriminant-analysis-and-classification.html#cb27-5" tabindex="-1"></a><span class="fu">summary</span>(mlogit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = admit ~ ., family = &quot;binomial&quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5694  -0.8871  -0.6374   1.1626   2.1672  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.340670   1.137622  -2.937  0.00332 ** 
## gre          0.002247   0.001092   2.059  0.03953 *  
## gpa          0.752857   0.328375   2.293  0.02187 *  
## prestige    -0.559515   0.127056  -4.404 1.06e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 496.15  on 396  degrees of freedom
## Residual deviance: 456.56  on 393  degrees of freedom
##   (3 observations deleted due to missingness)
## AIC: 464.56
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The fitted logistic regression is
<span class="math display">\[\begin{align*}
\ln \frac{P(admitted)}{P(not)}&amp;=\beta_0+\beta_1\times gre+\beta_2 \times gpa+\beta_3\times prestige\\
&amp;=-3.34067+0.002247\times gre+0.752857 \times gpa-0.559515\times prestige\\ &amp;\Longrightarrow \frac{P(admitted)}{P(not)}=e^{-3.34067+0.002247\times gre+0.752857 \times gpa-0.559515\tim es prestige}.
\end{align*}\]</span></p>
<p>All the three predictor variables are statistical significant at the 5% significance level. Interpretation of the coefficients of the fitted logistic regression: the <strong>odds ratio</strong>
<span class="math display">\[
\frac{\frac{P(admitted|gre=a+1)}{P(not)}}{\frac{P(admitted|gre=a)}{P(not)}}
\]</span>
equals to <span class="math inline">\(e^{0.002247}=1.00225\)</span> when the gre score increases by 1 given gpa and prestige remaining the same. This means that the odds of being admitted increase by a factor of 1.00225 or increase by <span class="math inline">\((1.00225-1)\times 100\%=0.225\%\)</span> when the gre score increases by 1 given gpa and prestige remaining the same. Similarly, the odds of being admitted increase by a factor of <span class="math inline">\(e^{0.752857}=2.123057\)</span> or increase by 112.3% when the gpa increases by 1 given gre score and prestige remaining the same; the odds of being admitted <strong>decreases</strong> by a factor of <span class="math inline">\(e^{0.559515}=1.749824\)</span> when the prestige increases by 1 (downgrade from the first rank to the second rank) given gre score and gpa remaining the same. The overall misclassificaiton rate is 0.2922.</p>
<p>Letâs predict the chance of being admitted for the first student who has a gre score 380, gpa 3.61 and graduated for 3rd class undergraduate institution.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="discriminant-analysis-and-classification.html#cb29-1" tabindex="-1"></a>pred1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(mlogit,data[<span class="dv">1</span>,],type <span class="ot">&lt;-</span> <span class="st">&quot;response&quot;</span>) <span class="co">#predict P(admit) for 1st student</span></span></code></pre></div>
<p>Since
<span class="math display">\[
\frac{P(admitted)}{1-P(admitted)}=e^{-3.34067+0.002247\times gre+0.752857 \times gpa-0.559515\times prestige} \Longrightarrow
\]</span>
<span class="math display">\[P(admitted)=\frac{e^{-3.34067+0.002247\times gre+0.752857 \times gpa-0.559515\times prestige}}{1+e^{-3.34067+0.002247\times gre+0.752857 \times gpa-0.559515\times prestige}}\]</span>
<span class="math display">\[=\frac{e^{-3.34067+0.002247\times 380+0.752857 \times 3.61-0.559515\times 3}}{1+e^{-3.34067+0.002247\times 380+0.752857 \times 3.61-0.559515\times 3}}=0.1904.\]</span></p>
<p>Since <span class="math inline">\(P(admitted)=0.1904&lt;0.5\)</span>, we predict this student as ânot admittedâ which is a correct decision since the first student was not admitted according to the data.</p>
<p>We can also confirm the probability of admission for the first student as follows.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="discriminant-analysis-and-classification.html#cb30-1" tabindex="-1"></a>bvec <span class="ot">&lt;-</span> mlogit<span class="sc">$</span>coefficients <span class="co">#the coefficients from the logistic regression</span></span>
<span id="cb30-2"><a href="discriminant-analysis-and-classification.html#cb30-2" tabindex="-1"></a>xvec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fu">as.numeric</span>(data[<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>]))</span>
<span id="cb30-3"><a href="discriminant-analysis-and-classification.html#cb30-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">sum</span>(xvec<span class="sc">*</span>bvec)</span>
<span id="cb30-4"><a href="discriminant-analysis-and-classification.html#cb30-4" tabindex="-1"></a>(p <span class="ot">&lt;-</span> <span class="fu">exp</span>(a)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(a))) <span class="co">#the same as pred1</span></span></code></pre></div>
<pre><code>## [1] 0.1903973</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="discriminant-analysis-and-classification.html#cb32-1" tabindex="-1"></a>pvec <span class="ot">&lt;-</span> <span class="fu">predict</span>(mlogit,data,type <span class="ot">&lt;-</span> <span class="st">&quot;response&quot;</span>) <span class="co">#predicted prob for all</span></span>
<span id="cb32-2"><a href="discriminant-analysis-and-classification.html#cb32-2" tabindex="-1"></a>tresult <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(pvec<span class="sc">&lt;</span><span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co">#convert prob to 1 (at least 0.5) or 0 (&lt;0.5)</span></span>
<span id="cb32-3"><a href="discriminant-analysis-and-classification.html#cb32-3" tabindex="-1"></a>(ltab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">True=</span>data[,<span class="dv">1</span>],<span class="at">Predict=</span>tresult)) <span class="co">#confusion table</span></span></code></pre></div>
<pre><code>##     Predict
## True   0   1
##    0 253  18
##    1  98  28</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="discriminant-analysis-and-classification.html#cb34-1" tabindex="-1"></a>(lmr <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">sum</span>(<span class="fu">diag</span>(ltab))<span class="sc">/</span><span class="fu">sum</span>(ltab)) <span class="co">#misclassification rate</span></span></code></pre></div>
<pre><code>## [1] 0.2921914</code></pre>
<p>Note that 0.5 is not always the proper cut-off. If the data is imbalanced, i.e., one class dominates, the relative frequency of class-1 observations is a better choice for the cut-off.</p>
</div>
</div>
<div id="logistic-regression-for-multi-class-nominal-data" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Logistic Regression for Multi-class Nominal Data<a href="discriminant-analysis-and-classification.html#logistic-regression-for-multi-class-nominal-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose the categorical response variable has <span class="math inline">\(c\)</span> levels, and arbitrarily let category 1 be the reference level. Logistic regression can be generalized to multi-class as follows.</p>
<ol style="list-style-type: decimal">
<li>Fit <span class="math inline">\(c-1\)</span> binary logistic regression
<span class="math display">\[
\ln \frac{P(Y=j)}{P(Y=1)}=\beta_0^{(j)}+\beta_1^{(j)}x_1+\cdots+\beta_p^{(j)}x_p=\mathbf{x}\mathbf{\beta}^{(j)} \Longrightarrow P(Y=j)=P(Y=1) e^{\mathbf{x} \mathbf{\beta}^{(j)}}, j=2, 3, \cdots, c.
\]</span>
That is
<span class="math display">\[
\begin{aligned}
P(Y=1)&amp;=P(Y=1)\\
P(Y=2)&amp;=P(Y=1)\times e^{\mathbf{x} \mathbf{\beta}^{(2)}}\\
P(Y=3)&amp;=P(Y=1)\times e^{\mathbf{x} \mathbf{\beta}^{(3)}}\\
\vdots&amp;=\vdots\\
P(Y=c)&amp;=P(Y=1)\times e^{\mathbf{x} \mathbf{\beta}^{(c)}}\\
\end{aligned}
\]</span>
Given the fact that <span class="math inline">\(\sum_{j=1}^c P(Y=j)=1\)</span>, we have
<span class="math display">\[
P(Y=1)(1+e^{\mathbf{x} \mathbf{\beta}^{(2)}}+e^{\mathbf{x} \mathbf{\beta}^{(3)}}+\cdots+e^{\mathbf{x} \mathbf{\beta}^{(c)}})=1\Longrightarrow P(Y=1)=\frac{1}{1+e^{\mathbf{x} \mathbf{\beta}^{(2)}+}e^{\mathbf{x} \mathbf{\beta}^{(3)}}+\cdots+e^{\mathbf{x} \mathbf{\beta}^{(c)}}}.
\]</span>
Therefore, we can solve for
<span class="math display">\[
P(Y=j)=\frac{e^{\mathbf{x} \mathbf{\beta}^{(j)}}}{1+e^{\mathbf{x} \mathbf{\beta}^{(2)}+}e^{\mathbf{x} \mathbf{\beta}^{(3)}}+\cdots+e^{\mathbf{x} \mathbf{\beta}^{(c)}}}, j=2, \cdots, c.
\]</span></li>
<li>Classify the observation to the class with the largest probability.</li>
</ol>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="discriminant-analysis-and-classification.html#cb36-1" tabindex="-1"></a><span class="fu">library</span>(nnet)</span>
<span id="cb36-2"><a href="discriminant-analysis-and-classification.html#cb36-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6194</span>)</span>
<span id="cb36-3"><a href="discriminant-analysis-and-classification.html#cb36-3" tabindex="-1"></a>splitIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(iris<span class="sc">$</span>Species, <span class="at">p =</span> .<span class="dv">8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb36-4"><a href="discriminant-analysis-and-classification.html#cb36-4" tabindex="-1"></a>training <span class="ot">&lt;-</span> iris[ splitIndex,] <span class="co"># training set</span></span>
<span id="cb36-5"><a href="discriminant-analysis-and-classification.html#cb36-5" tabindex="-1"></a>testing  <span class="ot">&lt;-</span> iris[<span class="sc">-</span>splitIndex,] <span class="co"># test set</span></span>
<span id="cb36-6"><a href="discriminant-analysis-and-classification.html#cb36-6" tabindex="-1"></a><span class="fu">table</span>(training<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## 
##     setosa versicolor  virginica 
##         40         40         40</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="discriminant-analysis-and-classification.html#cb38-1" tabindex="-1"></a>mlogit <span class="ot">&lt;-</span> <span class="fu">multinom</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> training, <span class="at">trace=</span>F)</span>
<span id="cb38-2"><a href="discriminant-analysis-and-classification.html#cb38-2" tabindex="-1"></a><span class="fu">summary</span>(mlogit)</span></code></pre></div>
<pre><code>## Call:
## multinom(formula = Species ~ ., data = training, trace = F)
## 
## Coefficients:
##            (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width
## versicolor    25.18619     -2.19761   -15.64359     12.00310   -1.846649
## virginica    -47.58622    -55.41464   -58.71412     98.52301   57.943819
## 
## Std. Errors:
##            (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width
## versicolor    625.4250     234.2197    320.0780     30.32423    207.0105
## virginica     247.3652     171.1546    368.0706    223.88557    180.1804
## 
## Residual Deviance: 1.154569 
## AIC: 21.15457</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="discriminant-analysis-and-classification.html#cb40-1" tabindex="-1"></a>training[<span class="dv">1</span>,]</span></code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa</code></pre>
<p>Based on the coefficients of <span class="math inline">\(\texttt{mlogit}\)</span>, we have <span class="math display">\[\ln \frac{P(Y=\text{versicolor})}{P(Y=\text{setosa})}=25.18619-2.19761x_1-15.64359x_2+12.00310x_3-1.846649x_4 \quad \Longrightarrow\]</span> <span class="math display">\[P(Y=\text{versicolor})=P(Y=\text{setosa})e^{25.18619-2.19761x_1-15.64359x_2+12.00310x_3-1.846649x_4}=P(Y=\text{setosa})e^{a_{21}},\]</span>
where <span class="math inline">\(x_1=\text{Sepal Length}, x_2=\text{Sepal Width}, x_1=\text{Petal Length}, x_2=\text{Petal Width}\)</span>.</p>
<p>Similarly,
<span class="math display">\[\ln \frac{P(Y=\text{virginica})}{P(Y=\text{setosa})}=-47.58622-55.41464x_1-58.71412x_2+98.52301x_3+57.943819x_4 \quad \Longrightarrow\]</span> <span class="math display">\[P(Y=\text{virginica})=P(Y=\text{setosa})e^{-47.58622-55.41464x_1-58.71412x_2+98.52301x_3+57.943819x_4}=P(Y=\text{setosa})e^{a_{31}}.\]</span></p>
<p>Given the fact that <span class="math inline">\(P(Y=\text{setosa})+P(Y=\text{versicolor})+P(Y=\text{virginica})=1\)</span>, we have <span class="math display">\[P(Y=\text{setosa})+P(Y=\text{versicolor})+P(Y=\text{virginica})=P(Y=\text{setosa})+P(Y=\text{setosa})e^{a_{21}}+P(Y=\text{setosa})e^{a_{31}}=1 \Longrightarrow\]</span>
<span class="math display">\[P(Y=\text{setosa})=\frac{1}{1+e^{a_{21}}+e^{a_{31}}}, \quad P(Y=\text{versicolor})=\frac{e^{a_{21}}}{1+e^{a_{21}}+e^{a_{31}}}, \quad P(Y=\text{virginica})=\frac{e^{a_{31}}}{1+e^{a_{21}}+e^{a_{31}}}.\]</span>
Assign the observation to the class giving the largest probability. For example, the first flower in the training set has <span class="math inline">\(x_1=5.1, x_2=3.5, x_3=1.4, x_4=0.2\)</span>,
<span class="math display">\[
\begin{aligned}
a_{21}&amp;=25.18619-2.19761x_1-15.64359x_2+12.00310x_3-1.846649x_4 \\
&amp;=25.18619-2.19761(5.1)-15.64359(3.5)+12.00310(1.4)-1.846649(0.2) =-24.33918.
\end{aligned}
\]</span>
Similarly,
<span class="math display">\[
\begin{aligned}
a_{31}&amp;=-47.58622-55.41464x_1-58.71412x_2+98.52301x_3+57.943819x_4\\
&amp;=-47.58622-55.41464(5.1)-58.71412(3.5)+98.52301(1.4)+57.943819(0.2)=-386.1793.
\end{aligned}
\]</span></p>
<p>Therefore,
<span class="math display">\[
P(Y=\text{setosa})=\frac{1}{1+e^{a_{21}}+e^{a_{31}}}=\frac{1}{1+e^{-24.33918}+e^{-386.1793}}=1;
\]</span>
and
<span class="math display">\[
P(Y=\text{versicolor})=\frac{e^{a_{21}}}{1+e^{a_{21}}+e^{a_{31}}}=\frac{e^{-24.33918}}{1+e^{-24.33918}+e^{-386.1793}}=2.689191\times 10^{-11};
\]</span>
and
<span class="math display">\[
P(Y=\text{virginica})=\frac{e^{a_{31}}}{1+e^{a_{21}}+e^{a_{31}}}=\frac{e^{-386.1793}}{1+e^{-24.33918}+e^{-386.1793}}=1.925122\times 10^{-168}.
\]</span>
Since <span class="math inline">\(P(Y=\text{setosa})&gt;P(Y=\text{versicolor})&gt;P(Y=\text{virginica})\)</span>, we classify the first flower as setosa, which turns out to be a correct decision.</p>
<p>The accuracy of multi-class logistic regression on the testing set is 0.93333 which is slightly worst than that of KNN.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="discriminant-analysis-and-classification.html#cb42-1" tabindex="-1"></a>mlogitp <span class="ot">&lt;-</span> <span class="fu">predict</span>(mlogit,testing)</span>
<span id="cb42-2"><a href="discriminant-analysis-and-classification.html#cb42-2" tabindex="-1"></a>(mltab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">Predict=</span>mlogitp,<span class="at">True=</span>testing<span class="sc">$</span>Species))</span></code></pre></div>
<pre><code>##             True
## Predict      setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          9         1
##   virginica       0          1         9</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="discriminant-analysis-and-classification.html#cb44-1" tabindex="-1"></a>(<span class="at">accuracy=</span><span class="fu">sum</span>(<span class="fu">diag</span>(mltab))<span class="sc">/</span><span class="fu">sum</span>(mltab))</span></code></pre></div>
<pre><code>## [1] 0.9333333</code></pre>
</div>
<div id="cumulative-logit-model-for-multi-class-ordinal-data" class="section level2 hasAnchor" number="8.7">
<h2><span class="header-section-number">8.7</span> Cumulative Logit Model for Multi-class Ordinal Data<a href="discriminant-analysis-and-classification.html#cumulative-logit-model-for-multi-class-ordinal-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When the response variable is ordinal with <span class="math inline">\(c\)</span> levels, we have
<span class="math display">\[
P(Y\le j)=P(Y=1)+P(Y=2)+\cdots+P(Y=j)=p_1+p_2+\cdots+p_j, j=1, 2, \cdots, c.
\]</span>
The cumulative probabilities reflect the ordering:
<span class="math display">\[
P(Y\le 1)\le P(Y\le 2)\le \cdots \le P(Y\le c)=1.
\]</span>
The cumulative logits are given by
<span class="math display">\[
\text{logit}[P(Y\le j)]=\ln\frac{P(Y\le j)}{1-P(Y\le j)}=\ln \frac{p_1+p_2+\cdots+p_j}{p_{j+1}+\cdots+p_c}, j=1, 2, \cdots, c-1.
\]</span>
Take <span class="math inline">\(c=3\)</span> for example, the two cumulative logits are
<span class="math display">\[
\text{logit}[P(Y \le 1)]=\ln \frac{p_1}{p_2+p_3}; \quad \text{logit}[P(Y\le 2)]=\ln \frac{p_1+p_2}{p_3}.
\]</span>
For cumulative logit model, we donât need to model <span class="math inline">\(P(Y\le c)\)</span> since <span class="math inline">\(P(Y\le c)=1\)</span>.</p>
<div id="cumulative-logit-models-with-proportional-odds" class="section level3 hasAnchor" number="8.7.1">
<h3><span class="header-section-number">8.7.1</span> Cumulative Logit Models with Proportional Odds<a href="discriminant-analysis-and-classification.html#cumulative-logit-models-with-proportional-odds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A cumulative logit model for level <span class="math inline">\(j\)</span> can be treated as a binary logistic regression model in which categories 1 to <span class="math inline">\(j\)</span> combine to form one category and categories <span class="math inline">\(j+1\)</span> to <span class="math inline">\(c\)</span> form the other. The cumulative logit model with only one explanatory variable <span class="math inline">\(x\)</span> is given by
<span class="math display">\[
\text{logit}[P(Y \le j)]=\alpha_j+\beta x, j=1, 2, \cdots, c-1.
\]</span>
The parameter <span class="math inline">\(\beta\)</span> quantifies the effect of <span class="math inline">\(x\)</span> on the log odds of resulting in category <span class="math inline">\(j\)</span> or below; and the effect is the same for all <span class="math inline">\(j=1, 2, \cdots, c-1\)</span>.</p>
<p>The interpretation of the coefficients of the fitted proportional odds regression model is as follows:</p>
<ul>
<li><p>Interpretation of <span class="math inline">\(\alpha_j\)</span>. For a fixed value of <span class="math inline">\(x\)</span>,
<span class="math display">\[
\text{logit} P(Y\le j|x=a)]-\text{logit} P(Y \le i|x=a)=(\alpha_j+\beta a)-(\alpha_i+\beta a)=\alpha_j-\alpha_i, j&gt;i
\]</span>
which implies
<span class="math display">\[
\frac{P(Y\le j)/P(Y&gt;j)}{P(Y\le i)/P(Y&gt;i)}=e^{a_j-a_i}.
\]</span>
That is the odds of resulting in category <span class="math inline">\(j\)</span> or below is <span class="math inline">\(e^{\alpha_j-\alpha_i}\)</span> times of the odds of resulting in category <span class="math inline">\(i\)</span> or below given that <span class="math inline">\(x\)</span> is the same.</p></li>
<li><p>Interpretation of <span class="math inline">\(\beta\)</span>. With one unit increase in <span class="math inline">\(x\)</span>,
<span class="math display">\[
\text{logit} P(Y\le j|x=a+1)-\text{logit} P(Y\le j|x=a)=[\alpha_j+\beta (a+1)]-[\alpha_j+\beta a]=\beta
\]</span>
which implies
<span class="math display">\[
\frac{P(Y\le j|x=a+1)/P(Y&gt;j|x=a+1)}{P(Y\le j|x=a)/P(Y&gt;j|x=a)}=e^{\beta}.
\]</span>
This means the odds of resulting in category <span class="math inline">\(j\)</span> or below increases by <span class="math inline">\((e^{\beta}-1)\times 100\%\)</span> when <span class="math inline">\(x\)</span> increases by 1 unit. The change is the same for each category <span class="math inline">\(j\)</span>; this property is called <em>proportional odds</em>.</p></li>
</ul>
<p>If <span class="math inline">\(\beta&gt;0\)</span> then the probability to fall into a lower category increases with increasing <span class="math inline">\(x\)</span>. This is against the usual interpretation, positive slope is associated with a positive correlation. For this reason, some statistical software models <span class="math inline">\(\text{logit}(P(Y\le j))=\alpha_j-\beta x\)</span> and reports the negative of the slope. Make sure that you read the documentation of the built-in functions. For example in R, <span class="math inline">\(\texttt{polr}\)</span> function in package <span class="math inline">\(\texttt{MASS}\)</span> reports the negative of the slope. However, the <span class="math inline">\(\texttt{vglm}\)</span> function in package <span class="math inline">\(\texttt{VGAM}\)</span> reports the positive of the slope.</p>
<p>In general, logistic regression for ordinal response is given by
<span class="math display">\[
\text{logit}[P(Y \le j)]=\alpha_j+\beta_1 x_1+\beta_2x_2+\cdots+\beta_kx_k, j=1, 2, \cdots, c-1,
\]</span>
the intercept <span class="math inline">\(\alpha\)</span> varies but the slopes <span class="math inline">\(\beta_i\)</span>s are the same for all categories <span class="math inline">\(j=1, 2, \cdots c-1\)</span>. This model has <span class="math inline">\((c-1)+k\)</span> parameters, much smaller than the number of parameters of the baseline logistic regression for nominal response which uses different intercept and slopes <span class="math inline">\(\beta_i\)</span>s for each categories <span class="math inline">\(j=2, \cdots c\)</span> given that <span class="math inline">\(c=1\)</span> is the reference level. The number of parameters in a baseline logistic regression model is <span class="math inline">\((c-1)(k+1)\)</span>.</p>
</div>
<div id="model-probability-of-each-category" class="section level3 hasAnchor" number="8.7.2">
<h3><span class="header-section-number">8.7.2</span> Model Probability of Each Category<a href="discriminant-analysis-and-classification.html#model-probability-of-each-category" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given that
<span class="math display">\[
\text{logit}[P(Y \le j)]=\alpha_j+\beta_1 x_1+\beta_2x_2+\cdots+\beta_kx_k, j=1, 2, \cdots, c-1,
\]</span>
we have
<span class="math display">\[
P(Y \le j)=\frac{e^{\alpha_j+\beta_1 x_1+\beta_2x_2+\cdots+\beta_kx_k}}{1+e^{\alpha_j+\beta_1 x_1+\beta_2x_2+\cdots+\beta_kx_k}}, j=1, 2, \cdots, c-1.
\]</span>
Therefore, the probability of each category can be calculated as
<span class="math display">\[
P(Y=j)=P(Y\le j)-P(Y\le j-1), j=2, 3, \cdots, c.
\]</span>
Note that <span class="math inline">\(P(Y=1)=P(Y\le 1)\)</span>. We assign the observation to the category with the largest probability, i.e.,
<span class="math display">\[
\hat y=\text{argmax}_j P(Y=j).
\]</span></p>
<p><span class="math inline">\(\textbf{Example}\)</span>: Proportional Odds Model for Ordinal Response</p>
<p>Consider the heart disease data with variables</p>
<ul>
<li>age : Age of the patient</li>
<li>sex : Sex of the patient (1 = male; 0 = female)</li>
<li>cp : Chest Pain type
<ul>
<li>Value 0: asymptomatic</li>
<li>Value 1: non-anginal pain</li>
<li>Value 2: atypical angina</li>
<li>Value 3: typical angina</li>
</ul></li>
<li>trtbps : resting blood pressure (in mm Hg)<br />
</li>
<li>chol : cholesterol in mg/dl fetched via BMI sensor</li>
<li>fbs : (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)</li>
<li>restecg : resting electrocardiographic results
<ul>
<li>Value 0: normal</li>
<li>Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)</li>
<li>Value 2: showing probable or definite left ventricular hypertrophy by Estesâ criteria</li>
</ul></li>
<li>thalach : maximum heart rate achieved<br />
</li>
<li>exang: exercise induced angina (1 = yes; 0 = no)</li>
<li>oldpeak: ST depression induced by exercise relative to rest. ST relates to positions on the ECG plot.</li>
<li>slp: the slope of the peak exercise ST segment
<ul>
<li>Value 1: upsloping</li>
<li>Value 2: flat</li>
<li>Value 3: downsloping</li>
</ul></li>
<li>ca: number of major vessels (0-3)</li>
<li>thal: A blood disorder called thalassemia
<ul>
<li>Value 3: normal blood flow</li>
<li>Value 6: fixed defect (no blood flow in some part of the heart)</li>
<li>Value 7: reversible defect (a blood flow is observed but it is not normal)</li>
</ul></li>
<li>output : heart disease (0-4, 0=no presence)</li>
</ul>
<p>Fit a proportional odds model on the response variable <span class="math inline">\(\texttt{output}\)</span> which has five levels (0-4) using <span class="math inline">\(\texttt{sex}\)</span> and number of major vessels with blockage <span class="math inline">\(\texttt{ca}\)</span> as the predictor variables.</p>
<p>We can use the <span class="math inline">\(\texttt{polr}\)</span> function in R package <span class="math inline">\(\texttt{MASS}\)</span> to fit a proportional odds logistic regression. Note that the <span class="math inline">\(\texttt{polr}\)</span> function reports negative of the slope.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="discriminant-analysis-and-classification.html#cb46-1" tabindex="-1"></a>hdf <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/heart_disease.csv&quot;</span>)</span>
<span id="cb46-2"><a href="discriminant-analysis-and-classification.html#cb46-2" tabindex="-1"></a>hdf<span class="sc">$</span>sex <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hdf<span class="sc">$</span>sex) <span class="co">#convert integer to factor</span></span>
<span id="cb46-3"><a href="discriminant-analysis-and-classification.html#cb46-3" tabindex="-1"></a>hdf<span class="sc">$</span>exang <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hdf<span class="sc">$</span>exang)</span>
<span id="cb46-4"><a href="discriminant-analysis-and-classification.html#cb46-4" tabindex="-1"></a>hdf<span class="sc">$</span>cp <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hdf<span class="sc">$</span>cp)</span>
<span id="cb46-5"><a href="discriminant-analysis-and-classification.html#cb46-5" tabindex="-1"></a>hdf<span class="sc">$</span>restecg <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hdf<span class="sc">$</span>restecg)</span>
<span id="cb46-6"><a href="discriminant-analysis-and-classification.html#cb46-6" tabindex="-1"></a>hdf<span class="sc">$</span>fbs <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hdf<span class="sc">$</span>fbs)</span>
<span id="cb46-7"><a href="discriminant-analysis-and-classification.html#cb46-7" tabindex="-1"></a>hdf<span class="sc">$</span>slp <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hdf<span class="sc">$</span>slp)</span>
<span id="cb46-8"><a href="discriminant-analysis-and-classification.html#cb46-8" tabindex="-1"></a>hdf<span class="sc">$</span>thal <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hdf<span class="sc">$</span>thal)</span>
<span id="cb46-9"><a href="discriminant-analysis-and-classification.html#cb46-9" tabindex="-1"></a><span class="co">#hdf$output &lt;- as.factor(hdf$output)</span></span>
<span id="cb46-10"><a href="discriminant-analysis-and-classification.html#cb46-10" tabindex="-1"></a><span class="fu">head</span>(hdf) <span class="co">#show the first 6 observations</span></span></code></pre></div>
<pre><code>##   age sex cp trtbps chol fbs restecg thalachh exang oldpeak slp ca thal output
## 1  63   1  3    145  233   1       2      150     0     2.3   3  0    6      0
## 2  67   1  0    160  286   0       2      108     1     1.5   2  3    3      2
## 3  67   1  0    120  229   0       2      129     1     2.6   2  2    7      1
## 4  37   1  1    130  250   0       0      187     0     3.5   3  0    3      0
## 5  41   0  2    130  204   0       2      172     0     1.4   1  0    3      0
## 6  56   1  2    120  236   0       0      178     0     0.8   1  0    3      0</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="discriminant-analysis-and-classification.html#cb48-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb48-2"><a href="discriminant-analysis-and-classification.html#cb48-2" tabindex="-1"></a>pofit <span class="ot">&lt;-</span> <span class="fu">polr</span>(<span class="fu">as.factor</span>(output)<span class="sc">~</span>sex<span class="sc">+</span>ca,<span class="at">data=</span>hdf,<span class="at">Hess=</span>T) <span class="co">#return the Hessian matrix</span></span>
<span id="cb48-3"><a href="discriminant-analysis-and-classification.html#cb48-3" tabindex="-1"></a><span class="fu">summary</span>(pofit)</span></code></pre></div>
<pre><code>## Call:
## polr(formula = as.factor(output) ~ sex + ca, data = hdf, Hess = T)
## 
## Coefficients:
##      Value Std. Error t value
## sex1 1.079     0.2782   3.877
## ca   1.151     0.1359   8.466
## 
## Intercepts:
##     Value   Std. Error t value
## 0|1  1.6434  0.2614     6.2858
## 1|2  2.7185  0.2943     9.2386
## 2|3  3.6417  0.3267    11.1475
## 3|4  5.3086  0.4260    12.4608
## 
## Residual Deviance: 665.1239 
## AIC: 677.1239 
## (4 observations deleted due to missingness)</code></pre>
<p>The fitted model is
<span class="math display">\[
\begin{aligned}
\text{logit} P(Y\le 0)&amp;=1.6434-1.079 x_1-1.151 x_2\\
\text{logit} P(Y\le 1)&amp;=2.7185-1.079 x_1-1.151 x_2\\
\text{logit} P(Y\le 2)&amp;=3.6417-1.079 x_1-1.151 x_2\\
\text{logit} P(Y\le 3)&amp;=5.3086-1.079 x_1-1.151 x_2\\
\end{aligned}
\]</span>
with
<span class="math display">\[
x_1=\left\{
\begin{array}{ll}
1&amp;\mbox{Male},\\
0&amp;\mbox{Female}.
\end{array}
\right. \quad x_2=\text{Ca (number of major vessels with blockage)}.
\]</span></p>
<p>The label <span class="math inline">\(\texttt{0|1}\)</span> shows the fact that the odds corresponding to <span class="math inline">\(\frac{P(Y\le 0)}{P(Y&gt;0)}\)</span> which is equivalent to <span class="math inline">\(\frac{P(Y\le 0)}{P(Y\ge 1)}\)</span>. Similarly, the label <span class="math inline">\(\texttt{1|2}\)</span> corresponds to <span class="math inline">\(\frac{P(Y\le 1)}{P(Y&gt;1)}=\frac{P(Y\le 1)}{P(Y\ge 2)}\)</span>. The same rule applies to all remaining labels.</p>
<p>We can also get confidence intervals for the parameter estimates. These can be obtained either by profiling the likelihood function or by using the standard errors and assuming a normal distribution. Note that profiled CIs are not symmetric (although they are usually close to symmetric). If the 95% CI does not cross 0, the parameter estimate is statistically significant.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="discriminant-analysis-and-classification.html#cb50-1" tabindex="-1"></a>ci <span class="ot">&lt;-</span> <span class="fu">confint</span>(pofit)</span>
<span id="cb50-2"><a href="discriminant-analysis-and-classification.html#cb50-2" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(pofit), ci))</span></code></pre></div>
<pre><code>##            OR    2.5 %   97.5 %
## sex1 2.940876 1.725483 5.152109
## ca   3.159960 2.432623 4.147272</code></pre>
<p><strong>Interpretation of the coefficients</strong>:</p>
<ul>
<li><p><span class="math inline">\(\hat \beta_1=1.079\)</span>: the odds of heart disease (i.e., 1-4 versus 0) for males is 2.94 times of the odds for females while holding <span class="math inline">\(\texttt{Ca}\)</span> constant.</p></li>
<li><p><span class="math inline">\(\hat \beta_2=1.151\)</span>: For every one unit increase in Ca (number of major vessels with blockage) the odds of heart disease (1-4 versus 0) is multiplied 3.16 times (i.e., increases 216%) while holding constant <span class="math inline">\(\texttt{sex}\)</span> variable.</p></li>
</ul>
<p><strong>Probability of Each Category</strong></p>
<p>We choose several subjects to predict their categories of heart disease status (0-4).</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="discriminant-analysis-and-classification.html#cb52-1" tabindex="-1"></a>(testdf <span class="ot">&lt;-</span> hdf[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">7</span>),<span class="fu">c</span>(<span class="st">&quot;sex&quot;</span>,<span class="st">&quot;ca&quot;</span>,<span class="st">&quot;output&quot;</span>)])</span></code></pre></div>
<pre><code>##   sex ca output
## 1   1  0      0
## 2   1  3      2
## 5   0  0      0
## 7   0  2      3</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="discriminant-analysis-and-classification.html#cb54-1" tabindex="-1"></a><span class="fu">predict</span>(pofit,testdf,<span class="at">type=</span><span class="st">&quot;p&quot;</span>) <span class="co">#probability of each category for each individual</span></span></code></pre></div>
<pre><code>##           0          1          2          3          4
## 1 0.6375414 0.19996170 0.09094055 0.05721049 0.01434581
## 2 0.0528015 0.08760571 0.15098063 0.39389758 0.31471458
## 5 0.8379993 0.10010868 0.03635438 0.02061294 0.00492470
## 7 0.3412560 0.26159419 0.18974070 0.16031809 0.04709106</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="discriminant-analysis-and-classification.html#cb56-1" tabindex="-1"></a><span class="fu">predict</span>(pofit,testdf) <span class="co">#return the category label</span></span></code></pre></div>
<pre><code>## [1] 0 3 0 0
## Levels: 0 1 2 3 4</code></pre>
<p>For the first subject who is a male (<span class="math inline">\(x_1=1\)</span>) and has no major vessel with blockage (<span class="math inline">\(x_2=0\)</span>), his probability of heart disease status is
<span class="math display">\[
\begin{aligned}
P(Y\le 0)&amp;=\frac{e^{1.6434-1.079 x_1-1.151 x_2}}{1+e^{1.6434-1.079 x_1-1.151 x_2}}=\frac{e^{1.6434-1.079\times 1-1.151\times 0}}{1+e^{1.6434-1.079\times 1-1.151\times 0}}=0.6375\\
P(Y\le 1)&amp;=\frac{e^{2.7185-1.079 x_1-1.151 x_2}}{1+e^{2.7185-1.079 x_1-1.151 x_2}}=\frac{e^{2.7185-1.079 \times 1-1.151 \times 0}}{1+e^{2.7185-1.079 \times 1-1.151 \times 0}}=0.8375\\
P(Y\le 2)&amp;=\frac{e^{3.6417-1.079 x_1-1.151 x_2}}{1+e^{3.6417-1.079 x_1-1.151 x_2}}=\frac{e^{3.6417-1.079 \times 1-1.151 \times 0}}{1+e^{3.6417-1.079 \times 1-1.151 \times 0}}=0.9284\\
P(Y\le 3)&amp;=\frac{e^{5.3086-1.079 x_1-1.151 x_2}}{1+e^{5.3086-1.079 x_1-1.151 x_2}}=\frac{e^{5.3086-1.079 \times 1-1.151 \times 0}}{1+e^{5.3086-1.079 \times 1-1.151 \times 0}}=0.9857\\
\end{aligned}
\]</span>
which gives
<span class="math display">\[
\begin{aligned}
P(Y=0)&amp;=P(Y\le 0)=0.6375\\
P(Y=1)&amp;=P(Y\le 1)-P(Y\le 0)=0.8375-0.6375=0.2\\
P(Y=2)&amp;=P(Y\le 2)-P(Y\le 1)=0.9284-0.8375=0.0909\\
P(Y=3)&amp;=P(Y\le 3)-P(Y\le 2)=0.9857-0.9284=0.0573\\
P(Y=4)&amp;=P(Y\le 4)-P(Y\le 3)=1-0.9857=0.0143\\
\end{aligned}
\]</span>
Since <span class="math inline">\(P(Y=0)\)</span> has the largest value, we predict the first subjectâs disease status as 0, which is a correct prediction. Similarly, for the 4th subject who is a female (<span class="math inline">\(x_1=0\)</span>) with 2 major vessels blocked (<span class="math inline">\(x_2=2\)</span>), her probability of each disease status can be calculated as
<span class="math display">\[
\begin{aligned}
P(Y\le 0)&amp;=\frac{e^{1.6434-1.079 x_1-1.151 x_2}}{1+e^{1.6434-1.079 x_1-1.151 x_2}}=\frac{e^{1.6434-1.079\times 0-1.151\times 2}}{1+e^{1.6434-1.079\times 0-1.151\times 2}}=0.3411\\
P(Y\le 1)&amp;=\frac{e^{2.7185-1.079 x_1-1.151 x_2}}{1+e^{2.7185-1.079 x_1-1.151 x_2}}=\frac{e^{2.7185-1.079 \times 0-1.151 \times 2}}{1+e^{2.7185-1.079 \times 0-1.151 \times 2}}=0.6026\\
P(Y\le 2)&amp;=\frac{e^{3.6417-1.079 x_1-1.151 x_2}}{1+e^{3.6417-1.079 x_1-1.151 x_2}}=\frac{e^{3.6417-1.079 \times 0-1.151 \times 2}}{1+e^{3.6417-1.079 \times 0-1.151 \times 2}}=0.7924\\
P(Y\le 3)&amp;=\frac{e^{5.3086-1.079 x_1-1.151 x_2}}{1+e^{5.3086-1.079 x_1-1.151 x_2}}=\frac{e^{5.3086-1.079 \times 0-1.151 \times 2}}{1+e^{5.3086-1.079 \times 0-1.151 \times 2}}=0.9529\\
\end{aligned}
\]</span>
which gives
<span class="math display">\[
\begin{aligned}
P(Y=0)&amp;=P(Y\le 0)=0.3411\\
P(Y=1)&amp;=P(Y\le 1)-P(Y\le 0)=0.6026-0.3411=0.2615\\
P(Y=2)&amp;=P(Y\le 2)-P(Y\le 1)=0.7924-0.6026=0.1898\\
P(Y=3)&amp;=P(Y\le 3)-P(Y\le 2)=0.9529-0.7924=0.1605\\
P(Y=4)&amp;=P(Y\le 4)-P(Y\le 3)=1-0.9529=0.0471\\
\end{aligned}
\]</span>
Since <span class="math inline">\(P(Y=0)\)</span> has the largest value, we predict the 4th subjectâs disease status as 0 which turns out to be an error, the true disease status is 3. This might be due to the fact that only <span class="math inline">\(\texttt{sex}\)</span> and <span class="math inline">\(\texttt{ca}\)</span> are used to fit the model.</p>
</div>
</div>
<div id="model-selection-for-logistic-regression" class="section level2 hasAnchor" number="8.8">
<h2><span class="header-section-number">8.8</span> Model Selection for Logistic Regression<a href="discriminant-analysis-and-classification.html#model-selection-for-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Similar multiple linear regression, we can use forward selection and backward elimination methods to choose the âbestâ subset of predictor variables to include in the logistic regression model. The Akaike information criterion (AIC) and the Bayesian Information Criterion (BIC) can be used to determine the âbestâ GLM.</p>
<p>Consider two nested models, one of which is more complicate than the other. The more complicate model has more terms and might fit the data closely, i.e., it has smaller bias but larger variance. On the other hand, the simpler model has larger bias but smaller variance. Therefore, it is not necessarily better to select the more complicate model. The best is in the sense that we only include those important variables. Ideally we would like to have a logistic regression model that is simple but still able to capture most of the variation in the response variable. Because a simple model with fewer predictor variable is easy to interpret and maintains reasonable accuracy when applied to new data.</p>
<div id="aic-and-bic" class="section level3 hasAnchor" number="8.8.1">
<h3><span class="header-section-number">8.8.1</span> AIC and BIC<a href="discriminant-analysis-and-classification.html#aic-and-bic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Akaike information criterion (AIC) and the Bayesian Information Criterion (BIC) balance the trade-off between bias and variance and account for both the likelihood and model complexity. The AIC is defined as
<span class="math display">\[
\mbox{AIC}=-2(\mbox{log likelihood})+2(\mbox{number of parameters in model})=-2l+2p.
\]</span>
where <span class="math inline">\(l\)</span> is the log likelihood and <span class="math inline">\(p\)</span> is the number of parameters in the model. And the BIC is calculated as
<span class="math display">\[
\mbox{BIC}=-2(\mbox{log likelihood}) + \ln(\mbox{sample size})\times(\mbox{number of parameters in model})=-2l+p\ln (n).
\]</span>
In general, <span class="math inline">\(\ln(n)\ge 2\)</span>; therefore, BIC penalizes more the model complexity. The model with a smaller AIC or BIC is regarded as a better model.</p>
<p><strong>Example: AIC and BIC</strong></p>
<p>Compare the AIC and BIC of the following two nested models using R:
<span class="math display">\[
\begin{aligned}
\mbox{Model 1}&amp;:\ln \frac{p}{1-p}=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\beta_4x_1x_2+\beta_5x_1x_3\\
\mbox{Model 2}&amp;:\ln \frac{p}{1-p}=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3\\
\end{aligned}
\]</span>
where <span class="math inline">\(x_1\)</span>=resting blood pressure (<span class="math inline">\(\texttt{trtpbs}\)</span>) and the categorical variable <span class="math inline">\(\texttt{thal}\)</span> recoded as
<span class="math display">\[
x_2=\left\{
\begin{array}{ll}
1&amp;\mbox{fixed defect},\\
0&amp;\mbox{Otherwise}.
\end{array}
\right.
x_3=\left\{
\begin{array}{ll}
1&amp;\mbox{reversible defect},\\
0&amp;\mbox{Otherwise}.
\end{array}
\right.
\]</span></p>
<p>The AIC and BIC for Model 1 are 336.9184 and 359.1611. The AIC and BIC for Model 2 are 333.0621 and 347.8905. Since Model 2 has smaller AIC and BIC, and hence it is regarded as a better model.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="discriminant-analysis-and-classification.html#cb58-1" tabindex="-1"></a>hdf<span class="sc">$</span>output <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(hdf<span class="sc">$</span>output<span class="sc">&gt;</span><span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span>
<span id="cb58-2"><a href="discriminant-analysis-and-classification.html#cb58-2" tabindex="-1"></a>hm2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(output<span class="sc">~</span>trtbps<span class="sc">*</span>thal,<span class="at">data=</span>hdf,<span class="at">family =</span> binomial) <span class="co">#model 1</span></span>
<span id="cb58-3"><a href="discriminant-analysis-and-classification.html#cb58-3" tabindex="-1"></a>hm3 <span class="ot">&lt;-</span> <span class="fu">glm</span>(output<span class="sc">~</span>trtbps<span class="sc">+</span>thal,<span class="at">data=</span>hdf,<span class="at">family =</span> binomial) <span class="co">#model 2</span></span>
<span id="cb58-4"><a href="discriminant-analysis-and-classification.html#cb58-4" tabindex="-1"></a><span class="fu">summary</span>(hm2) </span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = output ~ trtbps * thal, family = binomial, data = hdf)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0261  -0.7294  -0.6458   0.7806   1.8743  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)  -2.6184452  1.5219292  -1.720   0.0853 .
## trtbps        0.0105154  0.0115325   0.912   0.3619  
## thal6         1.9025277  4.0923245   0.465   0.6420  
## thal7         1.5684086  2.2373972   0.701   0.4833  
## trtbps:thal6 -0.0001823  0.0300346  -0.006   0.9952  
## trtbps:thal7  0.0061432  0.0169456   0.363   0.7170  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 415.20  on 300  degrees of freedom
## Residual deviance: 324.92  on 295  degrees of freedom
##   (2 observations deleted due to missingness)
## AIC: 336.92
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="discriminant-analysis-and-classification.html#cb60-1" tabindex="-1"></a><span class="fu">summary</span>(hm3) </span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = output ~ trtbps + thal, family = binomial, data = hdf)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9549  -0.7335  -0.6297   0.7733   1.9103  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.962791   1.067544  -2.775 0.005515 ** 
## trtbps       0.013141   0.008003   1.642 0.100598    
## thal6        1.866060   0.537872   3.469 0.000522 ***
## thal7        2.374344   0.287226   8.266  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 415.20  on 300  degrees of freedom
## Residual deviance: 325.06  on 297  degrees of freedom
##   (2 observations deleted due to missingness)
## AIC: 333.06
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="discriminant-analysis-and-classification.html#cb62-1" tabindex="-1"></a>m1.aic <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">logLik</span>(hm2)<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span><span class="fu">length</span>(hm2<span class="sc">$</span>coefficients)</span>
<span id="cb62-2"><a href="discriminant-analysis-and-classification.html#cb62-2" tabindex="-1"></a>m2.aic <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">logLik</span>(hm3)<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span><span class="fu">length</span>(hm3<span class="sc">$</span>coefficients)</span>
<span id="cb62-3"><a href="discriminant-analysis-and-classification.html#cb62-3" tabindex="-1"></a><span class="fu">c</span>(m1.aic,m2.aic)</span></code></pre></div>
<pre><code>## [1] 336.9184 333.0621</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="discriminant-analysis-and-classification.html#cb64-1" tabindex="-1"></a>m1.bic <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">logLik</span>(hm2)<span class="sc">+</span><span class="fu">log</span>(<span class="fu">length</span>(hm2<span class="sc">$</span>y))<span class="sc">*</span><span class="fu">length</span>(hm2<span class="sc">$</span>coefficients)</span>
<span id="cb64-2"><a href="discriminant-analysis-and-classification.html#cb64-2" tabindex="-1"></a>m2.bic <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">logLik</span>(hm3)<span class="sc">+</span><span class="fu">log</span>(<span class="fu">length</span>(hm3<span class="sc">$</span>y))<span class="sc">*</span><span class="fu">length</span>(hm3<span class="sc">$</span>coefficients)</span>
<span id="cb64-3"><a href="discriminant-analysis-and-classification.html#cb64-3" tabindex="-1"></a><span class="fu">c</span>(m1.bic,m2.bic)</span></code></pre></div>
<pre><code>## [1] 359.1611 347.8905</code></pre>
<p>We can also confirm the answer using the built-in functions <span class="math inline">\(\texttt{AIC()}\)</span> and <span class="math inline">\(\texttt{BIC()}\)</span>.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="discriminant-analysis-and-classification.html#cb66-1" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">AIC</span>(hm2),<span class="fu">AIC</span>(hm3)) <span class="co">#AIC</span></span></code></pre></div>
<pre><code>## [1] 336.9184 333.0621</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="discriminant-analysis-and-classification.html#cb68-1" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">BIC</span>(hm2),<span class="fu">BIC</span>(hm3)) <span class="co">#BIC</span></span></code></pre></div>
<pre><code>## [1] 359.1611 347.8905</code></pre>
</div>
<div id="forward-selection" class="section level3 hasAnchor" number="8.8.2">
<h3><span class="header-section-number">8.8.2</span> Forward Selection<a href="discriminant-analysis-and-classification.html#forward-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For forward selection, we start with a model contains no predictor variable and include one and only variable into the model in each step. The chosen variable is the one with the largest reduction in AIC. Repeat the steps until the AIC rises.</p>
<p>Forward selection can be conducted in R using the built-in function <span class="math inline">\(\texttt{step()}\)</span>. However, the function cannot handle missing values automatically. Remove rows with missing values before using the built-in function. Or we can use the function <span class="math inline">\(\texttt{add1()}\)</span>.</p>
<p>Consider the heart disease data, suppose the full model is the one with <span class="math inline">\(\texttt{age}\)</span>, <span class="math inline">\(\texttt{sex}\)</span>, <span class="math inline">\(\texttt{trtpbs}\)</span> and their interactions.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="discriminant-analysis-and-classification.html#cb70-1" tabindex="-1"></a>ms0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(output<span class="sc">~</span><span class="dv">1</span>,<span class="at">data=</span>hdf,<span class="at">family=</span>binomial)</span>
<span id="cb70-2"><a href="discriminant-analysis-and-classification.html#cb70-2" tabindex="-1"></a>ms1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(output<span class="sc">~</span>(age<span class="sc">+</span>sex<span class="sc">+</span>trtbps)<span class="sc">^</span><span class="dv">2</span>,<span class="at">data=</span>hdf,<span class="at">family=</span>binomial)</span>
<span id="cb70-3"><a href="discriminant-analysis-and-classification.html#cb70-3" tabindex="-1"></a>forwards <span class="ot">&lt;-</span> <span class="fu">step</span>(ms0, <span class="at">scope=</span><span class="fu">formula</span>(ms1), <span class="at">direction=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=419.98
## output ~ 1
## 
##          Df Deviance    AIC
## + sex     1   393.93 397.93
## + age     1   402.54 406.54
## + trtbps  1   411.03 415.03
## &lt;none&gt;        417.98 419.98
## 
## Step:  AIC=397.93
## output ~ sex
## 
##          Df Deviance    AIC
## + age     1   372.31 378.31
## + trtbps  1   384.31 390.31
## &lt;none&gt;        393.93 397.93
## 
## Step:  AIC=378.31
## output ~ sex + age
## 
##           Df Deviance    AIC
## + trtbps   1   368.38 376.38
## &lt;none&gt;         372.31 378.31
## + age:sex  1   372.16 380.16
## 
## Step:  AIC=376.38
## output ~ sex + age + trtbps
## 
##              Df Deviance    AIC
## + sex:trtbps  1   360.62 370.62
## &lt;none&gt;            368.38 376.38
## + age:sex     1   368.20 378.20
## + age:trtbps  1   368.36 378.36
## 
## Step:  AIC=370.62
## output ~ sex + age + trtbps + sex:trtbps
## 
##              Df Deviance    AIC
## &lt;none&gt;            360.62 370.62
## + age:sex     1   359.51 371.51
## + age:trtbps  1   359.86 371.86</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="discriminant-analysis-and-classification.html#cb72-1" tabindex="-1"></a>forwards <span class="co">#the final model</span></span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = output ~ sex + age + trtbps + sex:trtbps, family = binomial, 
##     data = hdf)
## 
## Coefficients:
## (Intercept)         sex1          age       trtbps  sex1:trtbps  
##   -10.74888      7.68108      0.06065      0.04525     -0.04503  
## 
## Degrees of Freedom: 302 Total (i.e. Null);  298 Residual
## Null Deviance:       418 
## Residual Deviance: 360.6     AIC: 370.6</code></pre>
<ol style="list-style-type: decimal">
<li><p>We first fit a logistic regression model with the intercept alone, the resulting AIC is 419.98.</p></li>
<li><p>Next, we fit every possible model with only one predictor. The model that gives the lowest AIC and also has a statistically significant reduction in AIC compared to the intercept-only model is the one with <span class="math inline">\(\texttt{sex}\)</span>. This model has an AIC of 397.93.</p></li>
<li><p>Then we consider every possible two-predictor model (<span class="math inline">\(\texttt{sex}\)</span> plus another predictor variable). The model with <span class="math inline">\(\texttt{age}\)</span> as the second predictor variable yields the largest reduction in AIC compared to the single-predictor model. The resulting model has an AIC of 378.31.</p></li>
<li><p>Next, we fit every possible three-predictor model (<span class="math inline">\(\texttt{sex}\)</span> and <span class="math inline">\(\texttt{age}\)</span> plus another predictor variable). The model with <span class="math inline">\(\texttt{trtbps}\)</span> as the third predictor variable has the smallest AIC of 376.38.</p></li>
<li><p>Next, we consider all possible four-predictor model (<span class="math inline">\(\texttt{sex}\)</span>, <span class="math inline">\(\texttt{age}\)</span> and <span class="math inline">\(\texttt{trtbps}\)</span> plus another predictor variable). The model with <span class="math inline">\(\texttt{sex*trtbps}\)</span> as the 4th predictor variable has the smallest AIC of 370.62.</p></li>
<li><p>Next, we fit every possible five-predictor model (<span class="math inline">\(\texttt{sex}\)</span>, <span class="math inline">\(\texttt{age}\)</span>, <span class="math inline">\(\texttt{trtbps}\)</span> and <span class="math inline">\(\texttt{sex*trtbps}\)</span> plus another variable). It turned out that none of these models reduces the AIC, thus we stop the procedure.</p></li>
</ol>
<p>The final fitted model is
<span class="math display">\[
\widehat{\ln \frac{p}{1-p}}=\hat \beta_0+\hat \beta_1x_1+\beta_2x_2+\hat\beta_3x_3+\hat\beta_4x_1x_3=-10.7489+7.6811x_1+0.0607x_2+0.0453x_3-0.0450x_1x_3,
\]</span>
where
<span class="math display">\[
x_1=\left\{
\begin{array}{ll}
1&amp;\mbox{male},\\
0&amp;\mbox{female}.
\end{array}
\right.,\quad  x_2=\mbox{age}, \quad x_3=\mbox{blood pressure}.
\]</span>
We can also summarize the procedure in a table.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="discriminant-analysis-and-classification.html#cb74-1" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb74-2"><a href="discriminant-analysis-and-classification.html#cb74-2" tabindex="-1"></a><span class="fu">kable</span>(forwards<span class="sc">$</span>anova) <span class="co">#summarize the procedure</span></span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Step</th>
<th align="right">Df</th>
<th align="right">Deviance</th>
<th align="right">Resid. Df</th>
<th align="right">Resid. Dev</th>
<th align="right">AIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">302</td>
<td align="right">417.9821</td>
<td align="right">419.9821</td>
</tr>
<tr class="even">
<td align="left">+ sex</td>
<td align="right">-1</td>
<td align="right">24.049284</td>
<td align="right">301</td>
<td align="right">393.9329</td>
<td align="right">397.9329</td>
</tr>
<tr class="odd">
<td align="left">+ age</td>
<td align="right">-1</td>
<td align="right">21.627036</td>
<td align="right">300</td>
<td align="right">372.3058</td>
<td align="right">378.3058</td>
</tr>
<tr class="even">
<td align="left">+ trtbps</td>
<td align="right">-1</td>
<td align="right">3.930785</td>
<td align="right">299</td>
<td align="right">368.3750</td>
<td align="right">376.3750</td>
</tr>
<tr class="odd">
<td align="left">+ sex:trtbps</td>
<td align="right">-1</td>
<td align="right">7.753546</td>
<td align="right">298</td>
<td align="right">360.6215</td>
<td align="right">370.6215</td>
</tr>
</tbody>
</table>
<p>We can also use other criteria such as p-value or chi-square scores to select the variable. For example, if we use the likelihood ratio test and add the significant predictor (p-value<span class="math inline">\(\le \alpha\)</span>) with the smallest p-value, the procedure is summarized in the following table.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="discriminant-analysis-and-classification.html#cb75-1" tabindex="-1"></a>flrt <span class="ot">&lt;-</span> <span class="fu">step</span>(ms0, <span class="at">scope=</span><span class="fu">formula</span>(ms1), <span class="at">direction=</span><span class="st">&quot;forward&quot;</span>, <span class="at">test=</span><span class="st">&quot;LRT&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=419.98
## output ~ 1
## 
##          Df Deviance    AIC     LRT  Pr(&gt;Chi)    
## + sex     1   393.93 397.93 24.0493 9.390e-07 ***
## + age     1   402.54 406.54 15.4466 8.487e-05 ***
## + trtbps  1   411.03 415.03  6.9542  0.008362 ** 
## &lt;none&gt;        417.98 419.98                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Step:  AIC=397.93
## output ~ sex
## 
##          Df Deviance    AIC     LRT  Pr(&gt;Chi)    
## + age     1   372.31 378.31 21.6270 3.312e-06 ***
## + trtbps  1   384.31 390.31  9.6261  0.001918 ** 
## &lt;none&gt;        393.93 397.93                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Step:  AIC=378.31
## output ~ sex + age
## 
##           Df Deviance    AIC    LRT Pr(&gt;Chi)  
## + trtbps   1   368.38 376.38 3.9308  0.04741 *
## &lt;none&gt;         372.31 378.31                  
## + age:sex  1   372.16 380.16 0.1490  0.69954  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Step:  AIC=376.38
## output ~ sex + age + trtbps
## 
##              Df Deviance    AIC    LRT Pr(&gt;Chi)   
## + sex:trtbps  1   360.62 370.62 7.7535 0.005361 **
## &lt;none&gt;            368.38 376.38                   
## + age:sex     1   368.20 378.20 0.1769 0.674061   
## + age:trtbps  1   368.36 378.36 0.0147 0.903389   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Step:  AIC=370.62
## output ~ sex + age + trtbps + sex:trtbps
## 
##              Df Deviance    AIC     LRT Pr(&gt;Chi)
## &lt;none&gt;            360.62 370.62                 
## + age:sex     1   359.51 371.51 1.11244   0.2916
## + age:trtbps  1   359.86 371.86 0.75868   0.3837</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="discriminant-analysis-and-classification.html#cb77-1" tabindex="-1"></a><span class="fu">kable</span>(flrt<span class="sc">$</span>anova)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Step</th>
<th align="right">Df</th>
<th align="right">Deviance</th>
<th align="right">Resid. Df</th>
<th align="right">Resid. Dev</th>
<th align="right">AIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">302</td>
<td align="right">417.9821</td>
<td align="right">419.9821</td>
</tr>
<tr class="even">
<td align="left">+ sex</td>
<td align="right">-1</td>
<td align="right">24.049284</td>
<td align="right">301</td>
<td align="right">393.9329</td>
<td align="right">397.9329</td>
</tr>
<tr class="odd">
<td align="left">+ age</td>
<td align="right">-1</td>
<td align="right">21.627036</td>
<td align="right">300</td>
<td align="right">372.3058</td>
<td align="right">378.3058</td>
</tr>
<tr class="even">
<td align="left">+ trtbps</td>
<td align="right">-1</td>
<td align="right">3.930785</td>
<td align="right">299</td>
<td align="right">368.3750</td>
<td align="right">376.3750</td>
</tr>
<tr class="odd">
<td align="left">+ sex:trtbps</td>
<td align="right">-1</td>
<td align="right">7.753546</td>
<td align="right">298</td>
<td align="right">360.6215</td>
<td align="right">370.6215</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="discriminant-analysis-and-classification.html#cb78-1" tabindex="-1"></a>flrt<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##  (Intercept)         sex1          age       trtbps  sex1:trtbps 
## -10.74888498   7.68108021   0.06064530   0.04525432  -0.04502625</code></pre>
</div>
<div id="backward-elimination" class="section level3 hasAnchor" number="8.8.3">
<h3><span class="header-section-number">8.8.3</span> Backward Elimination<a href="discriminant-analysis-and-classification.html#backward-elimination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Unlike the forward selection, backward elimination does the other way around. It starts with a model contains all predictor variables. In each step, remove one and only one variable that is least important and has the largest reduction in AIC until AIC starts increasing. P-value, <span class="math inline">\(z\)</span>-score, chi-square score can be used to determine which variable should be removed in each step.</p>
<p>Backward elimination can be conducted in R using the built-in function <span class="math inline">\(\texttt{step()}\)</span>. Setting the argument trace=0 tells R not to display the full results of the stepwise selection. We can also use the function <span class="math inline">\(\texttt{drop1()}\)</span>.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="discriminant-analysis-and-classification.html#cb80-1" tabindex="-1"></a>backward <span class="ot">&lt;-</span> <span class="fu">step</span>(ms1, <span class="at">direction=</span><span class="st">&#39;backward&#39;</span>, <span class="at">scope=</span><span class="fu">formula</span>(ms1)) <span class="co">#if trace=0: don&#39;t show the entire procedure</span></span></code></pre></div>
<pre><code>## Start:  AIC=372.85
## output ~ (age + sex + trtbps)^2
## 
##              Df Deviance    AIC
## - age:trtbps  1   359.51 371.51
## - age:sex     1   359.86 371.86
## &lt;none&gt;            358.85 372.85
## - sex:trtbps  1   368.19 380.19
## 
## Step:  AIC=371.51
## output ~ age + sex + trtbps + age:sex + sex:trtbps
## 
##              Df Deviance    AIC
## - age:sex     1   360.62 370.62
## &lt;none&gt;            359.51 371.51
## - sex:trtbps  1   368.20 378.20
## 
## Step:  AIC=370.62
## output ~ age + sex + trtbps + sex:trtbps
## 
##              Df Deviance    AIC
## &lt;none&gt;            360.62 370.62
## - sex:trtbps  1   368.38 376.38
## - age         1   376.73 384.73</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="discriminant-analysis-and-classification.html#cb82-1" tabindex="-1"></a><span class="fu">kable</span>(backward<span class="sc">$</span>anova) <span class="co">#summarize the procedure</span></span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Step</th>
<th align="right">Df</th>
<th align="right">Deviance</th>
<th align="right">Resid. Df</th>
<th align="right">Resid. Dev</th>
<th align="right">AIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">296</td>
<td align="right">358.8527</td>
<td align="right">372.8527</td>
</tr>
<tr class="even">
<td align="left">- age:trtbps</td>
<td align="right">1</td>
<td align="right">0.6563779</td>
<td align="right">297</td>
<td align="right">359.5090</td>
<td align="right">371.5090</td>
</tr>
<tr class="odd">
<td align="left">- age:sex</td>
<td align="right">1</td>
<td align="right">1.1124424</td>
<td align="right">298</td>
<td align="right">360.6215</td>
<td align="right">370.6215</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="discriminant-analysis-and-classification.html#cb83-1" tabindex="-1"></a>backward<span class="sc">$</span>coefficients <span class="co">#final model coefficients</span></span></code></pre></div>
<pre><code>##  (Intercept)          age         sex1       trtbps  sex1:trtbps 
## -10.74888498   0.06064530   7.68108021   0.04525432  -0.04502625</code></pre>
<p>Here is how to interpret the results:</p>
<ol style="list-style-type: decimal">
<li><p>First, we fit a model using all three predictors and their interactions. The full model has an AIC of 372.85.</p></li>
<li><p>Next, remove the interaction term <span class="math inline">\(\texttt{age:trtbps}\)</span>, the resulting model has an AIC of 371.51.</p></li>
<li><p>Next, remove the interaction term <span class="math inline">\(\texttt{age:age}\)</span>, the resulting model has an AIC of 370.62.</p></li>
<li><p>Cannot remove any more terms; otherwise AIC rises.</p></li>
</ol>
<p>The final fitted model by backward elimination is the same as the one given by forward selection. However, this is not always the case.</p>
</div>
</div>
<div id="model-checking" class="section level2 hasAnchor" number="8.9">
<h2><span class="header-section-number">8.9</span> Model Checking<a href="discriminant-analysis-and-classification.html#model-checking" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In multiple linear regression, residuals analysis is used to check the model assumptions. Adjusted <span class="math inline">\(R^2\)</span>, mean square error, <span class="math inline">\(t\)</span> test for a single slope, F test for multiple slopes can be applied to test the goodness-of-fit of the model.</p>
<div id="residual-analysis" class="section level3 hasAnchor" number="8.9.1">
<h3><span class="header-section-number">8.9.1</span> Residual Analysis<a href="discriminant-analysis-and-classification.html#residual-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For logistic regression model, the response variable <span class="math inline">\(y_i\)</span> is either 0 or 1 and the fitted value <span class="math inline">\(\hat y_i\)</span> is a probability between 0 and 1; therefore, the residual <span class="math inline">\(e_i=y_i-\hat y_i\)</span> is not that well defined as the one in multiple regression where both the observed and fitted values are numerical.</p>
<p>Residual analysis for generalized linear model has been implemented in R package <span class="math inline">\(\texttt{DHARMa}\)</span>. The main idea is to create interpretable residuals by simulation for generalized linear models that are standardized to values between 0 and 1. Two plots are generated:</p>
<ul>
<li>Left panel: a QQ-plot to detect overall deviations from the expected distribution. Departure from a linear pattern indicates lack of fit.</li>
<li>Right panel: a plot of the residuals against the predicted value (by default). It is highly recommended to plot residuals against a specific other predictors as well. Simulation outliers (data points that are outside the range of simulated values) are highlighted as red stars.</li>
</ul>
<p>More details can be found in this website:
<a href="https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html" class="uri">https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html</a></p>
<p>Letâs try the residual plots on the final logistic model chosen by forward selection method. The residual versus the predictor <span class="math inline">\(\texttt{age}\)</span> shows some curvature. None of the graphs shows strong evidence against that the model is adequate.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="discriminant-analysis-and-classification.html#cb85-1" tabindex="-1"></a><span class="fu">library</span>(DHARMa)</span>
<span id="cb85-2"><a href="discriminant-analysis-and-classification.html#cb85-2" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">glm</span>(output<span class="sc">~</span>age<span class="sc">+</span>sex<span class="sc">+</span>trtbps<span class="sc">+</span>sex<span class="sc">*</span>trtbps,data <span class="ot">&lt;-</span> hdf,<span class="at">family=</span>binomial)</span>
<span id="cb85-3"><a href="discriminant-analysis-and-classification.html#cb85-3" tabindex="-1"></a>res.mf <span class="ot">&lt;-</span> <span class="fu">simulateResiduals</span>(mf)</span>
<span id="cb85-4"><a href="discriminant-analysis-and-classification.html#cb85-4" tabindex="-1"></a><span class="fu">plot</span>(res.mf)</span></code></pre></div>
<p><img src="Plots/resids-1.png" width="90%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="discriminant-analysis-and-classification.html#cb86-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb86-2"><a href="discriminant-analysis-and-classification.html#cb86-2" tabindex="-1"></a><span class="fu">plotResiduals</span>(res.mf, hdf<span class="sc">$</span>age)</span>
<span id="cb86-3"><a href="discriminant-analysis-and-classification.html#cb86-3" tabindex="-1"></a><span class="fu">plotResiduals</span>(res.mf, hdf<span class="sc">$</span>sex)</span>
<span id="cb86-4"><a href="discriminant-analysis-and-classification.html#cb86-4" tabindex="-1"></a><span class="fu">plotResiduals</span>(res.mf, hdf<span class="sc">$</span>trtbps)</span></code></pre></div>
<p><img src="Plots/plotresids-1.png" width="864" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="discriminant-analysis-and-classification.html#cb87-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
</div>
<div id="preditive-power-accuracy-and-roc-curve" class="section level3 hasAnchor" number="8.9.2">
<h3><span class="header-section-number">8.9.2</span> Preditive Power: Accuracy and ROC Curve<a href="discriminant-analysis-and-classification.html#preditive-power-accuracy-and-roc-curve" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Accuracy calculated from a confusion table and area under the receiver operator characteristic (ROC) curve can be used to assess a fitted modelâs predictive power.</p>
<p>For the heart data set, the proportion of heart disease is <span class="math inline">\(\frac{139}{164+139}=0.4587\)</span>. Subjects with a fitted value beyond 0.4587 are classified as diseased. Based on the confusion table, the accuracy of logistic regression with <span class="math inline">\(\texttt{age}\)</span>,<span class="math inline">\(\texttt{sex}\)</span>, <span class="math inline">\(\texttt{trtbps}\)</span>, and interaction between <span class="math inline">\(\texttt{sex}\)</span> and <span class="math inline">\(\texttt{trtbps}\)</span> is <span class="math inline">\(\frac{98+103}{98+66+36+103}=0.6634\)</span>.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="discriminant-analysis-and-classification.html#cb88-1" tabindex="-1"></a><span class="fu">table</span>(hdf<span class="sc">$</span>output) <span class="co">#frequency of heart disease (1) or not (0)</span></span></code></pre></div>
<pre><code>## 
##   0   1 
## 164 139</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="discriminant-analysis-and-classification.html#cb90-1" tabindex="-1"></a>(pr <span class="ot">&lt;-</span> <span class="fu">sum</span>(hdf<span class="sc">$</span>output<span class="sc">==</span><span class="st">&quot;1&quot;</span>)<span class="sc">/</span><span class="fu">length</span>(hdf<span class="sc">$</span>output))</span></code></pre></div>
<pre><code>## [1] 0.4587459</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="discriminant-analysis-and-classification.html#cb92-1" tabindex="-1"></a>(tab <span class="ot">&lt;-</span> <span class="fu">table</span>(True <span class="ot">&lt;-</span> hdf<span class="sc">$</span>output,<span class="at">Predict=</span><span class="fu">as.numeric</span>(mf<span class="sc">$</span>fitted.values<span class="sc">&gt;</span>pr)))</span></code></pre></div>
<pre><code>##    Predict
##       0   1
##   0  98  66
##   1  36 103</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="discriminant-analysis-and-classification.html#cb94-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(tab))<span class="sc">/</span><span class="fu">sum</span>(tab)</span></code></pre></div>
<pre><code>## [1] 0.6633663</code></pre>
<p>Several packages in R provide ROC analysis such as <span class="math inline">\(\texttt{pROC}\)</span>, <span class="math inline">\(\texttt{performance}\)</span> and <span class="math inline">\(\texttt{PRROC}\)</span>. Here is the ROC curve of the logistic regression with <span class="math inline">\(\texttt{age}\)</span>,<span class="math inline">\(\texttt{sex}\)</span>, <span class="math inline">\(\texttt{trtbps}\)</span>, and interaction between <span class="math inline">\(\texttt{sex}\)</span> and <span class="math inline">\(\texttt{trtbps}\)</span> with an AUC 0.7369.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="discriminant-analysis-and-classification.html#cb96-1" tabindex="-1"></a><span class="fu">library</span>(PRROC)</span>
<span id="cb96-2"><a href="discriminant-analysis-and-classification.html#cb96-2" tabindex="-1"></a>mf <span class="ot">&lt;-</span> <span class="fu">glm</span>(output<span class="sc">~</span>age<span class="sc">+</span>sex<span class="sc">+</span>trtbps<span class="sc">+</span>sex<span class="sc">*</span>trtbps,<span class="at">data=</span>hdf,<span class="at">family=</span>binomial)</span>
<span id="cb96-3"><a href="discriminant-analysis-and-classification.html#cb96-3" tabindex="-1"></a>haroc <span class="ot">&lt;-</span> <span class="fu">roc.curve</span>(mf<span class="sc">$</span>fitted.values[hdf<span class="sc">$</span>output<span class="sc">==</span><span class="st">&quot;1&quot;</span>],mf<span class="sc">$</span>fitted.values[hdf<span class="sc">$</span>output<span class="sc">==</span><span class="st">&quot;0&quot;</span>],<span class="at">curve=</span>T)</span>
<span id="cb96-4"><a href="discriminant-analysis-and-classification.html#cb96-4" tabindex="-1"></a><span class="fu">plot</span>(haroc,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.35</span>)</span></code></pre></div>
<p><img src="Plots/prroc-1.png" width="60%" /></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="discriminant-analysis-and-classification.html#cb97-1" tabindex="-1"></a><span class="fu">library</span>(pROC)</span>
<span id="cb97-2"><a href="discriminant-analysis-and-classification.html#cb97-2" tabindex="-1"></a>rocplot <span class="ot">&lt;-</span> <span class="fu">roc</span>(output <span class="sc">~</span> <span class="fu">fitted</span>(mf), <span class="at">data=</span>hdf)</span>
<span id="cb97-3"><a href="discriminant-analysis-and-classification.html#cb97-3" tabindex="-1"></a><span class="fu">plot.roc</span>(rocplot, <span class="at">legacy.axes=</span><span class="cn">TRUE</span>) <span class="co"># Specificity on x axis if legacy.axes=F</span></span></code></pre></div>
<p><img src="Plots/prroc-2.png" width="60%" /></p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="discriminant-analysis-and-classification.html#cb98-1" tabindex="-1"></a><span class="fu">auc</span>(rocplot)</span></code></pre></div>
<pre><code>## Area under the curve: 0.7369</code></pre>
</div>
</div>
<div id="classification-tree-recursive-partitioning" class="section level2 hasAnchor" number="8.10">
<h2><span class="header-section-number">8.10</span> Classification Tree (Recursive Partitioning)<a href="discriminant-analysis-and-classification.html#classification-tree-recursive-partitioning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The main idea of the classification tree is to recursively partition the explanatory variable space into small rectangles (or cubes) to make the response variable as pure as possible within the rectangles. It picks one variable and one cut value at a time and chooses the cut that maximizes the purity or minimizes the impurity. There are two popular ways to quantify impurity: the Gini index and entropy.</p>
<ul>
<li>Gini index: <span class="math inline">\(G=\sum_{i=1}^c p_i(1-p_i)\)</span>. Measures total variance across all classes, smaller when <span class="math inline">\(p_i\)</span> is closer to either 0 or 1.</li>
<li>Entropy: <span class="math inline">\(D=-\sum_{i=1}^c p_i \ln{p_i}\)</span>. <span class="math inline">\(D\approx 0\)</span> if all <span class="math inline">\(p_i\)</span> are either 1 or 0.</li>
</ul>
<p><span class="math inline">\(\textbf{Example}\)</span>: Gini Index</p>
<p>Suppose there are two partitions: <span class="math inline">\(x_1=1.5\)</span> and <span class="math inline">\(x_2=2\)</span>. Calculate the Gini index for these two cuts and explain which cut is better, i.e., gives a smaller Gini index.
<img src="Plots/gini-1.png" width="50%" /></p>
<p>Keep searching and partitioning until the cut can not further improve the purity or a certain stopping criterion is met, say the number of items in each terminal node is at least 5. The pseudo-code for the algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>Start at the root node.</li>
<li>For each explanatory variable <span class="math inline">\(X\)</span>, find the set <span class="math inline">\(S\)</span> that minimizes the sum of the node impurities in the two child nodes and choose the split that gives the smallest value.</li>
<li>If a stopping criterion is met, exit; otherwise, apply step 2 to each child node in turn.</li>
</ol>
<p>When fitting a classification tree, especially with a large number of explanatory variables, in general we first grow a large tree <span class="math inline">\(\mathbf{T}_0\)</span> stopping the splitting process only when some stopping criterion is met, say minimum node size is 5. Then the large tree <span class="math inline">\(\mathbf{T}_0\)</span> is pruned by minimizing <span class="math inline">\(C(\mathbf{T})+\alpha|\mathbf{T}|\)</span>, where <span class="math inline">\(C(\mathbf{T})\)</span> is the error rate of the tree <span class="math inline">\(\mathbf{T}\)</span> and <span class="math inline">\(|\mathbf{T}|\)</span> is the size of the tree, <span class="math inline">\(\alpha\)</span> is the penalizing constant (complexity number). Larger values of <span class="math inline">\(\alpha\)</span> penalize big trees more and tend to lead to more pruning. Pruning improves the performance of trees.</p>
<p>We apply the classification tree on the iris data. Ten-fold cross-validation was used to select the optimal value of the complexity parameter. The graph suggests we should not prune the tree, i.e., <span class="math inline">\(\alpha=0\)</span>.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="discriminant-analysis-and-classification.html#cb100-1" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb100-2"><a href="discriminant-analysis-and-classification.html#cb100-2" tabindex="-1"></a>treem <span class="ot">&lt;-</span> <span class="fu">train</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> training, <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="at">trControl =</span> ctrl)</span>
<span id="cb100-3"><a href="discriminant-analysis-and-classification.html#cb100-3" tabindex="-1"></a><span class="fu">plot</span>(treem)</span></code></pre></div>
<p><img src="Plots/crossval-1.png" width="55%" /></p>
<p>To classify new observations, we just follow the paths of the tree. The resulting tree is as follows. It shows <span class="math inline">\(\texttt{Petal.Length}\)</span> is an important variable to separate the three species. Take the first flower in the training set for example, it has Petal.Length=1.4 which is smaller than 2.5, we classify as setosa. It turns out to be correct. And the accuracy for classification tree is 0.9333.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="discriminant-analysis-and-classification.html#cb101-1" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb101-2"><a href="discriminant-analysis-and-classification.html#cb101-2" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb101-3"><a href="discriminant-analysis-and-classification.html#cb101-3" tabindex="-1"></a>final_tree <span class="ot">&lt;-</span> treem<span class="sc">$</span>finalModel <span class="co"># Extract the final tree</span></span>
<span id="cb101-4"><a href="discriminant-analysis-and-classification.html#cb101-4" tabindex="-1"></a><span class="fu">rpart.plot</span>(final_tree) <span class="co"># Plot the tree</span></span></code></pre></div>
<p><img src="Plots/rpartTree-1.png" width="55%" /></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="discriminant-analysis-and-classification.html#cb102-1" tabindex="-1"></a>training[<span class="dv">1</span>,]</span></code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="discriminant-analysis-and-classification.html#cb104-1" tabindex="-1"></a><span class="co"># Evaluate the model on the test set</span></span>
<span id="cb104-2"><a href="discriminant-analysis-and-classification.html#cb104-2" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(treem, <span class="at">newdata =</span> testing)</span>
<span id="cb104-3"><a href="discriminant-analysis-and-classification.html#cb104-3" tabindex="-1"></a>ttab <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(predictions, testing<span class="sc">$</span>Species)<span class="sc">$</span>table</span>
<span id="cb104-4"><a href="discriminant-analysis-and-classification.html#cb104-4" tabindex="-1"></a>(<span class="at">accuracy=</span><span class="fu">sum</span>(<span class="fu">diag</span>(ttab))<span class="sc">/</span><span class="fu">sum</span>(ttab))</span></code></pre></div>
<pre><code>## [1] 0.9333333</code></pre>
<p><span class="math inline">\(\textbf{Example}:\)</span> Classification Tree on the Spam Data</p>
<p>The Spam Email database contains 4601 instances: 2788 emails (<span class="math inline">\(y=0\)</span>) and 1813 spams (<span class="math inline">\(y=1\)</span>). For each instance, 57 explanatory variables:</p>
<ul>
<li>48 variables indicating the frequencies of 48 words such as âeduâ, âyouâ.</li>
<li>6 variables indicating the frequencies of 6 characters such as $, !.</li>
<li>3 variables telling the average length of uninterrupted sequences of capital letters, length of longest uninterrupted sequence of capital letters, and total number of capital letters in the e-mail, respectively.</li>
</ul>
<p>We first split the data into 75% for training and 25% for testing.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="discriminant-analysis-and-classification.html#cb106-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb106-2"><a href="discriminant-analysis-and-classification.html#cb106-2" tabindex="-1"></a>spamdf <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/spam.csv&quot;</span>) <span class="co">#import the data</span></span>
<span id="cb106-3"><a href="discriminant-analysis-and-classification.html#cb106-3" tabindex="-1"></a>spamdf<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(spamdf<span class="sc">$</span>y<span class="sc">==</span><span class="dv">1</span>,<span class="st">&quot;spam&quot;</span>,<span class="st">&quot;email&quot;</span>))</span>
<span id="cb106-4"><a href="discriminant-analysis-and-classification.html#cb106-4" tabindex="-1"></a><span class="fu">table</span>(spamdf<span class="sc">$</span>y)</span></code></pre></div>
<pre><code>## 
## email  spam 
##  2788  1813</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="discriminant-analysis-and-classification.html#cb108-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6194</span>)</span>
<span id="cb108-2"><a href="discriminant-analysis-and-classification.html#cb108-2" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(spamdf<span class="sc">$</span>y, <span class="at">p=</span><span class="fl">0.75</span>, <span class="at">list=</span>F) <span class="co">#index not in a list</span></span>
<span id="cb108-3"><a href="discriminant-analysis-and-classification.html#cb108-3" tabindex="-1"></a>train <span class="ot">&lt;-</span> spamdf[ind,] <span class="co">#training set</span></span>
<span id="cb108-4"><a href="discriminant-analysis-and-classification.html#cb108-4" tabindex="-1"></a>test <span class="ot">&lt;-</span> spamdf[<span class="sc">-</span>ind,] <span class="co">#test set</span></span></code></pre></div>
<p>We first fit a classification tree without pruning (cp=0).</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="discriminant-analysis-and-classification.html#cb109-1" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb109-2"><a href="discriminant-analysis-and-classification.html#cb109-2" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb109-3"><a href="discriminant-analysis-and-classification.html#cb109-3" tabindex="-1"></a>mt0 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(y<span class="sc">~</span>., <span class="at">data=</span>train, <span class="at">cp=</span><span class="dv">0</span>) <span class="co">#classification tree without pruning</span></span>
<span id="cb109-4"><a href="discriminant-analysis-and-classification.html#cb109-4" tabindex="-1"></a><span class="fu">rpart.plot</span>(mt0)</span></code></pre></div>
<p><img src="Plots/noprune-1.png" width="672" /></p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="discriminant-analysis-and-classification.html#cb110-1" tabindex="-1"></a>tab <span class="ot">&lt;-</span> mt0<span class="sc">$</span>cptable <span class="co">#CP table </span></span>
<span id="cb110-2"><a href="discriminant-analysis-and-classification.html#cb110-2" tabindex="-1"></a><span class="fu">round</span>(tab, <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##        CP nsplit rel error xerror   xstd
## 1  0.4875      0    1.0000 1.0000 0.0211
## 2  0.1456      1    0.5125 0.5463 0.0178
## 3  0.0537      2    0.3669 0.4868 0.0170
## 4  0.0353      3    0.3132 0.3331 0.0146
## 5  0.0235      4    0.2779 0.3162 0.0143
## 6  0.0132      5    0.2544 0.3037 0.0140
## 7  0.0096      6    0.2412 0.2934 0.0138
## 8  0.0074      7    0.2316 0.2875 0.0137
## 9  0.0066      8    0.2243 0.2728 0.0134
## 10 0.0054      9    0.2176 0.2662 0.0132
## 11 0.0037     12    0.2015 0.2507 0.0129
## 12 0.0033     15    0.1904 0.2404 0.0127
## 13 0.0032     17    0.1838 0.2331 0.0125
## 14 0.0029     20    0.1743 0.2331 0.0125
## 15 0.0022     22    0.1684 0.2316 0.0124
## 16 0.0015     24    0.1640 0.2235 0.0122
## 17 0.0007     34    0.1478 0.2228 0.0122
## 18 0.0006     43    0.1412 0.2250 0.0123
## 19 0.0000     47    0.1390 0.2301 0.0124</code></pre>
<p>The CP (complexity parameter) table gives the following information:</p>
<ul>
<li>The first column <span class="math inline">\(\texttt{CP}\)</span> is the complexity parameter. It is <span class="math inline">\(\alpha\)</span> in the objective function <span class="math inline">\(C(\mathbf{T})+\alpha|\mathbf{T}|\)</span>. If <span class="math inline">\(\text{CP}=0\)</span>, we donât penalize the tree size, i.e., we will grow a big tree.</li>
<li>The second column <span class="math inline">\(\texttt{nsplit}\)</span> gives the number of splits. If <span class="math inline">\(\text{nsplit}=4\)</span>, it means the tree has 4 splits, i.e., the resulting tree has 5 terminal nodes.</li>
<li>The third column <span class="math inline">\(\texttt{rel error}\)</span> indicates the impact of adding or removing nodes on the modelâs performance. Lower relative error values suggest that the split significantly improves the accuracy, and such splits are favored during the construction of the decision tree.</li>
<li>The fourth column <span class="math inline">\(\verb`xerror`\)</span> is the cross-validated error. It indicates the error rate observed when the model is applied to unseen data. We use the CP value with the smallest cross-validated error to prune the tree.</li>
<li>The last column <span class="math inline">\(\verb`xstd`\)</span> gives the variation of the associated cross-validated error. Smaller xstd values indicate that the cross-validated error rate estimates are more stable and reliable.</li>
</ul>
<p>Based on the CP table, the tree without pruning has (47+1)=48 terminal nodes. The tree with 34 splits (i.e., 35 terminal nodes) gives the smallest cross-validated error (xerror), so we should prune the tree with CP=0.0007.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="discriminant-analysis-and-classification.html#cb112-1" tabindex="-1"></a>(cp1 <span class="ot">&lt;-</span> tab[<span class="fu">which.min</span>(tab[,<span class="dv">4</span>]),<span class="dv">1</span>]) <span class="co">#CP value for the tree with smallest xerror</span></span></code></pre></div>
<pre><code>## [1] 0.0007352941</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="discriminant-analysis-and-classification.html#cb114-1" tabindex="-1"></a>mt.prune <span class="ot">&lt;-</span> <span class="fu">prune</span>(mt0,<span class="at">cp=</span>cp1) <span class="co">#prune the tree</span></span>
<span id="cb114-2"><a href="discriminant-analysis-and-classification.html#cb114-2" tabindex="-1"></a><span class="fu">rpart.plot</span>(mt.prune)</span></code></pre></div>
<p><img src="Plots/pruned-1.png" width="672" /></p>
<p>Calculate the accuracy of the pruned classification tree.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="discriminant-analysis-and-classification.html#cb115-1" tabindex="-1"></a>pred_tr <span class="ot">&lt;-</span> <span class="fu">predict</span>(mt.prune,test) <span class="co">#return prob of each class</span></span>
<span id="cb115-2"><a href="discriminant-analysis-and-classification.html#cb115-2" tabindex="-1"></a><span class="fu">head</span>(pred_tr) </span></code></pre></div>
<pre><code>##         email       spam
## 1  0.11111111 0.88888889
## 3  0.02483660 0.97516340
## 7  0.11538462 0.88461538
## 14 0.04405286 0.95594714
## 15 0.00000000 1.00000000
## 21 0.92307692 0.07692308</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="discriminant-analysis-and-classification.html#cb117-1" tabindex="-1"></a>(spamtab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">True=</span>test<span class="sc">$</span>y,<span class="at">Predict=</span><span class="fu">ifelse</span>(pred_tr[,<span class="dv">1</span>]<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">&quot;email&quot;</span>,<span class="st">&quot;spam&quot;</span>)))</span></code></pre></div>
<pre><code>##        Predict
## True    email spam
##   email   656   41
##   spam     53  400</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="discriminant-analysis-and-classification.html#cb119-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(spamtab))<span class="sc">/</span><span class="fu">nrow</span>(test)</span></code></pre></div>
<pre><code>## [1] 0.9182609</code></pre>
</div>
<div id="regression-tree" class="section level2 hasAnchor" number="8.11">
<h2><span class="header-section-number">8.11</span> Regression Tree<a href="discriminant-analysis-and-classification.html#regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The recursive partitioning method can be applied to numerical responses as well. The tree model is called . The main difference between a regression tree and a classification tree is the performance measure. For each split, we choose the best cut that gives the smallest sum of squared deviation between the sample mean in each rectangle and the overall sample mean.</p>
<p><span class="math inline">\(\textbf{Example}:\)</span> Housing Values in Suburbs of Boston</p>
<p>The Boston data frame has 506 rows and 14 columns: crim (per capita crime rate by town), zn (proportion of residential land zoned for lots over 25,000 sq.ft.), indus (proportion of non-retail business acres per town), chas (Charles River dummy variable, 1 if tract bounds river; 0 otherwise), nox (nitrogen oxides concentration, parts per 10 million), rm (average number of rooms per dwelling), age (proportion of owner-occupied units built prior to 1940), dis (weighted mean of distances to five Boston employment centres), rad (index of accessibility to radial highways), tax (full-value property-tax rate per $10,000), ptratio (pupil-teacher ratio by town), black (<span class="math inline">\(1000(Bk - 0.63)^2\)</span> where <span class="math inline">\(Bk\)</span> is the proportion of blacks by town), lstat (lower status of the population (percent), medv (median value of owner-occupied homes in $1000s).</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="discriminant-analysis-and-classification.html#cb121-1" tabindex="-1"></a><span class="co">#library(MASS)</span></span>
<span id="cb121-2"><a href="discriminant-analysis-and-classification.html#cb121-2" tabindex="-1"></a>bdf <span class="ot">&lt;-</span> Boston <span class="co">#the data set is in R</span></span>
<span id="cb121-3"><a href="discriminant-analysis-and-classification.html#cb121-3" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">head</span>(bdf), <span class="at">caption =</span> <span class="st">&quot;Boston Housing Data&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:bostondata">Table 8.1: </span>Boston Housing Data</caption>
<colgroup>
<col width="10%" />
<col width="3%" />
<col width="7%" />
<col width="6%" />
<col width="7%" />
<col width="7%" />
<col width="6%" />
<col width="8%" />
<col width="5%" />
<col width="5%" />
<col width="10%" />
<col width="8%" />
<col width="7%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">crim</th>
<th align="right">zn</th>
<th align="right">indus</th>
<th align="right">chas</th>
<th align="right">nox</th>
<th align="right">rm</th>
<th align="right">age</th>
<th align="right">dis</th>
<th align="right">rad</th>
<th align="right">tax</th>
<th align="right">ptratio</th>
<th align="right">black</th>
<th align="right">lstat</th>
<th align="right">medv</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.00632</td>
<td align="right">18</td>
<td align="right">2.31</td>
<td align="right">0</td>
<td align="right">0.538</td>
<td align="right">6.575</td>
<td align="right">65.2</td>
<td align="right">4.0900</td>
<td align="right">1</td>
<td align="right">296</td>
<td align="right">15.3</td>
<td align="right">396.90</td>
<td align="right">4.98</td>
<td align="right">24.0</td>
</tr>
<tr class="even">
<td align="right">0.02731</td>
<td align="right">0</td>
<td align="right">7.07</td>
<td align="right">0</td>
<td align="right">0.469</td>
<td align="right">6.421</td>
<td align="right">78.9</td>
<td align="right">4.9671</td>
<td align="right">2</td>
<td align="right">242</td>
<td align="right">17.8</td>
<td align="right">396.90</td>
<td align="right">9.14</td>
<td align="right">21.6</td>
</tr>
<tr class="odd">
<td align="right">0.02729</td>
<td align="right">0</td>
<td align="right">7.07</td>
<td align="right">0</td>
<td align="right">0.469</td>
<td align="right">7.185</td>
<td align="right">61.1</td>
<td align="right">4.9671</td>
<td align="right">2</td>
<td align="right">242</td>
<td align="right">17.8</td>
<td align="right">392.83</td>
<td align="right">4.03</td>
<td align="right">34.7</td>
</tr>
<tr class="even">
<td align="right">0.03237</td>
<td align="right">0</td>
<td align="right">2.18</td>
<td align="right">0</td>
<td align="right">0.458</td>
<td align="right">6.998</td>
<td align="right">45.8</td>
<td align="right">6.0622</td>
<td align="right">3</td>
<td align="right">222</td>
<td align="right">18.7</td>
<td align="right">394.63</td>
<td align="right">2.94</td>
<td align="right">33.4</td>
</tr>
<tr class="odd">
<td align="right">0.06905</td>
<td align="right">0</td>
<td align="right">2.18</td>
<td align="right">0</td>
<td align="right">0.458</td>
<td align="right">7.147</td>
<td align="right">54.2</td>
<td align="right">6.0622</td>
<td align="right">3</td>
<td align="right">222</td>
<td align="right">18.7</td>
<td align="right">396.90</td>
<td align="right">5.33</td>
<td align="right">36.2</td>
</tr>
<tr class="even">
<td align="right">0.02985</td>
<td align="right">0</td>
<td align="right">2.18</td>
<td align="right">0</td>
<td align="right">0.458</td>
<td align="right">6.430</td>
<td align="right">58.7</td>
<td align="right">6.0622</td>
<td align="right">3</td>
<td align="right">222</td>
<td align="right">18.7</td>
<td align="right">394.12</td>
<td align="right">5.21</td>
<td align="right">28.7</td>
</tr>
</tbody>
</table>
<p>We want to predict the <span class="math inline">\(\verb`mevd`\)</span> (median value of the homes) using the explanatory variables such as <span class="math inline">\(\verb`crim`\)</span>, <span class="math inline">\(\verb`rm`\)</span>, <span class="math inline">\(\verb`Istat`\)</span>, and etc. We divide the data into the training and test sets: 2/3 for training and 1/3 for testing. Fit the regression tree model on the training set and check the performance of the fitted model on the test set.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="discriminant-analysis-and-classification.html#cb122-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6194</span>)</span>
<span id="cb122-2"><a href="discriminant-analysis-and-classification.html#cb122-2" tabindex="-1"></a>flds <span class="ot">&lt;-</span> <span class="fu">createFolds</span>(bdf<span class="sc">$</span>medv, <span class="at">k =</span> <span class="dv">3</span>, <span class="at">list =</span> <span class="cn">TRUE</span>, <span class="at">returnTrain =</span> <span class="cn">FALSE</span>)</span>
<span id="cb122-3"><a href="discriminant-analysis-and-classification.html#cb122-3" tabindex="-1"></a>train <span class="ot">&lt;-</span> bdf[<span class="sc">-</span>flds[[<span class="dv">3</span>]],]</span>
<span id="cb122-4"><a href="discriminant-analysis-and-classification.html#cb122-4" tabindex="-1"></a>test <span class="ot">&lt;-</span> bdf[flds[[<span class="dv">3</span>]],]</span>
<span id="cb122-5"><a href="discriminant-analysis-and-classification.html#cb122-5" tabindex="-1"></a>mrt <span class="ot">&lt;-</span> <span class="fu">rpart</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>train,<span class="at">cp=</span><span class="dv">0</span>) <span class="co">#fit a big tree without pruning</span></span>
<span id="cb122-6"><a href="discriminant-analysis-and-classification.html#cb122-6" tabindex="-1"></a><span class="fu">rpart.plot</span>(mrt) <span class="co">#plot the tree without pruning</span></span></code></pre></div>
<p><img src="Plots/bostonRegressiontree-1.png" width="672" /></p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="discriminant-analysis-and-classification.html#cb123-1" tabindex="-1"></a>(tab <span class="ot">&lt;-</span> mrt<span class="sc">$</span>cptable) <span class="co">#CP table</span></span></code></pre></div>
<pre><code>##              CP nsplit rel error    xerror       xstd
## 1  0.4820793362      0 1.0000000 1.0089155 0.10216415
## 2  0.1349486733      1 0.5179207 0.6088939 0.06594488
## 3  0.0993411447      2 0.3829720 0.4575943 0.05669686
## 4  0.0266546315      3 0.2836308 0.3557458 0.05102330
## 5  0.0241441318      4 0.2569762 0.3160056 0.04840921
## 6  0.0190860424      5 0.2328321 0.3178338 0.04940566
## 7  0.0162395766      6 0.2137460 0.3105117 0.04947553
## 8  0.0085731414      7 0.1975065 0.2923048 0.04654080
## 9  0.0067351258      8 0.1889333 0.2745256 0.04343057
## 10 0.0063353872      9 0.1821982 0.2714114 0.04338882
## 11 0.0060326922     10 0.1758628 0.2690827 0.04605941
## 12 0.0059133168     11 0.1698301 0.2684482 0.04606931
## 13 0.0035296589     12 0.1639168 0.2575551 0.04535724
## 14 0.0027195002     13 0.1603871 0.2578765 0.04539281
## 15 0.0026739050     14 0.1576676 0.2627563 0.04716961
## 16 0.0023741669     15 0.1549937 0.2640974 0.04716701
## 17 0.0019567575     16 0.1526196 0.2631027 0.04717093
## 18 0.0018613285     17 0.1506628 0.2613908 0.04715654
## 19 0.0018203937     18 0.1488015 0.2636982 0.04716872
## 20 0.0016699962     19 0.1469811 0.2633756 0.04720416
## 21 0.0016019975     20 0.1453111 0.2633518 0.04720498
## 22 0.0011137696     21 0.1437091 0.2637808 0.04720749
## 23 0.0006622958     22 0.1425953 0.2640179 0.04729943
## 24 0.0006258317     23 0.1419330 0.2658967 0.04734873
## 25 0.0005912388     24 0.1413072 0.2657652 0.04735076
## 26 0.0005793161     25 0.1407160 0.2657922 0.04735032
## 27 0.0004705356     26 0.1401366 0.2661397 0.04735135
## 28 0.0002834765     27 0.1396661 0.2659556 0.04735408
## 29 0.0000000000     28 0.1393826 0.2659094 0.04735478</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="discriminant-analysis-and-classification.html#cb125-1" tabindex="-1"></a>(cp1 <span class="ot">&lt;-</span> tab[<span class="fu">which.min</span>(tab[,<span class="dv">4</span>]),<span class="dv">1</span>]) <span class="co">#CP value for the tree with smallest xerror</span></span></code></pre></div>
<pre><code>## [1] 0.003529659</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="discriminant-analysis-and-classification.html#cb127-1" tabindex="-1"></a>mrt.prune <span class="ot">&lt;-</span> <span class="fu">prune</span>(mrt,<span class="at">cp=</span>cp1) <span class="co">#prune the tree</span></span>
<span id="cb127-2"><a href="discriminant-analysis-and-classification.html#cb127-2" tabindex="-1"></a><span class="fu">rpart.plot</span>(mrt.prune) <span class="co">#plot the pruned tree</span></span></code></pre></div>
<p><img src="Plots/bostonRegressiontree-2.png" width="672" /></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="discriminant-analysis-and-classification.html#cb128-1" tabindex="-1"></a><span class="co">#calculate the fitted value by regression tree</span></span>
<span id="cb128-2"><a href="discriminant-analysis-and-classification.html#cb128-2" tabindex="-1"></a>pvec1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(mrt.prune,test) </span>
<span id="cb128-3"><a href="discriminant-analysis-and-classification.html#cb128-3" tabindex="-1"></a><span class="fu">kable</span>(test[<span class="dv">1</span>,])</span></code></pre></div>
<table style="width:100%;">
<colgroup>
<col width="3%" />
<col width="9%" />
<col width="3%" />
<col width="7%" />
<col width="6%" />
<col width="7%" />
<col width="7%" />
<col width="6%" />
<col width="8%" />
<col width="4%" />
<col width="4%" />
<col width="9%" />
<col width="8%" />
<col width="7%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">crim</th>
<th align="right">zn</th>
<th align="right">indus</th>
<th align="right">chas</th>
<th align="right">nox</th>
<th align="right">rm</th>
<th align="right">age</th>
<th align="right">dis</th>
<th align="right">rad</th>
<th align="right">tax</th>
<th align="right">ptratio</th>
<th align="right">black</th>
<th align="right">lstat</th>
<th align="right">medv</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">4</td>
<td align="right">0.03237</td>
<td align="right">0</td>
<td align="right">2.18</td>
<td align="right">0</td>
<td align="right">0.458</td>
<td align="right">6.998</td>
<td align="right">45.8</td>
<td align="right">6.0622</td>
<td align="right">3</td>
<td align="right">222</td>
<td align="right">18.7</td>
<td align="right">394.63</td>
<td align="right">2.94</td>
<td align="right">33.4</td>
</tr>
</tbody>
</table>
<p>Based on the pruned regression tree, estimate the median values of homes for the first observation in the test set with the features above.</p>
</div>
<div id="random-forest" class="section level2 hasAnchor" number="8.12">
<h2><span class="header-section-number">8.12</span> Random Forest<a href="discriminant-analysis-and-classification.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest is an ensemble machine learning algorithm that builds multiple decision trees and combines their predictions to improve the accuracy and stability of the model. Random Forest is used for both regression and classification tasks.</p>
<p>The basic idea behind Random Forest is to randomly select a subset of the features for each tree in the forest, and to split each node in the tree using the best split among a random subset of the features. This process is repeated many times to create many decision trees, each of which provides a prediction for the target variable. The final prediction is then made by combining the predictions of all the trees, typically by taking the average or the majority vote.</p>
<p>Random Forest has several advantages over single decision trees, such as increased accuracy, reduced overfitting, and improved interpretability. Additionally, Random Forest can handle missing data and noisy data better than single decision trees.</p>
<p>We apply the random forest on the iris data. Ten-fold cross-validation was used to select the optimal number of variables to build the trees. The graph suggests we should use either two or three out of the four features. The final model uses mtry=2 and the accuracy on the testing set is 0.96667.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="discriminant-analysis-and-classification.html#cb129-1" tabindex="-1"></a><span class="co">#library(caret) # cross validation</span></span>
<span id="cb129-2"><a href="discriminant-analysis-and-classification.html#cb129-2" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb129-3"><a href="discriminant-analysis-and-classification.html#cb129-3" tabindex="-1"></a><span class="co"># Set the control parameters for cross-validation</span></span>
<span id="cb129-4"><a href="discriminant-analysis-and-classification.html#cb129-4" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb129-5"><a href="discriminant-analysis-and-classification.html#cb129-5" tabindex="-1"></a><span class="co"># Train the Random Forest model using 10-fold cross-validation</span></span>
<span id="cb129-6"><a href="discriminant-analysis-and-classification.html#cb129-6" tabindex="-1"></a>rfm <span class="ot">&lt;-</span> <span class="fu">train</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> training, <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="at">trControl =</span> ctrl)</span>
<span id="cb129-7"><a href="discriminant-analysis-and-classification.html#cb129-7" tabindex="-1"></a><span class="co"># Evaluate the model on the test set</span></span>
<span id="cb129-8"><a href="discriminant-analysis-and-classification.html#cb129-8" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(rfm, <span class="at">newdata =</span> testing)</span>
<span id="cb129-9"><a href="discriminant-analysis-and-classification.html#cb129-9" tabindex="-1"></a>(rftab <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(predictions, testing<span class="sc">$</span>Species)<span class="sc">$</span>table)</span></code></pre></div>
<pre><code>##             Reference
## Prediction   setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          9         0
##   virginica       0          1        10</code></pre>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="discriminant-analysis-and-classification.html#cb131-1" tabindex="-1"></a>(<span class="at">accuracy=</span><span class="fu">sum</span>(<span class="fu">diag</span>(rftab))<span class="sc">/</span><span class="fu">sum</span>(rftab))</span></code></pre></div>
<pre><code>## [1] 0.9666667</code></pre>
<p>One appealing feature of random forest is it provides a measure of the importance of each variable. The outputs below show that <span class="math inline">\(\verb`Petal.Length`\)</span> is the most important variable to separate the three species.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="discriminant-analysis-and-classification.html#cb133-1" tabindex="-1"></a><span class="co"># Extract the variable importance</span></span>
<span id="cb133-2"><a href="discriminant-analysis-and-classification.html#cb133-2" tabindex="-1"></a>importance <span class="ot">&lt;-</span> <span class="fu">varImp</span>(rfm<span class="sc">$</span>finalModel, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb133-3"><a href="discriminant-analysis-and-classification.html#cb133-3" tabindex="-1"></a><span class="co"># Sort the variables by importance and print the result</span></span>
<span id="cb133-4"><a href="discriminant-analysis-and-classification.html#cb133-4" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Variable =</span> <span class="fu">row.names</span>(importance), <span class="at">Importance =</span> importance[, <span class="st">&quot;Overall&quot;</span>])</span>
<span id="cb133-5"><a href="discriminant-analysis-and-classification.html#cb133-5" tabindex="-1"></a>result <span class="ot">&lt;-</span> result[<span class="fu">order</span>(<span class="sc">-</span>result<span class="sc">$</span>Importance), ]</span>
<span id="cb133-6"><a href="discriminant-analysis-and-classification.html#cb133-6" tabindex="-1"></a><span class="fu">print</span>(result)</span></code></pre></div>
<pre><code>##       Variable Importance
## 3 Petal.Length 41.5612190
## 4  Petal.Width 35.4531345
## 1 Sepal.Length  1.3640399
## 2  Sepal.Width  0.9808732</code></pre>
<p>Like decision tree, random forests can handle both the classification (for categorical response) and regression (for numerical response) problems.</p>
<p><span class="math inline">\(\textbf{Example}\)</span>: Random Forest for Regression on Boston Data</p>
<p>We fit a random forest model on the Boston Data and compare the sum of squares of the residuals with regression tree and multiple regression.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="discriminant-analysis-and-classification.html#cb135-1" tabindex="-1"></a>mrf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>train,<span class="at">importance=</span>T)</span>
<span id="cb135-2"><a href="discriminant-analysis-and-classification.html#cb135-2" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">importance</span>(mrf),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##         %IncMSE IncNodePurity
## crim      13.57       1752.04
## zn         3.49        169.81
## indus     10.05       1628.09
## chas       4.58        258.21
## nox       14.22       1844.16
## rm        31.77       8145.09
## age       10.57        829.38
## dis       12.67       1365.45
## rad        5.40        240.35
## tax       11.12        939.29
## ptratio   15.99       2184.76
## black      7.30        472.82
## lstat     29.52       8601.83</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="discriminant-analysis-and-classification.html#cb137-1" tabindex="-1"></a>prf <span class="ot">&lt;-</span> <span class="fu">predict</span>(mrf,test,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb137-2"><a href="discriminant-analysis-and-classification.html#cb137-2" tabindex="-1"></a></span>
<span id="cb137-3"><a href="discriminant-analysis-and-classification.html#cb137-3" tabindex="-1"></a><span class="co">#fit a multiple regression</span></span>
<span id="cb137-4"><a href="discriminant-analysis-and-classification.html#cb137-4" tabindex="-1"></a>mlm <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv<span class="sc">~</span>., <span class="at">data=</span>train)</span>
<span id="cb137-5"><a href="discriminant-analysis-and-classification.html#cb137-5" tabindex="-1"></a><span class="fu">summary</span>(mlm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = medv ~ ., data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.8613  -2.7534  -0.5527   1.6387  27.1694 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.217608   6.282701   5.924 8.05e-09 ***
## crim         -0.143947   0.043087  -3.341 0.000933 ***
## zn            0.037566   0.016802   2.236 0.026045 *  
## indus         0.029849   0.078568   0.380 0.704259    
## chas          3.230047   1.086542   2.973 0.003173 ** 
## nox         -18.786303   4.823356  -3.895 0.000119 ***
## rm            3.913611   0.513768   7.617 2.89e-13 ***
## age          -0.009712   0.016889  -0.575 0.565661    
## dis          -1.426398   0.246913  -5.777 1.79e-08 ***
## rad           0.351993   0.087932   4.003 7.76e-05 ***
## tax          -0.014395   0.005019  -2.868 0.004403 ** 
## ptratio      -0.977211   0.165481  -5.905 8.92e-09 ***
## black         0.009158   0.003607   2.539 0.011580 *  
## lstat        -0.462373   0.066304  -6.974 1.75e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.875 on 323 degrees of freedom
## Multiple R-squared:  0.7356, Adjusted R-squared:  0.725 
## F-statistic: 69.13 on 13 and 323 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="discriminant-analysis-and-classification.html#cb139-1" tabindex="-1"></a><span class="co">#fitted value by the multiple regression</span></span>
<span id="cb139-2"><a href="discriminant-analysis-and-classification.html#cb139-2" tabindex="-1"></a>pvec2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(mlm, test)</span>
<span id="cb139-3"><a href="discriminant-analysis-and-classification.html#cb139-3" tabindex="-1"></a><span class="co">#compare the SSE of regression tree and multiple regression</span></span>
<span id="cb139-4"><a href="discriminant-analysis-and-classification.html#cb139-4" tabindex="-1"></a></span>
<span id="cb139-5"><a href="discriminant-analysis-and-classification.html#cb139-5" tabindex="-1"></a><span class="fu">plot</span>(test<span class="sc">$</span>medv,pvec2, <span class="at">pch =</span> <span class="dv">17</span>,<span class="at">xlab=</span><span class="st">&quot;True medv&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Predicted medv&quot;</span>,</span>
<span id="cb139-6"><a href="discriminant-analysis-and-classification.html#cb139-6" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">max</span>(<span class="fu">c</span>(pvec1,pvec2))))</span>
<span id="cb139-7"><a href="discriminant-analysis-and-classification.html#cb139-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb139-8"><a href="discriminant-analysis-and-classification.html#cb139-8" tabindex="-1"></a><span class="fu">points</span>(test<span class="sc">$</span>medv,pvec1,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb139-9"><a href="discriminant-analysis-and-classification.html#cb139-9" tabindex="-1"></a><span class="fu">points</span>(test<span class="sc">$</span>medv,prf,<span class="at">pch=</span><span class="dv">20</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb139-10"><a href="discriminant-analysis-and-classification.html#cb139-10" tabindex="-1"></a><span class="fu">legend</span>(<span class="dv">5</span>,<span class="dv">45</span>,<span class="fu">c</span>(<span class="st">&quot;Random Forest&quot;</span>,<span class="st">&quot;Regression Tree&quot;</span>,<span class="st">&quot;Multiple Regression&quot;</span>),</span>
<span id="cb139-11"><a href="discriminant-analysis-and-classification.html#cb139-11" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;black&quot;</span>),<span class="at">pch=</span><span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">19</span>, <span class="dv">17</span>))</span></code></pre></div>
<p><img src="Plots/rfBoston-1.png" width="70%" /></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="discriminant-analysis-and-classification.html#cb140-1" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">sum</span>((prf<span class="sc">-</span>test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>),<span class="fu">sum</span>((pvec1<span class="sc">-</span>test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>),<span class="fu">sum</span>((pvec2<span class="sc">-</span>test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 1902.103 3318.591 3508.860</code></pre>
</div>
<div id="support-vector-machines" class="section level2 hasAnchor" number="8.13">
<h2><span class="header-section-number">8.13</span> Support Vector Machines<a href="discriminant-analysis-and-classification.html#support-vector-machines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Support vector machine (SVM) is a novel learning method originally introduced by Cortes and Vapnik (1995). It includes polynomial classifiers, radial basis function (RBF) networks, and single-layer neural networks as special cases. In a binary classification problem, suppose we have data <span class="math inline">\(S=\{(\mathbf{x}_1,y_1), \ldots, (\mathbf{x}_N,y_N)\}\)</span> with <span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> and <span class="math inline">\(y_i \in \{1, -1\}\)</span>. Notice that the two classes here are <span class="math inline">\(\{1, -1\}\)</span> rather than <span class="math inline">\(\{0, 1\}\)</span> in a typical binary classification problem. The data are said to be linearly separable if there exists a hyperplane <span class="math inline">\(f(\mathbf{x})=0\)</span> that perfectly separates the two classes; otherwise, the data are linearly non-separable. The real-valued function <span class="math inline">\(f(\mathbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}\)</span> is called the <em>decision function</em> (DF), which can be written as
<span class="math display">\[
\begin{aligned}
f(\mathbf{x})&amp;=\langle \mathbf{w}, \mathbf{x} \rangle +b\\
&amp;=\mathbf{w}^T \mathbf{x}+b.
\end{aligned}
\]</span></p>
<p>A new observation <span class="math inline">\(\mathbf{x}\)</span> is assigned to class 1 if <span class="math inline">\(f(\mathbf{x})\ge 0\)</span> and otherwise to class -1. The parameters <span class="math inline">\((\mathbf{w}, b)\)</span> can be estimated from the data.</p>
<p>When the data are linearly separable, we can find some hyperplane that perfectly separates the two classes. Let the margin of a
hyperplane <span class="math inline">\((\mathbf{w},b)\)</span> be <span class="math inline">\(2\gamma\)</span>, where <span class="math inline">\(\gamma\)</span> is the shortest distance from the hyperplane to a training point. The objective is to find the hyperplane that produces the largest margin. Figure <a href="discriminant-analysis-and-classification.html#fig:margin">8.5</a> shows two hyperplanes and simulated data points which are linearly separable. Note that each hyperplane is halfway between the two dashed lines. The hyperplane with <span class="math inline">\(\mbox{margin}_1\)</span> is better than the one with <span class="math inline">\(\mbox{margin}_2\)</span> in that it has a larger margin.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:margin"></span>
<img src="Plots/svm_margin.png" alt="Two hyperplances of simulated data points. The solid lines are the separating hyperplanes (or decision boundaries). The hyperplane with the larger margin (1) is better" width="50%" />
<p class="caption">
Figure 8.5: Two hyperplances of simulated data points. The solid lines are the separating hyperplanes (or decision boundaries). The hyperplane with the larger margin (1) is better
</p>
</div>
<p>It can be shown that finding the maximal margin hyperplane <span class="math inline">\((\mathbf{w}, b)\)</span> is equivalent to solving the following optimization
problem
<span class="math display" id="eq:svm-separable">\[\begin{equation}
\label{eq:svm-separable}
\begin{array}{rl}
\displaystyle
\min_{\mathbf{w}, b} &amp; \frac{1}{2}\|\mathbf{w}\|^2\\
\mbox{ subject to} &amp; y_i(\mathbf{w}^T \mathbf{x}_i +b ) \ge 1, i=1,
\ldots, N. \tag{8.1}
\end{array}
\end{equation}\]</span></p>
<p>When the data are linearly non-separable, no solution exists for the <a href="discriminant-analysis-and-classification.html#eq:svm-separable">(8.1)</a> above. One way to deal with this problem is to
still minimize <span class="math inline">\(\frac{1}{2}\|\mathbf{w}\|^2\)</span> while relaxing all the constraints by introducing some <em>slack variables</em> <span class="math inline">\(\mathbf{\xi}=(\xi_1, \ldots, \xi_N)\)</span>. These slack variables allow some observations to be on the wrong side of the margin. The optimization problem for linearly non-separable case is
<span class="math display" id="eq:svm-nonseparable">\[\begin{equation}
\label{eq:svm-nonseparable}
\begin{array}{rl}
\displaystyle
\min_{\mathbf{w}, b}&amp; \frac{1}{2}\|\mathbf{w}\|^2+C\sum_{i=1}^N \xi_i\\
\mbox{subject to}&amp;y_i(\mathbf{w}^T \mathbf{x}_i +b ) \ge 1-\xi_i,\\
&amp;\xi_i \ge 0, i=1, \ldots, N,\tag{8.2}
\end{array}
\end{equation}\]</span>
where <span class="math inline">\(C\)</span> (short for <em>Cost</em>) is a regularization parameter controlling the smoothness of the boundary. A large value of <span class="math inline">\(C\)</span> will discourage any positive <span class="math inline">\(\xi_i\)</span> and result in an overfit wiggly boundary; a small value of <span class="math inline">\(C\)</span> will lead to an over smooth boundary. This trade-off enables our choosing the optimal value of <span class="math inline">\(C\)</span> by cross-validation.</p>
<p>By using the Lagrangian method, both the , both <a href="discriminant-analysis-and-classification.html#eq:svm-separable">(8.1)</a>: and <a href="discriminant-analysis-and-classification.html#eq:svm-nonseparable">(8.2)</a> can be rephrased as quadratic programming problems
with linear inequality constraints It can be
shown that the solution for <span class="math inline">\(\mathbf{w}\)</span> has the form
<span class="math display">\[
\hat {\mathbf{w}}=\sum_{i=1}^N \hat \alpha_i y_i \mathbf{x}_i,
\]</span>
where <span class="math inline">\(\alpha_i \ge0\)</span> are Lagrange multipliers. Those observations
with strictly positive coefficients <span class="math inline">\(\alpha_i\)</span> are called <em>support
vectors</em> (SV), since the solution hyperplane depends on these
vectors alone. Any margin point (those SV with <span class="math inline">\(\alpha_i&gt;0\)</span> and
<span class="math inline">\(\xi_i=0\)</span>) can be used to solve for <span class="math inline">\(b\)</span>. Given <span class="math inline">\(\hat {\mathbf{w}}\)</span>
and <span class="math inline">\(\hat b\)</span>, the decision function is given by
<span class="math display" id="eq:dfinput">\[\begin{eqnarray}
\label{eq:dfinput}
\hat f(\mathbf{x})&amp;=&amp;\hat {\mathbf{w}}^T \mathbf{x}+\hat b\nonumber\\
&amp;=&amp;\sum_{\mathbf{x}_i \in \mbox{sv}} \hat \alpha_i y_i \mathbf{x}_i^T\mathbf{x}+\hat b.
\tag{8.3}
\end{eqnarray}\]</span></p>
<p>SVM can be generalized easily to construct nonlinear boundaries. The common strategy is to map the original data into a high-dimensional space and then construct a linear boundary classifier in the transformed space. The original space of the data is called the <em>input space</em> while the transformed high dimensional space is called the <em>feature space</em>. Figure <a href="discriminant-analysis-and-classification.html#fig:svmmap">8.6</a> shows an example of a feature mapping from a two-dimensional input space to a two-dimensional feature space. The data can not be separated by a linear function in the input space but can be in the feature space under the mapping <span class="math inline">\(\Phi\)</span>. Although the same dimension is used in Figure (fig:svmmap) for illustration, the feature space is usually of a much higher dimension than the input space.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:svmmap"></span>
<img src="Plots/tran.png" alt="An example of mapping where data can be separated by a linear function in the feature space but can not in the input space." width="80%" />
<p class="caption">
Figure 8.6: An example of mapping where data can be separated by a linear function in the feature space but can not in the input space.
</p>
</div>
<p>With a nonlinear mapping <span class="math inline">\(\Phi\)</span>, SVM is able to produce nonlinear boundaries in the input space by constructing a linear boundary in the feature space. The decision function of SVM now becomes
<span class="math display" id="eq:dffeature">\[\begin{equation}
\label{eq:dffeature}
\hat f(\mathbf{x})=\sum_{\mathbf{x}_i \in \mbox{sv}} \hat \alpha_i y_i \Phi(\mathbf{x}_i)^T \Phi(\mathbf{x})+\hat b, \tag{8.4}
\end{equation}\]</span>
which is in terms of inner products in the feature space. Calculating the inner product of <span class="math inline">\(\Phi(\mathbf{x}_i)^T \Phi(\mathbf{x})\)</span> might be expensive when the feature space is of high dimension. Fortunately, to calculate (<span class="math inline">\(\ref{eq:dffeature}\)</span>), we do not need to know <span class="math inline">\(\Phi\)</span> explicitly but only need to know how to evaluate the inner products <span class="math inline">\(\Phi(\mathbf{x}_i)^T \Phi(\mathbf{x})\)</span>. This can be done by using a suitable kernel function. A kernel function <span class="math inline">\(\mathcal{K}\)</span> is defined as
<span class="math display">\[
\begin{aligned}
\mathcal{K}(\mathbf{x}_i,\mathbf{x}_j)&amp;=\langle \Phi(\mathbf{x}_i),\Phi(\mathbf{x}_j) \rangle \\
&amp;=\Phi(\mathbf{x}_i)^T \Phi(\mathbf{x}_j),
\end{aligned}
\]</span>
where <span class="math inline">\(\mathbf{x}_i,\mathbf{x}_j\)</span> are two points in the input space. Many different forms of <span class="math inline">\(\mathcal{K}\)</span> are possible, each leading to a different feature space. Mercerâs theorem (Mercer, 1909) provides one way to
construct kernels. It says that a symmetric function in the input space, <span class="math inline">\(\mathcal{K}\)</span>, is a kernel function if and only if the Gram matrix
<span class="math display">\[
{\mathbf K}=[\mathcal{K}(\mathbf{x}_i, \mathbf{x}_j)]_{i,j=1}^N
\]</span>
is positive semi-definite, i.e., has non-negative eigenvalues. Three common kernels are:</p>
<ol style="list-style-type: decimal">
<li>polynomial: <span class="math inline">\(\mathcal{K}(\mathbf{x}_i, \mathbf{x}_j)=(\gamma \mathbf{x}_i^T \mathbf{x}_j+ r)^d, \gamma&gt;0\)</span>.</li>
<li>radial basis: <span class="math inline">\(\mathcal{K}(\mathbf{x}_i, \mathbf{x}_j)=\mbox{exp}(-\gamma \|\mathbf{x}_i-\mathbf{x}_j\|^2), \gamma&gt;0\)</span>.</li>
<li>sigmoid: <span class="math inline">\(\mathcal{K}(\mathbf{x}_i, \mathbf{x}_j)=\mbox{tanh}(\gamma \mathbf{x}_i^T \mathbf{x}_j+r)\)</span>.
\end{enumerate}</li>
</ol>
<p>Here, <span class="math inline">\(\gamma, r\)</span> and <span class="math inline">\(d\)</span> are the kernel parameters that need to be tuned by the training data. Given the kernel function <span class="math inline">\(\mathcal{K}\)</span>, the decision function (<span class="math inline">\(\ref{eq:dffeature}\)</span>) can be written as
<span class="math display">\[
\begin{aligned}
\hat f(\mathbf{x})&amp;=\sum_{\mathbf{x}_i \in \mbox{sv}} \hat \alpha_i y_i \Phi(\mathbf{x}_i)^T \Phi(\mathbf{x})+\hat b\\
&amp;=\sum_{\mathbf{x}_i \in \mbox{sv}} \hat \alpha_i y_i
\mathcal{K}(\mathbf{x}_i, \mathbf{x})+\hat b.
\end{aligned}
\]</span></p>
<p>SVM has been implemented in R (Meyer, 2007), which makes use of the C++ implementation of SVM by Chang and Lin (2007). The function <em>svm</em> implemented in R takes the majority class as class 1 and the minority one as class -1 by default; therefore, a data point that is far away from the separating hyperplane on the negative side casts a lot of confidence that this item belongs to the rare class.</p>
<p>We apply the SVM on the iris data. Ten-fold cross-validation was used to select the optimal tuning parameter sigma and cost. The final model with sigma=0.5 and cost=1 yields an accuracy of 0.96667 on the testing set.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="discriminant-analysis-and-classification.html#cb142-1" tabindex="-1"></a><span class="fu">library</span>(e1071) <span class="co"># for SVM</span></span>
<span id="cb142-2"><a href="discriminant-analysis-and-classification.html#cb142-2" tabindex="-1"></a><span class="co"># Train the SVM model with cross-validation</span></span>
<span id="cb142-3"><a href="discriminant-analysis-and-classification.html#cb142-3" tabindex="-1"></a>grid_sigma <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">^</span>(<span class="sc">-</span><span class="dv">15</span><span class="sc">:</span><span class="dv">0</span>)</span>
<span id="cb142-4"><a href="discriminant-analysis-and-classification.html#cb142-4" tabindex="-1"></a>grid_cost <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">^</span>(<span class="sc">-</span><span class="dv">5</span><span class="sc">:</span><span class="dv">0</span>)</span>
<span id="cb142-5"><a href="discriminant-analysis-and-classification.html#cb142-5" tabindex="-1"></a>svmm <span class="ot">&lt;-</span> <span class="fu">train</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> training, <span class="at">method =</span> <span class="st">&quot;svmRadial&quot;</span>, <span class="at">preProc =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>), </span>
<span id="cb142-6"><a href="discriminant-analysis-and-classification.html#cb142-6" tabindex="-1"></a>              <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb142-7"><a href="discriminant-analysis-and-classification.html#cb142-7" tabindex="-1"></a>              <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">sigma =</span> grid_sigma, <span class="at">C =</span> grid_cost))</span>
<span id="cb142-8"><a href="discriminant-analysis-and-classification.html#cb142-8" tabindex="-1"></a><span class="fu">plot</span>(svmm)</span></code></pre></div>
<p><img src="Plots/svmcrossval-1.png" width="70%" /></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="discriminant-analysis-and-classification.html#cb143-1" tabindex="-1"></a>svmm<span class="sc">$</span>finalModel</span></code></pre></div>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 1 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.5 
## 
## Number of Support Vectors : 54 
## 
## Objective Function Value : -3.6316 -3.7821 -19.1456 
## Training error : 0.016667</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="discriminant-analysis-and-classification.html#cb145-1" tabindex="-1"></a><span class="co"># Predict the species on the testing data</span></span>
<span id="cb145-2"><a href="discriminant-analysis-and-classification.html#cb145-2" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(svmm, <span class="at">newdata =</span> testing)</span>
<span id="cb145-3"><a href="discriminant-analysis-and-classification.html#cb145-3" tabindex="-1"></a><span class="co"># Evaluate the accuracy of the model</span></span>
<span id="cb145-4"><a href="discriminant-analysis-and-classification.html#cb145-4" tabindex="-1"></a>(svmtab <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(predictions, testing<span class="sc">$</span>Species)<span class="sc">$</span>table)</span></code></pre></div>
<pre><code>##             Reference
## Prediction   setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          9         0
##   virginica       0          1        10</code></pre>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="discriminant-analysis-and-classification.html#cb147-1" tabindex="-1"></a>(<span class="at">accuracy=</span><span class="fu">sum</span>(<span class="fu">diag</span>(svmtab))<span class="sc">/</span><span class="fu">sum</span>(svmtab))</span></code></pre></div>
<pre><code>## [1] 0.9666667</code></pre>
</div>
<div id="neural-networks" class="section level2 hasAnchor" number="8.14">
<h2><span class="header-section-number">8.14</span> Neural Networks<a href="discriminant-analysis-and-classification.html#neural-networks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A Neural Network or more precisely an Artificial Neural Network (ANN) is a computational model that is inspired by the way biological neural networks in the human brain process information. The basic unit of computation in a neural network is the neuron, often called a <em>node</em> or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (<span class="math inline">\(w\)</span>), which is assigned on the basis of its relative importance to other inputs. The node applies a function <span class="math inline">\(f\)</span> (called the <em>activation function</em>) to the weighted sum of its inputs as shown in the figure below (source: <a href="https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/" class="uri">https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</a>):</p>
<p><img src="Plots/single_neuron.png" width="50%" style="display: block; margin: auto;" /></p>
<p>There are three popular choices of activation functions:</p>
<ul>
<li>Sigmoid <span class="math inline">\(f(x)=\frac{1}{1+e^{-x}}\)</span>.</li>
<li>Hyperbolic tangent <span class="math inline">\(f(x)=tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\)</span>.</li>
<li>Rectified linear function <span class="math inline">\(f(x)=\max(0, x)\)</span>.</li>
</ul>
<p>The corresponding graphs of the activation functions are as follows:
<img src="Plots/activation_function.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Neural networks can also have multiple hidden layers and multiple output units. For example, here is a network with two hidden layers (L2 and L3) and two output units in layer L4:
<img src="Plots/network_layer_weight.png" width="80%" style="display: block; margin: auto;" /></p>
<p>For regression, there is only one output unit; for <span class="math inline">\(K\)</span>-class classification, there are <span class="math inline">\(K\)</span> output units with the <span class="math inline">\(k\)</span>th unit modelling the probability of class <span class="math inline">\(k\)</span>. Each node in the hidden layers is a function of linear combinations of the units in the previous layer. For example,
<span class="math display">\[
Z_1=f(w_{01}+w_{11}X_1+w_{21}X_2+w_{31}X_3)=f(\mathbf{w}_1^T\mathbf{X})
\]</span>
where <span class="math inline">\(f(.)\)</span> can be any of those activation functions, and <span class="math inline">\(\mathbf{X}^T=[1, X_1, X_2, X_3]\)</span>. In general,
<span class="math display">\[
Z_i=f(\mathbf{w}_i^T\mathbf{X}), i=1, 2, 3; \quad U_i=f(\mathbf{a}_i^T\mathbf{Z}), i=1, 2
\]</span>
where <span class="math inline">\(\mathbf{Z}^T=[1, Z_1, Z_2, Z_3]\)</span>. For the output layer, each node is a function of linear combinations of the last hidden layer
<span class="math display">\[
Y_i=g(\mathbf{b}_i^T\mathbf{U})=g(T_i), i=1, 2
\]</span>
where the output function <span class="math inline">\(g(.)\)</span> is typically the identify function <span class="math inline">\(Y_i=g(T_i)=T_i\)</span> for regression problem and the softmax function for classification problem. The softmax function is given by
<span class="math display">\[
Y_i=g(T_i)=\frac{e^{T_i}}{\sum_{j=1}^2 e^{T_j}}, i=1, 2
\]</span>
where <span class="math inline">\(\mathbf{U}^T=[1, U_1, U_2]\)</span>.</p>
<p>The neural network model has unknown parameters, the <em>weights</em>, e.g., those <span class="math inline">\(w_{ij}, a_{ij}\)</span> and <span class="math inline">\(b_{ij}\)</span> in the diagram above. The weights can be estimated by minimizing the sum of square errors <span class="math inline">\(SSE=\sum (y_i-\hat y_i)^2\)</span> for regression problems or minimizing the cross-entropy (deviance) <span class="math inline">\(\sum y_i\ln \hat y_i\)</span> for classification problems.</p>
<p>We apply the nnet on the iris data. A nnet with 3 hidden neurons in the first hidden layer and 2 hidden neurons in the second layer was fit on the training set. The net structure is shown in the graph below.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="discriminant-analysis-and-classification.html#cb149-1" tabindex="-1"></a><span class="fu">library</span>(neuralnet)</span>
<span id="cb149-2"><a href="discriminant-analysis-and-classification.html#cb149-2" tabindex="-1"></a><span class="co"># Convert the response variable to a factor</span></span>
<span id="cb149-3"><a href="discriminant-analysis-and-classification.html#cb149-3" tabindex="-1"></a>training<span class="sc">$</span>Species <span class="ot">=</span> <span class="fu">as.factor</span>(training<span class="sc">$</span>Species)</span>
<span id="cb149-4"><a href="discriminant-analysis-and-classification.html#cb149-4" tabindex="-1"></a><span class="co"># fit a nnet with (3, 2) hidden neurons</span></span>
<span id="cb149-5"><a href="discriminant-analysis-and-classification.html#cb149-5" tabindex="-1"></a>nnm<span class="ot">=</span><span class="fu">neuralnet</span>(Species <span class="sc">~</span> Sepal.Length <span class="sc">+</span> Sepal.Width <span class="sc">+</span> Petal.Length <span class="sc">+</span> Petal.Width, </span>
<span id="cb149-6"><a href="discriminant-analysis-and-classification.html#cb149-6" tabindex="-1"></a>              <span class="at">data =</span> training, <span class="at">hidden =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>), <span class="at">linear.output =</span> <span class="cn">FALSE</span>)</span>
<span id="cb149-7"><a href="discriminant-analysis-and-classification.html#cb149-7" tabindex="-1"></a><span class="fu">plot</span>(nnm,<span class="at">rep=</span><span class="st">&quot;best&quot;</span>)</span></code></pre></div>
<p><img src="Plots/nnettrain-1.png" width="70%" /></p>
<p>The resulting nnet yields an accuracy of 0.96667 on the testing set.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="discriminant-analysis-and-classification.html#cb150-1" tabindex="-1"></a><span class="co"># Predict the response on the testing set</span></span>
<span id="cb150-2"><a href="discriminant-analysis-and-classification.html#cb150-2" tabindex="-1"></a>nnpred <span class="ot">&lt;-</span>  <span class="fu">compute</span>(nnm, testing[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])<span class="sc">$</span>net.result</span>
<span id="cb150-3"><a href="discriminant-analysis-and-classification.html#cb150-3" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">apply</span>(nnpred,<span class="dv">1</span>,which.max)</span>
<span id="cb150-4"><a href="discriminant-analysis-and-classification.html#cb150-4" tabindex="-1"></a>nnresult <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;setosa&quot;</span>, <span class="st">&quot;versicolor&quot;</span>, <span class="st">&quot;virginica&quot;</span>)[ind]</span>
<span id="cb150-5"><a href="discriminant-analysis-and-classification.html#cb150-5" tabindex="-1"></a>(nntab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">Predict=</span>nnresult, <span class="at">True=</span>testing<span class="sc">$</span>Species))</span></code></pre></div>
<pre><code>##             True
## Predict      setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          9         0
##   virginica       0          1        10</code></pre>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="discriminant-analysis-and-classification.html#cb152-1" tabindex="-1"></a>(accuracy <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(nntab)))<span class="sc">/</span><span class="fu">sum</span>(nntab)</span></code></pre></div>
<pre><code>## [1] 0.9666667</code></pre>
</div>
<div id="classical-methods" class="section level2 hasAnchor" number="8.15">
<h2><span class="header-section-number">8.15</span> Classical Methods<a href="discriminant-analysis-and-classification.html#classical-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The classical methods covered are Mahalanobis distance, Bayesian posterior, Fisherâs LDA and QDA.</p>
<div id="mahalanobis-distance-method" class="section level3 hasAnchor" number="8.15.1">
<h3><span class="header-section-number">8.15.1</span> Mahalanobis Distance Method<a href="discriminant-analysis-and-classification.html#mahalanobis-distance-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to classify observation to the class with the shortest Mahalanobis distance to the class center. That is
<span class="math display">\[
y &lt;- \text{argmin}_i (\mathbf{x}-\mathbf{\mu_i})^T\mathbf{\Sigma}_i^{-1} (\mathbf{x}-\mathbf{\mu_i})
\]</span>
where <span class="math inline">\(\mathbf{\mu_i}\)</span> and <span class="math inline">\(\mathbf{\Sigma}_i\)</span> are the mean vector and variance-covariance matrix of class <span class="math inline">\(i\)</span>.</p>
</div>
<div id="bayes-posterior" class="section level3 hasAnchor" number="8.15.2">
<h3><span class="header-section-number">8.15.2</span> Bayes Posterior<a href="discriminant-analysis-and-classification.html#bayes-posterior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose there are <span class="math inline">\(K\)</span> classes, <span class="math inline">\(\mathbf{x}|Y=i \sim f_i(\mathbf{x})\)</span>, and the relative frequency of each class is <span class="math inline">\(P(Y=i)=\pi_i\)</span> (we call this the prior). By Bayes rule, the posterior probability is
<span class="math display">\[
P(Y=i|\mathbf{x})=\frac{P(Y=i, \mathbf{x})}{P(\mathbf{x})}=\frac{P(\mathbf{x}|Y=i)P(Y=i)}{\sum_{j=1}^KP(\mathbf{x}|Y=j)P(Y=j)}=\frac{f_i(\mathbf{x})\pi_i}{\sum_{j=1}^K f_j(\mathbf{x})\pi_j}\propto f_i(\mathbf{x})\pi_i
\]</span>
Assign the observation <span class="math inline">\(\mathbf{x}\)</span> to the class yielding the largest posterior probability, i.e.,
<span class="math display">\[
y=\text{argmax}_i \frac{f_i(\mathbf{x})\pi_i}{\sum_{j=1}^K f_j(\mathbf{x})\pi_j}=\text{argmax}_i f_i(\mathbf{x})\pi_i.
\]</span>
If there are only two classes, <span class="math inline">\(K=2\)</span>, assign <span class="math inline">\(\mathbf{x}\)</span> to class 1 if the posterior probability <span class="math inline">\(P(Y=1|\mathbf{x})\ge P(Y=2|\mathbf{x})\)</span>, i.e., if <span class="math inline">\(P(Y=1|\mathbf{x})\ge 0.5\)</span>.</p>
<p>If we further assume the distributions <span class="math inline">\(f_i\)</span> are multivariate normal distributions, we can obtain the discriminant function in a close form. If <span class="math inline">\(\mathbf{X}|Y=0\sim MVN(\mathbf{\mu}_0, \mathbf{\Sigma}_0), \mathbf{X}|Y=1\sim MVN(\mathbf{\mu}_1, \mathbf{\Sigma}_1)\)</span>, The Bayesian posterior is given by
<span class="math display">\[
P(Y=0|\mathbf{x})=\frac{\pi_0 f_0(\mathbf{x})}{\pi_0 f_0(\mathbf{x})+\pi_1 f_1(\mathbf{x})}, \quad P(Y=1|\mathbf{x})=\frac{\pi_1 f_1(\mathbf{x})}{\pi_0 f_0(\mathbf{x})+\pi_1 f_1(\mathbf{x})}
\]</span>
Assign the observation <span class="math inline">\(\mathbf{x}\)</span> to class 0 if <span class="math inline">\(P(Y=0|\mathbf{x})\ge P(Y=1|\mathbf{x})\)</span> or
<span class="math display">\[
\frac{P(Y=0|\mathbf{x})}{P(Y=1|\mathbf{x})}\ge 1 \Longrightarrow \log \frac{P(Y=0|\mathbf{x})}{P(Y=1|\mathbf{x})}=\log \frac{\pi_0 f_0(\mathbf{x})}{\pi_1 f_1(\mathbf{x})}\ge 0
\]</span>
That is
<span class="math display">\[\begin{align*}
&amp;\log \frac{P(Y=0|\mathbf{x})}{P(Y=1|\mathbf{x})}=\log \frac{\pi_0}{\pi_1}+\log \frac{f_0(\mathbf{x})}{f_1(\mathbf{x})}=\log \frac{\pi_0}{\pi_1}+\log \left\{\frac{\frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}_0|^{1/2}}\exp \left[-\frac{(\mathbf{x}-\mathbf{\mu_0})^{T}\mathbf{\Sigma_0}^{-1} (\mathbf{x}-\mathbf{\mu_0})}{2}\right]}{\frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}_1|^{1/2}}\exp \left[-\frac{(\mathbf{x}-\mathbf{\mu_1})^{T}\mathbf{\Sigma_1}^{-1} (\mathbf{x}-\mathbf{\mu_1})}{2}\right]}\right\}\\
&amp;=\log \frac{\pi_0}{\pi_1}+\frac{1}{2}\log\left(\frac{|\mathbf{\Sigma}_0|}{|\mathbf{\Sigma}_1|}\right)-\frac{1}{2}\left[(\mathbf{x}-\mathbf{\mu_0})^{T}\mathbf{\Sigma}_0^{-1} (\mathbf{x}-\mathbf{\mu_0})-(\mathbf{x}-\mathbf{\mu_1})^{T}\mathbf{\Sigma}_1^{-1} (\mathbf{x}-\mathbf{\mu_1}) \right]\\
&amp;=C-\frac{1}{2}\left[\mathbf{x}^T\mathbf{\Sigma}_0^{-1}\mathbf{x}-\mathbf{\mu_0}^T\mathbf{\Sigma}_0^{-1}\mathbf{x}-\mathbf{x}^T\mathbf{\Sigma}_0^{-1}\mathbf{\mu_0}+\mathbf{\mu_0}^T\mathbf{\Sigma}_0^{-1}\mathbf{\mu_0}-\mathbf{x}^T\mathbf{\Sigma}_1^{-1}\mathbf{x}+\mathbf{\mu_1}^T\mathbf{\Sigma}_1^{-1}\mathbf{x}+\mathbf{x}^T\mathbf{\Sigma}_1^{-1}\mathbf{\mu_1}-\mathbf{\mu_1}^T\mathbf{\Sigma}_1^{-1}\mathbf{\mu_1}\right]
\end{align*}\]</span></p>
<ul>
<li>Linear discriminant analysis (LDA) when <span class="math inline">\(\mathbf{\Sigma}_0=\mathbf{\Sigma}_1=\mathbf{\Sigma}\)</span>, the quadratic terms cancel out:
<span class="math display">\[\begin{align*}
&amp;\log \frac{P(Y=0|\mathbf{x})}{P(Y=1|\mathbf{x})}=\log \frac{\pi_0}{\pi_1}+ (\mathbf{\mu_0-\mu_1})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}\left(\mathbf{\mu_0}^T\mathbf{\Sigma}^{-1}\mathbf{\mu_0}-\mathbf{\mu_1}^T\mathbf{\Sigma}^{-1}\mathbf{\mu_1}\right)\\
&amp;=\log \frac{\pi_0}{\pi_1}+(\mathbf{\mu_0-\mu_1})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}\left(\mathbf{\mu_0}^T\mathbf{\Sigma}^{-1}\mathbf{\mu_0}-\mathbf{\mu_0}^T\mathbf{\Sigma}^{-1}\mathbf{\mu_1}+\mathbf{\mu_0}^T\mathbf{\Sigma}^{-1}\mathbf{\mu_1}-\mathbf{\mu_1}^T\mathbf{\Sigma}^{-1}\mathbf{\mu_1}\right)\\
&amp;=\log \frac{\pi_0}{\pi_1}+(\mathbf{\mu_0-\mu_1})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}(\mathbf{\mu_0-\mu_1})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_0+\mu_1})
\end{align*}\]</span>
Therefore the discriminant function
<span class="math display">\[
f(\mathbf{w}, \mathbf{x})=\log \frac{\pi_0}{\pi_1}+\mathbf{w}^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}\mathbf{w}^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_0+\mu_1})  \quad \quad \mbox{ (Let $\mathbf{w}=\mathbf{\mu}_0-\mathbf{\mu}_1$)}
\]</span>
is a linear combination of the vector <span class="math inline">\(\mathbf{x}\)</span>. We assign <span class="math inline">\(\mathbf{x}\)</span> to class 0 if the discriminant function <span class="math inline">\(f(\mathbf{w}, \mathbf{x})\ge 0\)</span>.</li>
<li>Quadratic discriminant analysis (QDA) when <span class="math inline">\(\mathbf{\Sigma}_0\ne \mathbf{\Sigma}_1\)</span>. The quadratic terms can not cancel out each other; therefore, the discriminant function
<span class="math display">\[
f(\mathbf{w}, \mathbf{x})=C-\frac{1}{2}\left[\mathbf{x}^T\mathbf{\Sigma}_0^{-1}\mathbf{x}-\mathbf{\mu_0}^T\mathbf{\Sigma}_0^{-1}\mathbf{x}-\mathbf{x}^T\mathbf{\Sigma}_0^{-1}\mathbf{\mu_0}+\mathbf{\mu_0}^T\mathbf{\Sigma}_0^{-1}\mathbf{\mu_0}-\mathbf{x}^T\mathbf{\Sigma}_1^{-1}\mathbf{x}+\mathbf{\mu_1}^T\mathbf{\Sigma}_1^{-1}\mathbf{x}+\mathbf{x}^T\mathbf{\Sigma}_1^{-1}\mathbf{\mu_1}-\mathbf{\mu_1}^T\mathbf{\Sigma}_1^{-1}\mathbf{\mu_1}\right]
\]</span>
is a quadratic function in <span class="math inline">\(\mathbf{x}\)</span>.</li>
</ul>
<p>The idea of discriminant function can be generalized to three classes problems.</p>
<ul>
<li><p>Assign <span class="math inline">\(\mathbf{x}\)</span> to class 0 if
<span class="math display">\[\begin{align*}
f_{01}(\mathbf{x})&amp;=(\mathbf{\mu_0-\mu_1})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}(\mathbf{\mu_0-\mu_1})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_0+\mu_1})\ge 0 \mbox{ and }\\
f_{02}(\mathbf{x})&amp;=(\mathbf{\mu_0-\mu_2})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}(\mathbf{\mu_0-\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_0+\mu_2})\ge 0
\end{align*}\]</span></p></li>
<li><p>Assign <span class="math inline">\(\mathbf{x}\)</span> to class 1 if
<span class="math display">\[\begin{align*}
f_{10}(\mathbf{x})&amp;=(\mathbf{\mu_1-\mu_0})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}(\mathbf{\mu_1-\mu_0})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1+\mu_0})\ge 0 \mbox{ and }\\
f_{12}(\mathbf{x})&amp;=(\mathbf{\mu_1-\mu_2})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}(\mathbf{\mu_1-\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1+\mu_2})\ge 0
\end{align*}\]</span></p></li>
<li><p>Assign <span class="math inline">\(\mathbf{x}\)</span> to class 2 if
<span class="math display">\[\begin{align*}
f_{20}(\mathbf{x})&amp;=(\mathbf{\mu_2-\mu_0})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}(\mathbf{\mu_2-\mu_0})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_2+\mu_0})\ge 0 \mbox{ and }\\
f_{21}(\mathbf{x})&amp;=(\mathbf{\mu_2-\mu_1})^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}(\mathbf{\mu_2-\mu_1})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_2+\mu_1})\ge 0
\end{align*}\]</span></p></li>
</ul>
<p>In general, we assign <span class="math inline">\(\mathbf{x}\)</span> to the class giving the largest value of <span class="math inline">\(\delta(\cdot)\)</span>, i.e., <span class="math inline">\(y=\text{argmax}_i \delta_i(\mathbf{x})\)</span> where the <span class="math inline">\(\delta(\cdot)\)</span> is defined as
<span class="math display">\[\begin{align*}
\delta_i(\mathbf{x})&amp;=\log(\pi_i)+\mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}\mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i, i=1, 2, \cdots, k, \mbox {for LDA}\\
\delta_i(\mathbf{x})&amp;=\log(\pi_i)-\frac{1}{2}\log|\mathbf{\Sigma}_i|-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_i)^T\mathbf{\Sigma}_i^{-1}(\mathbf{x}-\mathbf{\mu}_i), i=1, 2, \cdots, k, \mbox {for QDA}
\end{align*}\]</span></p>
</div>
<div id="fishers-discriminant-analysis" class="section level3 hasAnchor" number="8.15.3">
<h3><span class="header-section-number">8.15.3</span> Fisherâs Discriminant Analysis<a href="discriminant-analysis-and-classification.html#fishers-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For Fisherâs discriminant analysis, we do not assume the distribution of <span class="math inline">\(\mathbf{X}\)</span> is multivariate normal. And we want to find a linear combination (projection) of <span class="math inline">\(\mathbf{X}\)</span>, such that the between-group variation in the new coordinate is maximized. That is to find
<span class="math display">\[
\text{argmax}_{\mathbf{a}} \frac{\mathbf{a}^T\mathbf{S}_B\mathbf{a}}{\mathbf{a}^T\mathbf{S}_W\mathbf{a}}
\]</span>
which is equivalent to finding <span class="math inline">\(\text{argmax}_{\mathbf{a}} \mathbf{a}^T\mathbf{S}_B\mathbf{a}\)</span> with the constraint <span class="math inline">\(\mathbf{a}^T\mathbf{S}_W\mathbf{a}=1\)</span>. By Lagrange multiplier method, the objective function is
<span class="math display">\[
Q= \mathbf{a}^T\mathbf{S}_B\mathbf{a}-\lambda(\mathbf{a}^T\mathbf{S}_W\mathbf{a}-1)
\]</span>
<span class="math display">\[\begin{align*}
\frac{\partial Q}{\partial \mathbf{a}}&amp;=2\mathbf{S}_B\mathbf{a}-2\lambda \mathbf{S}_W \mathbf{a}=0 \Longrightarrow
\mathbf{S}_B\mathbf{a}=\lambda \mathbf{S}_W\mathbf{a}  \Longrightarrow \mathbf{S}_W^{-1}\mathbf{S}_B\mathbf{a}=\lambda\mathbf{a}\\
\frac{\partial Q}{\partial \lambda}&amp;=\mathbf{a}^T\mathbf{S}_W\mathbf{a}-1=0\Longrightarrow \mathbf{a}^T\mathbf{S}_W\mathbf{a}=1
\end{align*}\]</span>
The first equation implies that <span class="math inline">\(\lambda\)</span> is an eigenvalue of the matrix <span class="math inline">\(\mathbf{S}_W^{-1}\mathbf{S}_B\)</span> and <span class="math inline">\(\mathbf{a}\)</span> is the corresponding eigenvector. The first equation also implies that <span class="math inline">\(\mathbf{a}^T\mathbf{S}_B\mathbf{a}=\lambda\)</span>, that is, to maximize <span class="math inline">\(\mathbf{a}^T\mathbf{S}_B\mathbf{a}\)</span> is to maximize <span class="math inline">\(\lambda\)</span>. As a result, <span class="math inline">\(\mathbf{e}\)</span> is the eigenvector associated to the largest eigenvalue of the matrix <span class="math inline">\(\mathbf{S}_W^{-1}\mathbf{S}_B\)</span>, then
<span class="math display">\[
\mathbf{a}=\frac{\mathbf{e}}{\sqrt{c}}, \quad \mbox{ where $c=\mathbf{e}^T\mathbf{S}_W\mathbf{e}$}.
\]</span>
Assign <span class="math inline">\(\mathbf{x}\)</span> to the class giving the smallest projected distance to the class center, i.e.,
<span class="math display">\[
y=\arg\min_i [\mathbf{a}^T(\mathbf{x}-\mathbf{\mu}_i)]^2.
\]</span>
<span class="math inline">\(\textbf{Example}\)</span>: Bayes Posterior and Fisherâs Discriminant Analysis</p>
<p>Suppose <span class="math inline">\(\mathbf{\mu}_1=[2.5, 2.5]^T, \mathbf{\mu}_2=[7.5, 7.5]^T\)</span> and
<span class="math display">\[
\mathbf{\Sigma}_1=\mathbf{\Sigma}_2=\left[
\begin{array}{rr}
1&amp;0.2\\
0.2&amp;4
\end{array}
\right].
\]</span>
Classify <span class="math inline">\(\mathbf{x}=[4, 6]^T\)</span> to either class 1 or class 2.</p>
<p>Here is the <span class="math inline">\(\textsf{R}\)</span> code to do the example.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="discriminant-analysis-and-classification.html#cb154-1" tabindex="-1"></a>mu1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.5</span>, <span class="fl">2.5</span>)</span>
<span id="cb154-2"><a href="discriminant-analysis-and-classification.html#cb154-2" tabindex="-1"></a>mu2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">7.5</span>, <span class="fl">7.5</span>)</span>
<span id="cb154-3"><a href="discriminant-analysis-and-classification.html#cb154-3" tabindex="-1"></a>sigma1 <span class="ot">&lt;-</span> sigma2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.2</span>,<span class="fl">0.2</span>,<span class="dv">4</span>),<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb154-4"><a href="discriminant-analysis-and-classification.html#cb154-4" tabindex="-1"></a><span class="co">#given a new data (4, 6), which class shall we assign?</span></span>
<span id="cb154-5"><a href="discriminant-analysis-and-classification.html#cb154-5" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)</span>
<span id="cb154-6"><a href="discriminant-analysis-and-classification.html#cb154-6" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">solve</span>(sigma1)<span class="sc">%*%</span><span class="fu">matrix</span>(mu1<span class="sc">-</span>mu2,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb154-7"><a href="discriminant-analysis-and-classification.html#cb154-7" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">t</span>(e)<span class="sc">%*%</span>sigma1<span class="sc">%*%</span>e</span>
<span id="cb154-8"><a href="discriminant-analysis-and-classification.html#cb154-8" tabindex="-1"></a>a <span class="ot">&lt;-</span> e<span class="sc">/</span><span class="fu">sqrt</span>(c[<span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb154-9"><a href="discriminant-analysis-and-classification.html#cb154-9" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">t</span>(a)<span class="sc">%*%</span>(<span class="fu">matrix</span>(x,<span class="dv">2</span>,<span class="dv">1</span>)<span class="sc">-</span>mu1))</span>
<span id="cb154-10"><a href="discriminant-analysis-and-classification.html#cb154-10" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">t</span>(a)<span class="sc">%*%</span>(<span class="fu">matrix</span>(x,<span class="dv">2</span>,<span class="dv">1</span>)<span class="sc">-</span>mu2))</span>
<span id="cb154-11"><a href="discriminant-analysis-and-classification.html#cb154-11" tabindex="-1"></a><span class="fu">c</span>(d1,d2) <span class="co">#closer to mu1, classify to class 1</span></span></code></pre></div>
<pre><code>## [1] 1.991556 3.397359</code></pre>
<p>We apply LDA and QDA on the iris data focusing on the two species: versicolor and virginica. We first split the data into training and test sets. We did a stratified sampling with 75% training and 25% testing.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="discriminant-analysis-and-classification.html#cb156-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb156-2"><a href="discriminant-analysis-and-classification.html#cb156-2" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb156-3"><a href="discriminant-analysis-and-classification.html#cb156-3" tabindex="-1"></a>data <span class="ot">&lt;-</span> iris[<span class="dv">51</span><span class="sc">:</span><span class="dv">150</span>,]</span>
<span id="cb156-4"><a href="discriminant-analysis-and-classification.html#cb156-4" tabindex="-1"></a><span class="fu">levels</span>(data<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="discriminant-analysis-and-classification.html#cb158-1" tabindex="-1"></a>data<span class="sc">$</span>Species <span class="ot">&lt;-</span> <span class="fu">droplevels</span>(data<span class="sc">$</span>Species) <span class="co">#drop the level with no observations</span></span>
<span id="cb158-2"><a href="discriminant-analysis-and-classification.html#cb158-2" tabindex="-1"></a><span class="fu">levels</span>(data<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## [1] &quot;versicolor&quot; &quot;virginica&quot;</code></pre>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="discriminant-analysis-and-classification.html#cb160-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4061</span>)</span>
<span id="cb160-2"><a href="discriminant-analysis-and-classification.html#cb160-2" tabindex="-1"></a>flds <span class="ot">&lt;-</span> <span class="fu">createFolds</span>(data<span class="sc">$</span>Species, <span class="at">k =</span> <span class="dv">4</span>, <span class="at">list =</span> <span class="cn">TRUE</span>, <span class="at">returnTrain =</span> <span class="cn">FALSE</span>) <span class="co">#response must be factor for stratified sampling</span></span>
<span id="cb160-3"><a href="discriminant-analysis-and-classification.html#cb160-3" tabindex="-1"></a>train <span class="ot">&lt;-</span> data[<span class="sc">-</span>flds[[<span class="dv">1</span>]],]</span>
<span id="cb160-4"><a href="discriminant-analysis-and-classification.html#cb160-4" tabindex="-1"></a>test <span class="ot">&lt;-</span> data[flds[[<span class="dv">1</span>]],]</span>
<span id="cb160-5"><a href="discriminant-analysis-and-classification.html#cb160-5" tabindex="-1"></a><span class="fu">table</span>(train<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## 
## versicolor  virginica 
##         37         38</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="discriminant-analysis-and-classification.html#cb162-1" tabindex="-1"></a><span class="fu">table</span>(test<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## 
## versicolor  virginica 
##         13         12</code></pre>
<p>The misclassification rate of Fisherâs LDA method is 0.04, i.e., the accuracy is 96%.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="discriminant-analysis-and-classification.html#cb164-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb164-2"><a href="discriminant-analysis-and-classification.html#cb164-2" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb164-3"><a href="discriminant-analysis-and-classification.html#cb164-3" tabindex="-1"></a><span class="fu">library</span>(klaR)</span>
<span id="cb164-4"><a href="discriminant-analysis-and-classification.html#cb164-4" tabindex="-1"></a><span class="co">#pairs.panels(train[,1:4],bg=c(&quot;red&quot;,&quot;blue&quot;)[train$Species],pch=21)</span></span>
<span id="cb164-5"><a href="discriminant-analysis-and-classification.html#cb164-5" tabindex="-1"></a>objlda <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species<span class="sc">~</span>.,<span class="at">data=</span>train)</span>
<span id="cb164-6"><a href="discriminant-analysis-and-classification.html#cb164-6" tabindex="-1"></a>ldap <span class="ot">&lt;-</span> <span class="fu">predict</span>(objlda,test)</span>
<span id="cb164-7"><a href="discriminant-analysis-and-classification.html#cb164-7" tabindex="-1"></a>(<span class="at">ldatab=</span><span class="fu">table</span>(<span class="at">True=</span>test<span class="sc">$</span>Species,<span class="at">Predict=</span>ldap<span class="sc">$</span>class))</span></code></pre></div>
<pre><code>##             Predict
## True         versicolor virginica
##   versicolor         12         1
##   virginica           0        12</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="discriminant-analysis-and-classification.html#cb166-1" tabindex="-1"></a>(<span class="at">ldar=</span><span class="dv">1</span><span class="sc">-</span><span class="fu">sum</span>(<span class="fu">diag</span>(ldatab))<span class="sc">/</span><span class="fu">sum</span>(ldatab))</span></code></pre></div>
<pre><code>## [1] 0.04</code></pre>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="discriminant-analysis-and-classification.html#cb168-1" tabindex="-1"></a><span class="fu">partimat</span>(Species<span class="sc">~</span>.,<span class="at">data=</span>train,<span class="at">method=</span><span class="st">&quot;lda&quot;</span>)</span></code></pre></div>
<p><img src="Plots/misrate-1.png" width="85%" /></p>
<p>The misclassification rate of Fisherâs QDA method is 0.04, i.e., the accuracy is 96%.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="discriminant-analysis-and-classification.html#cb169-1" tabindex="-1"></a>objqda <span class="ot">&lt;-</span> <span class="fu">qda</span>(Species<span class="sc">~</span>.,<span class="at">data=</span>train)</span>
<span id="cb169-2"><a href="discriminant-analysis-and-classification.html#cb169-2" tabindex="-1"></a>qdap <span class="ot">&lt;-</span> <span class="fu">predict</span>(objqda,test)</span>
<span id="cb169-3"><a href="discriminant-analysis-and-classification.html#cb169-3" tabindex="-1"></a>(qdatab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">True=</span>test<span class="sc">$</span>Species,<span class="at">Predict=</span>qdap<span class="sc">$</span>class))</span></code></pre></div>
<pre><code>##             Predict
## True         versicolor virginica
##   versicolor         12         1
##   virginica           0        12</code></pre>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="discriminant-analysis-and-classification.html#cb171-1" tabindex="-1"></a>(qdar <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">sum</span>(<span class="fu">diag</span>(qdatab))<span class="sc">/</span><span class="fu">sum</span>(qdatab))</span></code></pre></div>
<pre><code>## [1] 0.04</code></pre>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="discriminant-analysis-and-classification.html#cb173-1" tabindex="-1"></a><span class="fu">partimat</span>(Species<span class="sc">~</span>.,<span class="at">data=</span>train,<span class="at">method=</span><span class="st">&quot;qda&quot;</span>)</span></code></pre></div>
<p><img src="Plots/qdametrics-1.png" width="85%" /></p>
</div>
</div>
<div id="summary" class="section level2 hasAnchor" number="8.16">
<h2><span class="header-section-number">8.16</span> Summary<a href="discriminant-analysis-and-classification.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here I summarize briefly the pros and cons of each classifier.</p>
<p><span class="math display">\[
\begin{array}{|||}
\hline
{\bf Method}&amp;{\bf Advantages}&amp;{\bf Disadvantages}\\
\hline
\text{KNN}&amp;\text{easy to understand and calculate}&amp;\text{based on distance, not designed for mixed data types}\\
&amp;\text{able to capture local structure}&amp;\text{need to tune $k$: # of nearest neighbors}\\
\hline
\text{Logistic}&amp;\text{easy to interpret}&amp;\text{create linear boundary}\\
\text{regression}&amp;\text{easy to handle mixed data types}&amp;\text{unstable if the classes are well separated}\\
\hline
\text{Bayes} &amp;\text{linear (LDA) and quadratic boundary (QDA)}&amp;\text{distribution assumption}\\
\text{posterior}&amp;\text{optimal under normality}&amp;\text{need estimate means and variances}\\
\hline
\text{Classification}&amp;\text{easy to interpret (graph)}&amp;\text{large variation}\\
\text{tree}&amp;\text{easy to handle mixed data types}&amp;\text{tend to overfit}\\
\hline
\text{Random} &amp;\text{good performance}&amp;\text{hard to interpret}\\
\text{forest}&amp;\text{importance table}&amp;\text{need to tune parameters}\\
\hline
\text{Neural}&amp;\text{good performance}&amp;\text{hard to interpret}\\
\text{network}&amp;\text{handle mixed data type}&amp;\text{need to tune parameters}\\
\hline
\text{SVM}&amp;\text{good performance, flexible boundary}&amp;\text{need to tune parameters}\\
\hline
\end{array}
\]</span></p>
</div>
<div id="revisit-learning-outcomes" class="section level2 unnumbered hasAnchor">
<h2>Revisit Learning Outcomes<a href="discriminant-analysis-and-classification.html#revisit-learning-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After finishing this chapter, students should be able to</p>
<ul>
<li>Use proper performance metrics to compare classification methods.</li>
<li>Use cross-validation method to fit classification models.</li>
<li>Explain the main idea of each classification method covered: K-nearest neighbor, logistic regression, classification tree, random forests, neural network, support vector machines, and Fisherâs LDA and QDA.</li>
<li>Use R to fit the models listed above and interpret the computer outputs.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="factor-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clustering-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
